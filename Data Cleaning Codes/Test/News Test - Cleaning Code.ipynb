{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a25639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to /Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/News_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Define the file path for the dataset (TSV format)\n",
    "file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/news.tsv'\n",
    "\n",
    "# Read the dataset (without header)\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "# Insert the header row\n",
    "header = [\n",
    "    \"News ID\", \n",
    "    \"Category\", \n",
    "    \"Subcategory\", \n",
    "    \"Title\", \n",
    "    \"Abstract\", \n",
    "    \"URL\", \n",
    "    \"Entities Mentioned\", \n",
    "    \"Entities in Abstract\"\n",
    "]\n",
    "df.columns = header\n",
    "\n",
    "# 1. Handle Missing Data (Drop for Critical Fields)\n",
    "# -------------------------------------------------\n",
    "# Drop rows where 'Title', 'Abstract', or 'URL' are missing\n",
    "df.dropna(subset=['Title', 'Abstract', 'URL'], inplace=True)\n",
    "\n",
    "# 2. Impute Missing Values for Entities Mentioned and Entities in Abstract\n",
    "# ------------------------------------------------------------------------\n",
    "# Impute missing values in 'Entities Mentioned' and 'Entities in Abstract' with 'unknown'\n",
    "df['Entities Mentioned'] = df['Entities Mentioned'].fillna('unknown')\n",
    "df['Entities in Abstract'] = df['Entities in Abstract'].fillna('unknown')\n",
    "\n",
    "# For 'Category' and 'Subcategory', impute missing values with 'unknown'\n",
    "df['Category'] = df['Category'].fillna('unknown')\n",
    "df['Subcategory'] = df['Subcategory'].fillna('unknown')\n",
    "\n",
    "# 3. Convert All Text to Lowercase\n",
    "# --------------------------------\n",
    "df['Title'] = df['Title'].str.lower()\n",
    "df['Abstract'] = df['Abstract'].str.lower()\n",
    "df['Category'] = df['Category'].str.lower()\n",
    "df['Subcategory'] = df['Subcategory'].str.lower()\n",
    "\n",
    "# 4. Clean 'Subcategory' to Remove Numbers, Spaces, and Special Characters\n",
    "# ------------------------------------------------------------------------\n",
    "def clean_subcategory(subcategory):\n",
    "    # Remove all non-letter characters (numbers, special characters, spaces)\n",
    "    cleaned_subcategory = re.sub(r'[^a-zA-Z]', '', subcategory)\n",
    "    return cleaned_subcategory\n",
    "\n",
    "# Apply cleaning to the 'Subcategory' column\n",
    "df['Subcategory'] = df['Subcategory'].apply(clean_subcategory)\n",
    "\n",
    "# 5. Ensure Title and Abstract Are Separated by Only One Space\n",
    "# ------------------------------------------------------------\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces, ensure only one space between words\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "df['Title'] = df['Title'].apply(clean_text)\n",
    "df['Abstract'] = df['Abstract'].apply(clean_text)\n",
    "\n",
    "# 6. Validate URLs\n",
    "# ----------------\n",
    "def validate_url(url):\n",
    "    # Check if URL is properly formed and contains a valid scheme (http or https)\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.scheme in ['http', 'https']\n",
    "\n",
    "# Apply URL validation\n",
    "df = df[df['URL'].apply(validate_url)]\n",
    "\n",
    "# 7. Format 'Entities Mentioned' and 'Entities in Abstract' as Lists\n",
    "# ------------------------------------------------------------------\n",
    "import json\n",
    "\n",
    "def convert_to_list(entity_column):\n",
    "    # Convert JSON-like strings to Python lists\n",
    "    try:\n",
    "        return json.loads(entity_column)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return [\"unknown\"] if entity_column == \"unknown\" else []\n",
    "\n",
    "df['Entities Mentioned'] = df['Entities Mentioned'].apply(convert_to_list)\n",
    "df['Entities in Abstract'] = df['Entities in Abstract'].apply(convert_to_list)\n",
    "\n",
    "# 8. Final Check for Missing Values and Data Cleaning\n",
    "# ---------------------------------------------------\n",
    "df.replace({\"\": pd.NA, \"[]\": pd.NA}, inplace=True)\n",
    "\n",
    "# 9. Save the Cleaned Data\n",
    "# -------------------------\n",
    "new_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/News_cleaned.csv'\n",
    "df.to_csv(new_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {new_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
