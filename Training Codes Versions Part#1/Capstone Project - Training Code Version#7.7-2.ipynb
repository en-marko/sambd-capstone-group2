{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67477b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import os\n",
    "import joblib\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --------- Step 1: Setup Paths and Variables ---------\n",
    "print(\"Setting up paths and variables...\")\n",
    "save_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Machine Learning Codes/Trained Models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "news_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/News_cleaned.csv'\n",
    "behavior_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/cleaned_behavior_dataset.csv'\n",
    "\n",
    "embedding_size = 64\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 1e-4\n",
    "batch_size = 2048\n",
    "num_heads = 64  # Adjust based on your available resources\n",
    "\n",
    "# --------- Step 2: Set Device for PyTorch ---------\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------- Step 3: Load and Encode Data with Optimization ---------\n",
    "print(\"Loading behavior dataset in chunks...\")\n",
    "def load_behavior_data(file_path, chunksize=500000):\n",
    "    return pd.concat(pd.read_csv(file_path, chunksize=chunksize), ignore_index=True)\n",
    "\n",
    "behavior_df = load_behavior_data(behavior_file_path)\n",
    "\n",
    "print(\"Loading and processing news dataset...\")\n",
    "news_df = pd.read_csv(news_file_path, usecols=['Category', 'Subcategory', 'Title', 'Abstract'])\n",
    "news_df['Text'] = news_df[['Category', 'Subcategory', 'Title', 'Abstract']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "print(\"Creating dictionary-based encoders for users and news items...\")\n",
    "def create_encoding_map(values):\n",
    "    unique_values = pd.Series(values.unique())\n",
    "    return {v: i for i, v in enumerate(unique_values)}\n",
    "\n",
    "user_map = create_encoding_map(behavior_df['User ID'])\n",
    "all_news_ids = pd.concat([\n",
    "    behavior_df['Clicked News IDs'].str.split(',').explode(),\n",
    "    behavior_df['Not-Clicked News IDs'].str.split(',').explode()\n",
    "]).dropna().unique()\n",
    "news_map = {news_id: idx for idx, news_id in enumerate(all_news_ids)}\n",
    "\n",
    "joblib.dump(user_map, os.path.join(save_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(news_map, os.path.join(save_dir, 'news_encoder.pkl'))\n",
    "\n",
    "# Parallelized Safe Encoding Function\n",
    "def safe_encode_parallel(values, encoding_map):\n",
    "    return np.array([encoding_map.get(v, -1) for v in values])\n",
    "\n",
    "def parallel_safe_encode(series, encoding_map, n_jobs=multiprocessing.cpu_count()):\n",
    "    values = series.values\n",
    "    return np.hstack(Parallel(n_jobs=n_jobs)(\n",
    "        delayed(safe_encode_parallel)(chunk, encoding_map) for chunk in np.array_split(values, n_jobs)\n",
    "    ))\n",
    "\n",
    "def encode_behavior_data(df, user_map, news_map):\n",
    "    print(\"Encoding clicked news data with parallel safe encoding...\")\n",
    "    clicked_df = df[['User ID', 'Clicked News IDs']].explode('Clicked News IDs').dropna()\n",
    "    clicked_df['User ID'] = parallel_safe_encode(clicked_df['User ID'], user_map)\n",
    "    clicked_df['News ID'] = parallel_safe_encode(clicked_df['Clicked News IDs'], news_map)\n",
    "    clicked_df['rating'] = 1.0\n",
    "\n",
    "    print(\"Encoding not-clicked news data with parallel safe encoding...\")\n",
    "    not_clicked_df = df[['User ID', 'Not-Clicked News IDs']].explode('Not-Clicked News IDs').dropna()\n",
    "    not_clicked_df['User ID'] = parallel_safe_encode(not_clicked_df['User ID'], user_map)\n",
    "    not_clicked_df['News ID'] = parallel_safe_encode(not_clicked_df['Not-Clicked News IDs'], news_map)\n",
    "    not_clicked_df['rating'] = 0.0\n",
    "\n",
    "    return pd.concat([clicked_df, not_clicked_df], ignore_index=True)\n",
    "\n",
    "combined_df = encode_behavior_data(behavior_df, user_map, news_map)\n",
    "num_users = len(user_map)\n",
    "num_items = len(news_map)\n",
    "\n",
    "# Prepare dataset for PyTorch\n",
    "print(\"Preparing PyTorch dataset...\")\n",
    "inputs = torch.tensor(np.stack([combined_df['User ID'].values, combined_df['News ID'].values], axis=1), dtype=torch.long)\n",
    "ratings = torch.tensor(combined_df['rating'].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(inputs, ratings)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --------- Step 4: Define the Deep Cross Network with Multi-Head Attention ---------\n",
    "class FactorizationMachinesLayer(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(FactorizationMachinesLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def forward(self, user_embedding, item_embedding):\n",
    "        summed_features_emb = user_embedding + item_embedding\n",
    "        summed_features_emb_square = summed_features_emb ** 2\n",
    "        squared_sum_features_emb = user_embedding ** 2 + item_embedding ** 2\n",
    "        cross_term = 0.5 * (summed_features_emb_square - squared_sum_features_emb)\n",
    "        return cross_term\n",
    "\n",
    "class DeepCrossNetworkWithMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size, dropout_rate, num_heads):\n",
    "        super(DeepCrossNetworkWithMultiHeadAttention, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.fm_layer = FactorizationMachinesLayer(embedding_size)\n",
    "        self.dense_cross = nn.Linear(embedding_size, embedding_size)\n",
    "        self.deep_component = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.prediction_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        user_input, item_input = inputs[:, 0], inputs[:, 1]\n",
    "        user_vector = self.user_embedding(user_input)\n",
    "        item_vector = self.item_embedding(item_input)\n",
    "        \n",
    "        # Concatenate and apply multi-head attention\n",
    "        combined_vector = torch.cat((user_vector.unsqueeze(1), item_vector.unsqueeze(1)), dim=1)\n",
    "        attn_output, _ = self.multihead_attention(combined_vector, combined_vector, combined_vector)\n",
    "        attn_output = attn_output[:, 0, :]  # Focus on the user context\n",
    "        \n",
    "        # Factorization Machines and Deep Component\n",
    "        fm_output = self.fm_layer(attn_output, attn_output)\n",
    "        cross_output = torch.relu(self.dense_cross(fm_output))\n",
    "        deep_output = self.deep_component(cross_output)\n",
    "        return torch.sigmoid(self.prediction_layer(deep_output)).squeeze()\n",
    "\n",
    "# --------- Step 5: Train the Model with Learning Rate Scheduler and Early Stopping ---------\n",
    "print(\"Starting model training with adaptive learning rate and early stopping...\")\n",
    "model = DeepCrossNetworkWithMultiHeadAttention(num_users, num_items, embedding_size, dropout_rate, num_heads).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning Rate Scheduler and Early Stopping\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "early_stop_patience = 5\n",
    "max_epochs = 70\n",
    "\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    # Adjust learning rate if loss plateaus\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# --------- Step 6: Save Models and Encoders ---------\n",
    "print(\"Saving models and encoders...\")\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, 'deep_cross_network_with_multihead_attention.pt'))\n",
    "joblib.dump(user_map, os.path.join(save_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(news_map, os.path.join(save_dir, 'news_encoder.pkl'))\n",
    "\n",
    "# --------- Step 7: Train and Save Word2Vec and KMeans Models ---------\n",
    "print(\"Training Word2Vec model...\")\n",
    "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=2, workers=multiprocessing.cpu_count())\n",
    "word2vec_model.build_vocab(news_df['Text'].str.split())\n",
    "word2vec_model.train(news_df['Text'].str.split(), total_examples=len(news_df), epochs=5)\n",
    "\n",
    "# Helper function to get mean embeddings for each text entry\n",
    "def get_mean_embedding(text, model, vector_size=100):\n",
    "    # Retrieve embeddings only for words in the model's vocabulary\n",
    "    embeddings = [model.wv[word] for word in text.split() if word in model.wv]\n",
    "    # Calculate mean if embeddings are available, otherwise return a zero vector\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(vector_size)\n",
    "\n",
    "# Generate embeddings for each text in the dataset\n",
    "print(\"Generating mean embeddings for KMeans training...\")\n",
    "news_embeddings = np.vstack(news_df['Text'].apply(lambda x: get_mean_embedding(x, word2vec_model, vector_size=100)))\n",
    "\n",
    "# Train the KMeans model on the generated embeddings\n",
    "print(\"Training KMeans model...\")\n",
    "mini_batch_kmeans = MiniBatchKMeans(n_clusters=70, batch_size=500, n_init='auto')\n",
    "mini_batch_kmeans.fit(news_embeddings)\n",
    "\n",
    "# Save the Word2Vec and KMeans models\n",
    "word2vec_model.save(os.path.join(save_dir, 'word2vec_model.model'))\n",
    "joblib.dump(mini_batch_kmeans, os.path.join(save_dir, 'mini_batch_kmeans_news_model.pkl'))\n",
    "print(\"All models saved successfully.\")\n",
    "\n",
    "#--------end--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
