{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import lit\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# ----------- Step 1: Load the News and Behavior Datasets -----------\n",
    "\n",
    "# Load the news dataset\n",
    "news_df = pd.read_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/News_cleaned.csv', sep=',', names=[\n",
    "    \"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Entities Mentioned\", \"Entities in Abstract\"\n",
    "])\n",
    "\n",
    "# Load the behavior dataset\n",
    "behavior_df = pd.read_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/cleaned_behavior_dataset.csv', sep=',', names=[\n",
    "    \"Impression ID\", \"User ID\", \"Timestamp\", \"Displayed News List\", \"Impression List (Clicked Status)\",\n",
    "    \"Impression Dictionary\", \"Clicked News IDs\", \"Not-Clicked News IDs\"\n",
    "])\n",
    "\n",
    "# ----------- Step 2: Preprocess the News Dataset (for Content-Based Filtering) -----------\n",
    "\n",
    "# Include both category and subcategory in the 'Text' field, alongside Title and Abstract\n",
    "news_df['Text'] = news_df['Category'] + \" \" + news_df['Subcategory'] + \" \" + news_df['Title'] + \" \" + news_df['Abstract']\n",
    "\n",
    "# TF-IDF vectorization for content-based recommendations\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(news_df['Text'])\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# ----------- Step 3: Word2Vec for News Embedding (Advanced Content-Based) -----------\n",
    "\n",
    "# Tokenize text into sentences for Word2Vec\n",
    "sentences = [text.split() for text in news_df['Text']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Function to create embeddings for each article by averaging word embeddings\n",
    "def get_article_embedding(text):\n",
    "    words = text.split()\n",
    "    word_vecs = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100)\n",
    "\n",
    "# Create embeddings for each article\n",
    "news_df['Article Embedding'] = news_df['Text'].apply(get_article_embedding)\n",
    "\n",
    "# ----------- Step 4: KMeans Clustering with Best K Value Automatically Selected -----------\n",
    "\n",
    "def find_best_k(data, max_k=15):\n",
    "    \"\"\"\n",
    "    Determine the best number of clusters using the Silhouette Score.\n",
    "    \"\"\"\n",
    "    best_k = 2  # Initialize\n",
    "    best_score = -1  # Silhouette score\n",
    "    \n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        score = silhouette_score(data, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    \n",
    "    return best_k\n",
    "\n",
    "# Prepare news embeddings for clustering\n",
    "news_embeddings = np.vstack(news_df['Article Embedding'].values)\n",
    "\n",
    "# Automatically select the best K using Silhouette Score\n",
    "best_k = find_best_k(news_embeddings)\n",
    "\n",
    "# Train KMeans with the best K\n",
    "kmeans_news = KMeans(n_clusters=best_k, random_state=42)\n",
    "news_df['News Cluster'] = kmeans_news.fit_predict(news_embeddings)\n",
    "\n",
    "# Save KMeans model for future use\n",
    "save_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Machine Learning Codes/Trained Models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "joblib.dump(kmeans_news, os.path.join(save_dir, 'kmeans_news_model.pkl'))\n",
    "\n",
    "# ----------- Step 5: ALS (Collaborative Filtering) for Clicked and Not-Clicked News -----------\n",
    "\n",
    "# Initialize Spark for ALS\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NewsRecommendationALS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Prepare behavior data for ALS: Combine Clicked and Not-Clicked News IDs\n",
    "clicked_df = behavior_df[['User ID', 'Clicked News IDs']].copy()\n",
    "clicked_df = clicked_df.assign(Clicked_News=clicked_df['Clicked News IDs'].str.split(',')).explode('Clicked_News').drop(columns='Clicked News IDs')\n",
    "clicked_df['Clicked_News'] = clicked_df['Clicked_News'].astype(str)\n",
    "\n",
    "not_clicked_df = behavior_df[['User ID', 'Not-Clicked News IDs']].copy()\n",
    "not_clicked_df = not_clicked_df.assign(Not_Clicked_News=not_clicked_df['Not-Clicked News IDs'].str.split(',')).explode('Not_Clicked_News').drop(columns='Not-Clicked News IDs')\n",
    "not_clicked_df['Not_Clicked_News'] = not_clicked_df['Not_Clicked_News'].astype(str)\n",
    "\n",
    "# Add a rating of 1.0 for clicked news (positive interactions)\n",
    "clicked_df = clicked_df.withColumn('rating', lit(1.0))\n",
    "\n",
    "# Add a rating of 0.0 for not-clicked news (negative interactions)\n",
    "not_clicked_df = not_clicked_df.withColumn('rating', lit(0.0))\n",
    "\n",
    "# Combine both clicked and not-clicked data\n",
    "combined_behavior_df = clicked_df.union(not_clicked_df.rename(columns={'Not_Clicked_News': 'News ID'}))\n",
    "\n",
    "# Convert combined data to Spark DataFrame\n",
    "behavior_spark_df = spark.createDataFrame(combined_behavior_df)\n",
    "\n",
    "# ALS model initialization\n",
    "als = ALS(userCol=\"User ID\", itemCol=\"News ID\", ratingCol=\"rating\", implicitPrefs=True, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Hyperparameter grid search for ALS\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 50, 100]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(als.maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "crossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit ALS model with cross-validation\n",
    "cv_model = crossval.fit(behavior_spark_df)\n",
    "best_als_model = cv_model.bestModel\n",
    "\n",
    "# Save ALS model\n",
    "best_als_model.save(os.path.join(save_dir, 'best_als_model'))\n",
    "\n",
    "# Save other models for future use\n",
    "joblib.dump(vectorizer, os.path.join(save_dir, 'tfidf_vectorizer.pkl'))\n",
    "word2vec_model.save(os.path.join(save_dir, 'word2vec.model'))\n",
    "\n",
    "\n",
    "# ----------- end -----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
