{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import lit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# ----------- Step 1: Spark Session Initialization -----------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NewsRecommendationALS\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.hadoop.hadoop.security.authentication\", \"simple\") \\\n",
    "    .config(\"spark.hadoop.hadoop.security.authorization\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")  # Reduce log verbosity\n",
    "print(\"Spark session created successfully!\")\n",
    "\n",
    "save_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Machine Learning Codes/Trained Models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# ----------- Step 2: Load News and Behavior Datasets in Batches -----------\n",
    "batch_size = 2000  # Reduced batch size to minimize memory issues\n",
    "\n",
    "news_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/News_cleaned.csv'\n",
    "news_columns = [\n",
    "    \"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Entities Mentioned\", \"Entities in Abstract\"\n",
    "]\n",
    "news_df_iterator = pd.read_csv(news_file_path, sep=',', names=news_columns, chunksize=batch_size)\n",
    "\n",
    "behavior_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/cleaned_behavior_dataset.csv'\n",
    "behavior_columns = [\n",
    "    \"Impression ID\", \"User ID\", \"Timestamp\", \"Displayed News List\", \"Impression List (Clicked Status)\",\n",
    "    \"Impression Dictionary\", \"Clicked News IDs\", \"Not-Clicked News IDs\"\n",
    "]\n",
    "behavior_df_iterator = pd.read_csv(behavior_file_path, sep=',', names=behavior_columns, chunksize=batch_size)\n",
    "\n",
    "# ----------- Step 3: Initialize Label Encoders for User ID and News ID -----------\n",
    "user_encoder = LabelEncoder()\n",
    "news_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoders on complete datasets (to ensure consistency across batches)\n",
    "user_ids = []\n",
    "news_ids = []\n",
    "for behavior_df in behavior_df_iterator:\n",
    "    user_ids.extend(behavior_df['User ID'].unique())\n",
    "    news_ids.extend(behavior_df['Clicked News IDs'].str.split(',').explode().dropna().unique())\n",
    "    news_ids.extend(behavior_df['Not-Clicked News IDs'].str.split(',').explode().dropna().unique())\n",
    "\n",
    "user_encoder.fit(user_ids)\n",
    "news_encoder.fit(news_ids)\n",
    "\n",
    "# Restart iterators after fitting label encoders\n",
    "behavior_df_iterator = pd.read_csv(behavior_file_path, sep=',', names=behavior_columns, chunksize=batch_size)\n",
    "news_df_iterator = pd.read_csv(news_file_path, sep=',', names=news_columns, chunksize=batch_size)\n",
    "\n",
    "# ----------- Step 4: Counter Function -----------\n",
    "# Simple batch counter function\n",
    "def batch_counter(start=0):\n",
    "    count = start\n",
    "    while True:\n",
    "        yield count\n",
    "        count += 1\n",
    "\n",
    "# Create an instance of the counter generator\n",
    "counter = batch_counter()\n",
    "\n",
    "# Initialize MiniBatchKMeans for incremental learning\n",
    "best_k = 70\n",
    "mini_batch_kmeans = MiniBatchKMeans(n_clusters=best_k, batch_size=500, random_state=42)\n",
    "\n",
    "# Initialize Word2Vec model for incremental training\n",
    "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# To accumulate ALS training data across batches\n",
    "combined_behavior_spark_df = None\n",
    "\n",
    "# ----------- Step 5: Process Each Batch -----------\n",
    "for news_df, behavior_df in zip(news_df_iterator, behavior_df_iterator):\n",
    "    \n",
    "    batch_number = next(counter)\n",
    "    print(f\"Processing batch number: {batch_number}\")\n",
    "    \n",
    "    # ----------- Preprocess the News Dataset (for Content-Based Filtering) -----------\n",
    "    news_df['Text'] = news_df['Category'] + \" \" + news_df['Subcategory'] + \" \" + news_df['Title'] + \" \" + news_df['Abstract']\n",
    "\n",
    "    # Incremental TF-IDF Vectorizer (requires a global vectorizer initialized once, then updated with each batch)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(news_df['Text'])\n",
    "    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # ----------- Word2Vec for News Embedding (Incremental Training) -----------\n",
    "    sentences = [text.split() for text in news_df['Text']]\n",
    "    word2vec_model.build_vocab(sentences, update=True)\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=5)\n",
    "\n",
    "    def get_article_embedding(text):\n",
    "        words = text.split()\n",
    "        word_vecs = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "        return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100)\n",
    "\n",
    "    news_df['Article Embedding'] = news_df['Text'].apply(get_article_embedding)\n",
    "\n",
    "    # ----------- Mini-Batch KMeans Clustering (Incremental Update) -----------\n",
    "    news_embeddings = np.vstack(news_df['Article Embedding'].values)\n",
    "    mini_batch_kmeans.partial_fit(news_embeddings)\n",
    "\n",
    "    joblib.dump(mini_batch_kmeans, os.path.join(save_dir, f'mini_batch_kmeans_news_model.pkl'))\n",
    "    print(\"Mini-Batch KMeans Model Updated and Saved successfully\")\n",
    "\n",
    "    # ----------- ALS (Collaborative Filtering) for Clicked and Not-Clicked News -----------\n",
    "    clicked_df = behavior_df[['User ID', 'Clicked News IDs']].copy()\n",
    "    clicked_df = clicked_df.assign(Clicked_News=clicked_df['Clicked News IDs'].str.split(',')).explode('Clicked_News').drop(columns='Clicked News IDs')\n",
    "    clicked_df['Clicked_News'] = clicked_df['Clicked_News'].astype(str)\n",
    "\n",
    "    not_clicked_df = behavior_df[['User ID', 'Not-Clicked News IDs']].copy()\n",
    "    not_clicked_df = not_clicked_df.assign(Not_Clicked_News=not_clicked_df['Not-Clicked News IDs'].str.split(',')).explode('Not_Clicked_News').drop(columns='Not-Clicked News IDs')\n",
    "    not_clicked_df['Not_Clicked_News'] = not_clicked_df['Not_Clicked_News'].astype(str)\n",
    "\n",
    "    # Encode User ID and News ID using the fitted label encoders\n",
    "    clicked_df['User ID'] = user_encoder.transform(clicked_df['User ID'])\n",
    "    clicked_df['Clicked_News'] = news_encoder.transform(clicked_df['Clicked_News'])\n",
    "    not_clicked_df['User ID'] = user_encoder.transform(not_clicked_df['User ID'])\n",
    "    not_clicked_df['Not_Clicked_News'] = news_encoder.transform(not_clicked_df['Not_Clicked_News'])\n",
    "\n",
    "    clicked_spark_df = spark.createDataFrame(clicked_df.dropna())\n",
    "    not_clicked_spark_df = spark.createDataFrame(not_clicked_df.dropna())\n",
    "\n",
    "    clicked_spark_df = clicked_spark_df.withColumn('rating', lit(1.0))\n",
    "    not_clicked_spark_df = not_clicked_spark_df.withColumn('rating', lit(0.0))\n",
    "\n",
    "    if combined_behavior_spark_df is None:\n",
    "        combined_behavior_spark_df = clicked_spark_df.union(\n",
    "            not_clicked_spark_df.withColumnRenamed('Not_Clicked_News', 'Clicked_News')\n",
    "        ).withColumnRenamed('Clicked_News', 'News ID')\n",
    "    else:\n",
    "        batch_behavior_spark_df = clicked_spark_df.union(\n",
    "            not_clicked_spark_df.withColumnRenamed('Not_Clicked_News', 'Clicked_News')\n",
    "        ).withColumnRenamed('Clicked_News', 'News ID')\n",
    "        combined_behavior_spark_df = combined_behavior_spark_df.union(batch_behavior_spark_df)\n",
    "\n",
    "    # ALS model initialization and progressive training (retraining with accumulated data)\n",
    "    als = ALS(userCol=\"User ID\", itemCol=\"News ID\", ratingCol=\"rating\", implicitPrefs=True, coldStartStrategy=\"drop\",\n",
    "              rank=10, maxIter=10, regParam=0.1)\n",
    "\n",
    "    try:\n",
    "        als_model = als.fit(combined_behavior_spark_df)\n",
    "        als_model.save(os.path.join(save_dir, 'als_model_incremental.pkl'))\n",
    "        print(\"ALS Model Updated and Saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to train ALS model for batch {batch_number}: {e}\")\n",
    "\n",
    "    # Save other models for future use\n",
    "    joblib.dump(vectorizer, os.path.join(save_dir, 'tfidf_vectorizer.pkl'))\n",
    "    print(\"TF-IDF Vectorizer Model Saved successfully\")\n",
    "    word2vec_model.save(os.path.join(save_dir, 'word2vec_model.model'))\n",
    "    print(\"Word2Vec Model Updated and Saved successfully\")\n",
    "    print(\"All Models Updated and Saved successfully\")\n",
    "\n",
    "    print(f\"Finished processing batch number: {batch_number}\\n\")\n",
    "    \n",
    "print(\"Training Completed and models were saved successfully\")\n",
    "# ----------- End -----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
