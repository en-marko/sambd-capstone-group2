{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67477b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Loading and encoding datasets...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import os\n",
    "import joblib\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --------- Step 1: Setup Paths and Variables ---------\n",
    "save_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Machine Learning Codes/Trained Models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "news_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/News_cleaned.csv'\n",
    "behavior_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/cleaned_behavior_dataset.csv'\n",
    "\n",
    "embedding_size = 64\n",
    "num_heads = 4\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 1e-4\n",
    "batch_size = 512\n",
    "\n",
    "# --------- Step 2: GPU Initialization ---------\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(f\"Using GPU: {physical_devices[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error enabling GPU: {e}. Using CPU instead.\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "# --------- Step 3: Load and Encode Data with Optimization ---------\n",
    "print(\"Loading and encoding datasets...\")\n",
    "\n",
    "# Load data in chunks to reduce memory load\n",
    "def load_and_process_behavior(file_path, chunksize=100000):\n",
    "    chunked_data = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        chunked_data.append(chunk)\n",
    "    return pd.concat(chunked_data, ignore_index=True)\n",
    "\n",
    "behavior_df = load_and_process_behavior(behavior_file_path)\n",
    "\n",
    "# Optimize the merging of text columns with minimal copying\n",
    "news_df = pd.read_csv(news_file_path, usecols=['Category', 'Subcategory', 'Title', 'Abstract'])\n",
    "news_df['Text'] = news_df[['Category', 'Subcategory', 'Title', 'Abstract']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Fit encoders on the unique values only to minimize encoding time\n",
    "user_encoder = LabelEncoder().fit(behavior_df['User ID'].unique())\n",
    "news_encoder = LabelEncoder().fit(pd.concat([\n",
    "    behavior_df['Clicked News IDs'].str.split(',').explode(),\n",
    "    behavior_df['Not-Clicked News IDs'].str.split(',').explode()\n",
    "]).dropna().unique())\n",
    "\n",
    "# --------- Parallelized Safe Encoding Function ---------\n",
    "def safe_encode_parallel(values, encoder):\n",
    "    known_labels = set(encoder.classes_)\n",
    "    return np.array([encoder.transform([v])[0] if v in known_labels else -1 for v in values])\n",
    "\n",
    "# --------- Parallel Encoding for Large Dataframes ---------\n",
    "def parallel_safe_encode(df, column, encoder, n_jobs=multiprocessing.cpu_count()):\n",
    "    values = df[column].values\n",
    "    return np.hstack(Parallel(n_jobs=n_jobs)(delayed(safe_encode_parallel)(chunk, encoder) for chunk in np.array_split(values, n_jobs)))\n",
    "\n",
    "# Transform User and News IDs Safely with Parallel Processing\n",
    "clicked_df = behavior_df[['User ID', 'Clicked News IDs']].explode('Clicked News IDs').dropna()\n",
    "not_clicked_df = behavior_df[['User ID', 'Not-Clicked News IDs']].explode('Not-Clicked News IDs').dropna()\n",
    "\n",
    "clicked_df['User ID'] = parallel_safe_encode(clicked_df, 'User ID', user_encoder)\n",
    "clicked_df['News ID'] = parallel_safe_encode(clicked_df, 'Clicked News IDs', news_encoder)\n",
    "clicked_df['rating'] = 1.0\n",
    "\n",
    "not_clicked_df['User ID'] = parallel_safe_encode(not_clicked_df, 'User ID', user_encoder)\n",
    "not_clicked_df['News ID'] = parallel_safe_encode(not_clicked_df, 'Not-Clicked News IDs', news_encoder)\n",
    "not_clicked_df['rating'] = 0.0\n",
    "\n",
    "# Combine clicked and not-clicked datasets\n",
    "combined_df = pd.concat([clicked_df, not_clicked_df], ignore_index=True)\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(news_encoder.classes_)\n",
    "\n",
    "# --------- Step 4: Factorization Machines Layer ---------\n",
    "class FactorizationMachinesLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(FactorizationMachinesLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def call(self, user_embedding, item_embedding):\n",
    "        summed_features_emb = tf.add(user_embedding, item_embedding)\n",
    "        summed_features_emb_square = tf.square(summed_features_emb)\n",
    "\n",
    "        squared_sum_features_emb = tf.add(tf.square(user_embedding), tf.square(item_embedding))\n",
    "\n",
    "        cross_term = 0.5 * tf.subtract(summed_features_emb_square, squared_sum_features_emb)\n",
    "        return cross_term\n",
    "\n",
    "# --------- Step 5: Define the Advanced Model with DCN ---------\n",
    "class DeepCrossNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, num_heads, dropout_rate):\n",
    "        super(DeepCrossNetwork, self).__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = tf.keras.layers.Embedding(num_items, embedding_size)\n",
    "\n",
    "        self.transformer_encoder = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_size)\n",
    "\n",
    "        self.fm_layer = FactorizationMachinesLayer(embedding_size)\n",
    "\n",
    "        self.dense_cross = tf.keras.layers.Dense(embedding_size, activation='relu')\n",
    "\n",
    "        self.deep_component = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(32, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "        user_vector = self.user_embedding(user_input)\n",
    "        item_vector = self.item_embedding(item_input)\n",
    "\n",
    "        fm_output = self.fm_layer(user_vector, item_vector)\n",
    "        attention_output = self.transformer_encoder(user_vector, item_vector)\n",
    "\n",
    "        cross_output = self.dense_cross(tf.concat([fm_output, attention_output], axis=1))\n",
    "        deep_output = self.deep_component(cross_output)\n",
    "\n",
    "        return self.prediction_layer(deep_output)\n",
    "\n",
    "# --------- Step 6: Train the Model ---------\n",
    "def train_model():\n",
    "    model = DeepCrossNetwork(num_users, num_items, embedding_size, num_heads, dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    inputs = np.stack([combined_df['User ID'].values, combined_df['News ID'].values], axis=1)\n",
    "    ratings = combined_df['rating'].values\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5)\n",
    "\n",
    "    model.fit(inputs, ratings, epochs=10, batch_size=batch_size, validation_split=0.1,\n",
    "              callbacks=[early_stopping, lr_callback])\n",
    "    return model\n",
    "\n",
    "print(\"Training the model...\")\n",
    "model = train_model()\n",
    "\n",
    "# --------- Step 7: Save Models and Encoders ---------\n",
    "model.save(os.path.join(save_dir, 'deep_cross_network'))\n",
    "joblib.dump(user_encoder, os.path.join(save_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(news_encoder, os.path.join(save_dir, 'news_encoder.pkl'))\n",
    "\n",
    "print(\"Model and encoders saved successfully.\")\n",
    "\n",
    "# --------- Step 8: Train and Save Word2Vec and KMeans Models ---------\n",
    "print(\"Training Word2Vec and KMeans models...\")\n",
    "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=2, workers=multiprocessing.cpu_count())\n",
    "word2vec_model.build_vocab(news_df['Text'].str.split())\n",
    "word2vec_model.train(news_df['Text'].str.split(), total_examples=len(news_df), epochs=5)\n",
    "\n",
    "mini_batch_kmeans = MiniBatchKMeans(n_clusters=70, batch_size=500, n_init='auto')\n",
    "news_embeddings = np.vstack(news_df['Text'].apply(\n",
    "    lambda x: np.mean([word for word in x.split() if word in word2vec_model.wv], axis=0)\n",
    "))\n",
    "mini_batch_kmeans.fit(news_embeddings)\n",
    "\n",
    "word2vec_model.save(os.path.join(save_dir, 'word2vec_model.model'))\n",
    "joblib.dump(mini_batch_kmeans, os.path.join(save_dir, 'mini_batch_kmeans_news_model.pkl'))\n",
    "\n",
    "print(\"All models saved successfully.\")\n",
    "\n",
    "\n",
    "#--------end--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a299eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
