{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67477b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up paths and variables...\n",
      "Initializing GPU...\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Loading behavior dataset in chunks...\n",
      "Loading and processing news dataset...\n",
      "Creating dictionary-based encoders for users and news items...\n",
      "Encoding clicked news data with parallel safe encoding...\n",
      "Encoding not-clicked news data with parallel safe encoding...\n",
      "Defining Factorization Machines Layer...\n",
      "Building Deep Cross Network (DCN) model with single attention mechanism...\n",
      "Preparing dataset using tf.data with interleave for efficient loading...\n",
      "Starting model training...\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 11:43:16.829473: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-10-31 11:43:16.829503: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 48.00 GB\n",
      "2024-10-31 11:43:16.829515: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 18.00 GB\n",
      "2024-10-31 11:43:16.829533: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-31 11:43:16.829544: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import os\n",
    "import joblib\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision for improved GPU utilization\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# --------- Step 1: Setup Paths and Variables ---------\n",
    "print(\"Setting up paths and variables...\")\n",
    "save_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Machine Learning Codes/Trained Models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "news_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/News_cleaned.csv'\n",
    "behavior_file_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_train/Cleaned Datasets/cleaned_behavior_dataset.csv'\n",
    "\n",
    "embedding_size = 64\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 1e-4\n",
    "batch_size = 512\n",
    "\n",
    "# --------- Step 2: GPU Initialization ---------\n",
    "print(\"Initializing GPU...\")\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(f\"Using GPU: {physical_devices[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error enabling GPU: {e}. Using CPU instead.\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "# --------- Step 3: Load and Encode Data with Optimization ---------\n",
    "print(\"Loading behavior dataset in chunks...\")\n",
    "def load_behavior_data(file_path, chunksize=500000):\n",
    "    return pd.concat(pd.read_csv(file_path, chunksize=chunksize), ignore_index=True)\n",
    "\n",
    "behavior_df = load_behavior_data(behavior_file_path)\n",
    "\n",
    "print(\"Loading and processing news dataset...\")\n",
    "news_df = pd.read_csv(news_file_path, usecols=['Category', 'Subcategory', 'Title', 'Abstract'])\n",
    "news_df['Text'] = news_df[['Category', 'Subcategory', 'Title', 'Abstract']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Dictionary-based encoding to reduce LabelEncoder time\n",
    "print(\"Creating dictionary-based encoders for users and news items...\")\n",
    "def create_encoding_map(values):\n",
    "    unique_values = pd.Series(values.unique())\n",
    "    return {v: i for i, v in enumerate(unique_values)}\n",
    "\n",
    "user_map = create_encoding_map(behavior_df['User ID'])\n",
    "\n",
    "all_news_ids = pd.concat([\n",
    "    behavior_df['Clicked News IDs'].str.split(',').explode(),\n",
    "    behavior_df['Not-Clicked News IDs'].str.split(',').explode()\n",
    "]).dropna().unique()\n",
    "\n",
    "news_map = {news_id: idx for idx, news_id in enumerate(all_news_ids)}\n",
    "\n",
    "# Save both user and news encoders\n",
    "joblib.dump(user_map, os.path.join(save_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(news_map, os.path.join(save_dir, 'news_encoder.pkl'))\n",
    "\n",
    "# Parallelized Safe Encoding Function\n",
    "def safe_encode_parallel(values, encoding_map):\n",
    "    return np.array([encoding_map.get(v, -1) for v in values])\n",
    "\n",
    "def parallel_safe_encode(series, encoding_map, n_jobs=multiprocessing.cpu_count()):\n",
    "    values = series.values\n",
    "    return np.hstack(Parallel(n_jobs=n_jobs)(\n",
    "        delayed(safe_encode_parallel)(chunk, encoding_map) for chunk in np.array_split(values, n_jobs)\n",
    "    ))\n",
    "\n",
    "def encode_behavior_data(df, user_map, news_map):\n",
    "    print(\"Encoding clicked news data with parallel safe encoding...\")\n",
    "    clicked_df = df[['User ID', 'Clicked News IDs']].explode('Clicked News IDs').dropna()\n",
    "    clicked_df['User ID'] = parallel_safe_encode(clicked_df['User ID'], user_map)\n",
    "    clicked_df['News ID'] = parallel_safe_encode(clicked_df['Clicked News IDs'], news_map)\n",
    "    clicked_df['rating'] = 1.0\n",
    "\n",
    "    print(\"Encoding not-clicked news data with parallel safe encoding...\")\n",
    "    not_clicked_df = df[['User ID', 'Not-Clicked News IDs']].explode('Not-Clicked News IDs').dropna()\n",
    "    not_clicked_df['User ID'] = parallel_safe_encode(not_clicked_df['User ID'], user_map)\n",
    "    not_clicked_df['News ID'] = parallel_safe_encode(not_clicked_df['Not-Clicked News IDs'], news_map)\n",
    "    not_clicked_df['rating'] = 0.0\n",
    "\n",
    "    return pd.concat([clicked_df, not_clicked_df], ignore_index=True)\n",
    "\n",
    "combined_df = encode_behavior_data(behavior_df, user_map, news_map)\n",
    "num_users = len(user_map)\n",
    "num_items = len(news_map)\n",
    "\n",
    "# --------- Step 4: Factorization Machines Layer ---------\n",
    "print(\"Defining Factorization Machines Layer...\")\n",
    "class FactorizationMachinesLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(FactorizationMachinesLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def call(self, user_embedding, item_embedding):\n",
    "        summed_features_emb = tf.add(user_embedding, item_embedding)\n",
    "        summed_features_emb_square = tf.square(summed_features_emb)\n",
    "        squared_sum_features_emb = tf.add(tf.square(user_embedding), tf.square(item_embedding))\n",
    "        cross_term = 0.5 * tf.subtract(summed_features_emb_square, squared_sum_features_emb)\n",
    "        return cross_term\n",
    "\n",
    "# --------- Step 5: Define the Advanced Model with Single Attention Mechanism ---------\n",
    "print(\"Building Deep Cross Network (DCN) model with single attention mechanism...\")\n",
    "class DeepCrossNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size, dropout_rate):\n",
    "        super(DeepCrossNetwork, self).__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = tf.keras.layers.Embedding(num_items, embedding_size)\n",
    "        \n",
    "        # Attention mechanism with a single dense layer\n",
    "        self.attention_dense = tf.keras.layers.Dense(embedding_size, activation='tanh')\n",
    "        self.attention_score = tf.keras.layers.Dense(1, activation='softmax')\n",
    "\n",
    "        self.fm_layer = FactorizationMachinesLayer(embedding_size)\n",
    "        self.dense_cross = tf.keras.layers.Dense(embedding_size, activation='relu')\n",
    "        self.deep_component = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(32, activation='relu')\n",
    "        ])\n",
    "        self.prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs[:, 0], inputs[:, 1]\n",
    "        user_vector = self.user_embedding(user_input)\n",
    "        item_vector = self.item_embedding(item_input)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        combined_vector = tf.concat([user_vector, item_vector], axis=-1)\n",
    "        attention_output = self.attention_dense(combined_vector)\n",
    "        attention_weights = self.attention_score(attention_output)\n",
    "        weighted_vector = combined_vector * attention_weights\n",
    "\n",
    "        fm_output = self.fm_layer(weighted_vector, weighted_vector)\n",
    "        cross_output = self.dense_cross(fm_output)\n",
    "        deep_output = self.deep_component(cross_output)\n",
    "        return self.prediction_layer(deep_output)\n",
    "\n",
    "# --------- Step 6: Prepare Dataset Using tf.data ---------\n",
    "print(\"Preparing dataset using tf.data with interleave for efficient loading...\")\n",
    "def create_dataset(inputs, ratings, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, ratings))\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(buffer_size=1024)\n",
    "    dataset = dataset.interleave(lambda x, y: tf.data.Dataset.from_tensors((x, y)),\n",
    "                                 cycle_length=multiprocessing.cpu_count(), \n",
    "                                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# --------- Step 7: Train the Model with tf.function ---------\n",
    "print(\"Starting model training...\")\n",
    "@tf.function\n",
    "def train_model_step(inputs, labels, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train_model():\n",
    "    model = DeepCrossNetwork(num_users, num_items, embedding_size, dropout_rate)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    inputs = np.stack([combined_df['User ID'].values, combined_df['News ID'].values], axis=1)\n",
    "    ratings = combined_df['rating'].values\n",
    "    dataset = create_dataset(inputs, ratings, batch_size)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        print(f\"Starting epoch {epoch + 1}\")\n",
    "        for batch_inputs, batch_labels in dataset:\n",
    "            loss = train_model_step(batch_inputs, batch_labels, model, optimizer)\n",
    "            print(f\"Batch loss: {loss.numpy().mean()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model()\n",
    "\n",
    "# --------- Step 8: Save Models and Encoders ---------\n",
    "print(\"Saving models and encoders...\")\n",
    "model.save(os.path.join(save_dir, 'deep_cross_network'))\n",
    "joblib.dump(user_map, os.path.join(save_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(news_map, os.path.join(save_dir, 'news_encoder.pkl'))\n",
    "print(\"Model and encoders saved successfully.\")\n",
    "\n",
    "# --------- Step 9: Train and Save Word2Vec and KMeans Models ---------\n",
    "print(\"Training Word2Vec model...\")\n",
    "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=2, workers=multiprocessing.cpu_count())\n",
    "word2vec_model.build_vocab(news_df['Text'].str.split())\n",
    "word2vec_model.train(news_df['Text'].str.split(), total_examples=len(news_df), epochs=5)\n",
    "\n",
    "print(\"Training KMeans model...\")\n",
    "mini_batch_kmeans = MiniBatchKMeans(n_clusters=70, batch_size=500, n_init='auto')\n",
    "news_embeddings = np.vstack(news_df['Text'].apply(\n",
    "    lambda x: np.mean([word for word in x.split() if word in word2vec_model.wv], axis=0)\n",
    "))\n",
    "mini_batch_kmeans.fit(news_embeddings)\n",
    "\n",
    "word2vec_model.save(os.path.join(save_dir, 'word2vec_model.model'))\n",
    "joblib.dump(mini_batch_kmeans, os.path.join(save_dir, 'mini_batch_kmeans_news_model.pkl'))\n",
    "print(\"All models saved successfully.\")\n",
    "\n",
    "\n",
    "#--------end--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ddd24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
