{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import joblib\n",
    "from functools import partial\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import lit\n",
    "import os\n",
    "\n",
    "# ----------- Initialize Spark Session -----------\n",
    "spark = SparkSession.builder.appName(\"NewsRecommendationGUI\").getOrCreate()\n",
    "\n",
    "# ----------- Step 1: Load Pre-trained Models -----------\n",
    "\n",
    "# Load pre-trained models from the saved directory\n",
    "save_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Machine Learning Codes/Trained Models'\n",
    "\n",
    "# Load the TF-IDF Vectorizer model\n",
    "vectorizer = joblib.load(f'{save_dir}/tfidf_vectorizer.pkl')\n",
    "print(\"TF-IDF Vectorizer model loaded successfully.\")\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = Word2Vec.load(f'{save_dir}/word2vec.model')\n",
    "print(\"Word2Vec model loaded successfully.\")\n",
    "\n",
    "# Load the KMeans clustering model\n",
    "kmeans_news = joblib.load(f'{save_dir}/kmeans_news_model.pkl')\n",
    "print(\"KMeans clustering model loaded successfully.\")\n",
    "\n",
    "# Load the ALS model using Spark\n",
    "spark = SparkSession.builder.appName(\"NewsRecommendationALS\").getOrCreate()\n",
    "als_model_path = f'{save_dir}/best_als_model'\n",
    "best_als_model = ALSModel.load(als_model_path)\n",
    "print(\"ALS model loaded successfully.\")\n",
    "\n",
    "# ----------- Step 2: Load Validation and Test Datasets -----------\n",
    "\n",
    "validation_news_df = pd.read_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_dev/Cleaned Dataset/News_cleaned.csv', sep=',', names=[\n",
    "    \"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Entities Mentioned\", \"Entities in Abstract\"\n",
    "])\n",
    "\n",
    "validation_behavior_df = pd.read_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_dev/Cleaned Dataset/cleaned_behavior_dataset.csv', sep=',', names=[\n",
    "    \"Impression ID\", \"User ID\", \"Timestamp\", \"Displayed News List\", \"Impression List (Clicked Status)\",\n",
    "    \"Impression Dictionary\", \"Clicked News IDs\", \"Not-Clicked News IDs\"\n",
    "])\n",
    "\n",
    "test_news_df = pd.read_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_test/Cleaned Dataset/News_cleaned.csv', sep=',', names=[\n",
    "    \"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Entities Mentioned\", \"Entities in Abstract\"\n",
    "])\n",
    "\n",
    "test_behavior_df = pd.read_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_test/Cleaned Dataset/cleaned_behavior_dataset.csv', sep=',', names=[\n",
    "    \"Impression ID\", \"User ID\", \"Timestamp\", \"Displayed News List\", \"Impression List (Clicked Status)\",\n",
    "    \"Impression Dictionary\", \"Clicked News IDs\", \"Not-Clicked News IDs\"\n",
    "])\n",
    "\n",
    "# ----------- Step 3: Preprocess Validation and Test Datasets -----------\n",
    "\n",
    "def preprocess_datasets(news_df, behavior_df, vectorizer, word2vec_model, kmeans_news):\n",
    "    # Preprocess the news dataset\n",
    "    news_df['Text'] = news_df['Category'] + \" \" + news_df['Subcategory'] + \" \" + news_df['Title'] + \" \" + news_df['Abstract']\n",
    "\n",
    "    # Transform text using the pre-trained TF-IDF vectorizer\n",
    "    tfidf_matrix = vectorizer.transform(news_df['Text'])\n",
    "    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Word2Vec embeddings for news\n",
    "    def get_article_embedding(text):\n",
    "        words = text.split()\n",
    "        word_vecs = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "        return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100)\n",
    "\n",
    "    news_df['Article Embedding'] = news_df['Text'].apply(get_article_embedding)\n",
    "\n",
    "    # Assign news clusters using the pre-trained KMeans model\n",
    "    news_embeddings = np.vstack(news_df['Article Embedding'].values)\n",
    "    news_df['News Cluster'] = kmeans_news.predict(news_embeddings)\n",
    "\n",
    "    # Prepare behavior data for ALS: Combine Clicked and Not-Clicked News IDs\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"NewsRecommendationValidation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Spark session initialized for validation.\")\n",
    "\n",
    "    # Prepare Clicked News Data\n",
    "    clicked_df = behavior_df[['User ID', 'Clicked News IDs']].copy()\n",
    "    clicked_df = clicked_df.assign(Clicked_News=clicked_df['Clicked News IDs'].str.split(',')).explode('Clicked_News').drop(columns='Clicked News IDs')\n",
    "    clicked_df['Clicked_News'] = clicked_df['Clicked_News'].astype(str)\n",
    "\n",
    "    # Prepare Not-Clicked News Data\n",
    "    not_clicked_df = behavior_df[['User ID', 'Not-Clicked News IDs']].copy()\n",
    "    not_clicked_df = not_clicked_df.assign(Not_Clicked_News=not_clicked_df['Not-Clicked News IDs'].str.split(',')).explode('Not_Clicked_News').drop(columns='Not-Clicked News IDs')\n",
    "    not_clicked_df['Not_Clicked_News'] = not_clicked_df['Not_Clicked_News'].astype(str)\n",
    "\n",
    "    # Add a rating of 1.0 for clicked news (positive interactions)\n",
    "    clicked_df['rating'] = 1.0\n",
    "\n",
    "    # Add a rating of 0.0 for not-clicked news (negative interactions)\n",
    "    not_clicked_df['rating'] = 0.0\n",
    "\n",
    "    # Combine both clicked and not-clicked data\n",
    "    combined_behavior_df = pd.concat([clicked_df.rename(columns={'Clicked_News': 'News ID'}), \n",
    "                                  not_clicked_df.rename(columns={'Not_Clicked_News': 'News ID'})], ignore_index=True)\n",
    "\n",
    "    # Ensure 'News ID' is a string for consistency\n",
    "    combined_behavior_df['News ID'] = combined_behavior_df['News ID'].astype(str)\n",
    "\n",
    "    # Convert combined behavior data to Spark DataFrame\n",
    "    behavior_spark_df = spark.createDataFrame(combined_behavior_df)\n",
    "\n",
    "    print(\"Behavior data prepared for ALS, including both clicked and not-clicked interactions.\")\n",
    "    \n",
    "    return news_df, behavior_df, behavior_spark_df, cosine_sim_matrix\n",
    "\n",
    "\n",
    "# ----------- Step 4: Recommendation Generation Function (Hybrid) -----------\n",
    "\n",
    "def hybrid_recommendation(user_id, behavior_df, top_n_per_component=15):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation function that takes into account the user's clicked news articles\n",
    "    and generates recommendations based on collaborative filtering, content-based filtering,\n",
    "    and clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: ID of the user for whom to generate recommendations.\n",
    "    - behavior_df: The behavior dataset that contains 'Clicked News IDs'.\n",
    "    - top_n_per_component: Number of recommendations to generate per technique.\n",
    "\n",
    "    Returns:\n",
    "    - List of recommendations combining collaborative, content-based, and clustering techniques.\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Step 1: Get Clicked News IDs for the User\n",
    "    user_clicked_news = behavior_df[behavior_df['User ID'] == user_id]['Clicked News IDs'].tolist()\n",
    "    \n",
    "    # If the user has not clicked on any news, handle it (new user scenario)\n",
    "    if not user_clicked_news:\n",
    "        print(f\"No clicked articles found for User {user_id}. Providing recommendations without historical data.\")\n",
    "        return generate_new_user_recommendations(user_id)  # Function to handle new users\n",
    "    \n",
    "    # Otherwise, use the clicked news articles for recommendations\n",
    "    user_clicked_news = user_clicked_news[0].split(',')  # Convert the comma-separated string into a list of news IDs\n",
    "    \n",
    "    # Step 2: Collaborative Filtering Recommendations (based on user_id)\n",
    "    collab_recs = collaborative_recommendations(user_id, top_n=top_n_per_component)\n",
    "    recommendations.extend(collab_recs)\n",
    "    \n",
    "    # Step 3: Content-Based Recommendations (based on user's clicked news)\n",
    "    for news_id in user_clicked_news:\n",
    "        content_recs = content_based_recommendations(news_id, top_n=top_n_per_component)\n",
    "        recommendations.extend(content_recs)\n",
    "    \n",
    "    # Step 4: Clustering-Based Recommendations (based on user's clicked news)\n",
    "    for news_id in user_clicked_news:\n",
    "        cluster_recs = get_articles_in_same_cluster(news_id, top_n=top_n_per_component)\n",
    "        recommendations.extend(cluster_recs)\n",
    "\n",
    "   # Step 5: Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_recs = []\n",
    "    for rec in recommendations:\n",
    "        if rec not in seen:\n",
    "            seen.add(rec)\n",
    "            unique_recs.append(rec)\n",
    "\n",
    "    # Return all unique recommendations\n",
    "    return unique_recs\n",
    "\n",
    "#--------Side Step:  Function to generate recommendations for new users--------\n",
    "\n",
    "def generate_new_user_recommendations(user_id, top_n=15):\n",
    "    \"\"\"\n",
    "    Generate recommendations for new users based on popular categories, content-based filtering,\n",
    "    or cluster-level information.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: ID of the new user.\n",
    "    - top_n: Number of recommendations to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recommended news articles for the new user.\n",
    "    \"\"\"\n",
    "    # For new users, we can recommend based on popularity, top clusters, or random articles\n",
    "    print(f\"Generating recommendations for new user (User ID: {user_id})\")\n",
    "    \n",
    "    # Example: Recommend top articles from popular clusters or categories\n",
    "    top_news_in_clusters = news_df.groupby('News Cluster').apply(lambda x: x.sample(n=1)).reset_index(drop=True)\n",
    "    \n",
    "    # Select top articles for new users based on clusters or categories\n",
    "    recommendations = top_news_in_clusters['News ID'].head(top_n).tolist()\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "#--------Side Step:  Function to generate recommendations based on collaborative filtering--------\n",
    "\n",
    "def collaborative_recommendations(user_id, top_n=15):\n",
    "    \"\"\"\n",
    "    Generate top N collaborative filtering recommendations for the given user using the pre-trained ALS model.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: The ID of the user for whom to generate recommendations.\n",
    "    - top_n: The number of recommendations to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recommended news IDs based on ALS predictions.\n",
    "    \"\"\"\n",
    "    # Convert user_id to a Spark DataFrame so that it can be used for prediction\n",
    "    user_df = spark.createDataFrame([(user_id,)], [\"User ID\"])\n",
    "\n",
    "    # Generate predictions using the ALS model (predict ratings for all news items)\n",
    "    user_recommendations = best_als_model.recommendForUserSubset(user_df, top_n)\n",
    "\n",
    "    # Extract the recommended news IDs from the model's output\n",
    "    recommendations = user_recommendations.collect()[0].recommendations\n",
    "    recommended_news_ids = [row['News ID'] for row in recommendations]\n",
    "\n",
    "    return recommended_news_ids\n",
    "\n",
    "#--------Side Step:  Function to generate recommendations content_based filtering--------\n",
    "\n",
    "def content_based_recommendations(news_id, top_n=15):\n",
    "    \"\"\"\n",
    "    Generate top N content-based recommendations for a given news article using cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - news_id: The ID of the news article based on which recommendations will be generated.\n",
    "    - top_n: The number of recommendations to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recommended news IDs based on content similarity.\n",
    "    \"\"\"\n",
    "    # Find the index of the given news_id in the dataset\n",
    "    idx = news_df[news_df['News ID'] == news_id].index[0]\n",
    "    \n",
    "    # Compute the pairwise cosine similarities for the given news article using the pre-trained TF-IDF matrix\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "    \n",
    "    # Sort the articles based on similarity scores in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top N most similar articles (excluding the given article itself)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    \n",
    "    # Extract the indices of the recommended news articles\n",
    "    recommended_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Get the corresponding news IDs\n",
    "    recommended_news_ids = news_df.iloc[recommended_indices]['News ID'].tolist()\n",
    "    \n",
    "    return recommended_news_ids\n",
    "\n",
    "#--------Side Step:  Function to generate recommendations culster_based--------\n",
    "\n",
    "def get_articles_in_same_cluster(news_id, top_n=15):\n",
    "    \"\"\"\n",
    "    Generate top N clustering-based recommendations for a given news article by retrieving articles\n",
    "    from the same cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - news_id: The ID of the news article based on which recommendations will be generated.\n",
    "    - top_n: The number of recommendations to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - List of recommended news IDs from the same cluster.\n",
    "    \"\"\"\n",
    "    # Find the cluster label of the given news_id (from the preprocessed dataset)\n",
    "    cluster_label = news_df[news_df['News ID'] == news_id]['News Cluster'].values[0]  # Use preprocessed 'News Cluster' labels\n",
    "    \n",
    "    # Get all articles in the same cluster\n",
    "    articles_in_cluster = news_df[news_df['News Cluster'] == cluster_label]  # Select articles from the same cluster\n",
    "    \n",
    "    # Remove the article itself from the list of recommendations\n",
    "    articles_in_cluster = articles_in_cluster[articles_in_cluster['News ID'] != news_id]\n",
    "    \n",
    "    # Randomly sample top_n articles from the same cluster\n",
    "    recommended_news_ids = articles_in_cluster['News ID'].sample(n=top_n, replace=False).tolist()\n",
    "    \n",
    "    return recommended_news_ids\n",
    "\n",
    "\n",
    "# ----------- Step 5: Evaluation Metrics -----------\n",
    "\n",
    "def evaluate_recommendations(recommended_clicks, actual_clicks, recommended_not_clicks, actual_not_clicks):\n",
    "    \"\"\"\n",
    "    Evaluate the predicted recommendations against the actual clicked and not-clicked news.\n",
    "\n",
    "    Parameters:\n",
    "    - recommended_clicks: Set of news IDs that were predicted as clicked.\n",
    "    - actual_clicks: Set of news IDs that were actually clicked.\n",
    "    - recommended_not_clicks: Set of news IDs that were predicted as not-clicked.\n",
    "    - actual_not_clicks: Set of news IDs that were actually not clicked.\n",
    "\n",
    "    Returns:\n",
    "    - Precision, Recall, and F1 for both clicked and not-clicked predictions.\n",
    "    \"\"\"\n",
    "    # Precision, recall, and F1 for clicked news\n",
    "    precision_clicks = len(recommended_clicks.intersection(actual_clicks)) / len(recommended_clicks) if recommended_clicks else 0\n",
    "    recall_clicks = len(recommended_clicks.intersection(actual_clicks)) / len(actual_clicks) if actual_clicks else 0\n",
    "    f1_clicks = 2 * (precision_clicks * recall_clicks) / (precision_clicks + recall_clicks) if (precision_clicks + recall_clicks) > 0 else 0\n",
    "\n",
    "    # Precision, recall, and F1 for not-clicked news\n",
    "    precision_not_clicks = len(recommended_not_clicks.intersection(actual_not_clicks)) / len(recommended_not_clicks) if recommended_not_clicks else 0\n",
    "    recall_not_clicks = len(recommended_not_clicks.intersection(actual_not_clicks)) / len(actual_not_clicks) if actual_not_clicks else 0\n",
    "    f1_not_clicks = 2 * (precision_not_clicks * recall_not_clicks) / (precision_not_clicks + recall_not_clicks) if (precision_not_clicks + recall_not_clicks) > 0 else 0\n",
    "\n",
    "    return precision_clicks, recall_clicks, f1_clicks, precision_not_clicks, recall_not_clicks, f1_not_clicks\n",
    "\n",
    "\n",
    "# ----------- Step 6: Predict and Evaluate on Validation Set -----------\n",
    "\n",
    "def predict_clicks_for_user(user_id, displayed_news_list, behavior_df, top_n_per_component=5):\n",
    "    \"\"\"\n",
    "    Predict whether a user will click on the news articles in their displayed news list.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: ID of the user\n",
    "    - displayed_news_list: List of news articles displayed to the user (from Displayed News List)\n",
    "    \n",
    "    Returns:\n",
    "    - clicked_news_ids: List of news IDs predicted to be clicked by the user\n",
    "    - not_clicked_news_ids: List of news IDs predicted to not be clicked by the user\n",
    "    \"\"\"\n",
    "    clicked_news_ids = []\n",
    "    not_clicked_news_ids = []\n",
    "    \n",
    "    # Go through each news ID in the displayed news list and predict whether the user will click\n",
    "    for news_id in displayed_news_list:\n",
    "        # For each news article, predict the click using hybrid recommendation function\n",
    "        recommendations = hybrid_recommendation(user_id=user_id, behavior_df=behavior_df, top_n_per_component=top_n_per_component)\n",
    "        \n",
    "        # If the news_id is part of the top recommendations, assume it is clicked\n",
    "        if news_id in recommendations:\n",
    "            clicked_news_ids.append(news_id)\n",
    "        else:\n",
    "            not_clicked_news_ids.append(news_id)\n",
    "    \n",
    "    return clicked_news_ids, not_clicked_news_ids\n",
    "\n",
    "\n",
    "def validate_predictions_on_validation_set(validation_behavior_df, validation_news_df):\n",
    "    \"\"\"\n",
    "    Predict the clicked and not-clicked news for the users in the validation dataset, and compare\n",
    "    the predictions to the actual clicked and not-clicked news for evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - validation_behavior_df: DataFrame containing the validation behavior dataset.\n",
    "    - validation_news_df: DataFrame containing the validation news dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Average precision, recall, and F1 scores for clicked and not-clicked news.\n",
    "    \"\"\"\n",
    "    precision_clicks_scores_val = []\n",
    "    recall_clicks_scores_val = []\n",
    "    f1_clicks_scores_val = []\n",
    "\n",
    "    precision_not_clicks_scores_val = []\n",
    "    recall_not_clicks_scores_val = []\n",
    "    f1_not_clicks_scores_val = []\n",
    "\n",
    "    for index, row in validation_behavior_df.iterrows():\n",
    "        user_id = row['User ID']\n",
    "        displayed_news_list = row['Displayed News List'].split(',')  # Convert displayed news list to a list\n",
    "        \n",
    "        # Predict clicks for this user and their displayed news\n",
    "        predicted_clicked_news_ids, predicted_not_clicked_news_ids = predict_clicks_for_user(user_id, displayed_news_list, validation_behavior_df)\n",
    "        \n",
    "        # Get actual clicked and not-clicked news from the validation set\n",
    "        actual_clicked_news_ids = row['Clicked News IDs'].split(',') if row['Clicked News IDs'] else []\n",
    "        actual_not_clicked_news_ids = row['Not-Clicked News IDs'].split(',') if row['Not-Clicked News IDs'] else []\n",
    "        \n",
    "        # Evaluate the predicted recommendations against the actual results\n",
    "        precision_clicks, recall_clicks, f1_clicks, precision_not_clicks, recall_not_clicks, f1_not_clicks = evaluate_recommendations(\n",
    "            set(predicted_clicked_news_ids), set(actual_clicked_news_ids),\n",
    "            set(predicted_not_clicked_news_ids), set(actual_not_clicked_news_ids)\n",
    "        )\n",
    "        \n",
    "        # Append the results to the respective lists\n",
    "        precision_clicks_scores_val.append(precision_clicks)\n",
    "        recall_clicks_scores_val.append(recall_clicks)\n",
    "        f1_clicks_scores_val.append(f1_clicks)\n",
    "\n",
    "        precision_not_clicks_scores_val.append(precision_not_clicks)\n",
    "        recall_not_clicks_scores_val.append(recall_not_clicks)\n",
    "        f1_not_clicks_scores_val.append(f1_not_clicks)\n",
    "\n",
    "    # Calculate average metrics for validation set\n",
    "    avg_precision_clicks_val = np.mean(precision_clicks_scores_val)\n",
    "    avg_recall_clicks_val = np.mean(recall_clicks_scores_val)\n",
    "    avg_f1_clicks_val = np.mean(f1_clicks_scores_val)\n",
    "\n",
    "    avg_precision_not_clicks_val = np.mean(precision_not_clicks_scores_val)\n",
    "    avg_recall_not_clicks_val = np.mean(recall_not_clicks_scores_val)\n",
    "    avg_f1_not_clicks_val = np.mean(f1_not_clicks_scores_val)\n",
    "\n",
    "    return (avg_precision_clicks_val, avg_recall_clicks_val, avg_f1_clicks_val, \n",
    "            avg_precision_not_clicks_val, avg_recall_not_clicks_val, avg_f1_not_clicks_val)\n",
    "\n",
    "# Preprocess Validation dataSets\n",
    "validation_news_df, validation_behavior_df, behavior_spark_df_val, cosine_sim_matrix_val = preprocess_datasets(\n",
    "    validation_news_df, validation_behavior_df, vectorizer, word2vec_model, kmeans_news\n",
    ")\n",
    "\n",
    "# Run the validation process for all users in the validation set\n",
    "(avg_precision_clicks_val, avg_recall_clicks_val, avg_f1_clicks_val, \n",
    " avg_precision_not_clicks_val, avg_recall_not_clicks_val, avg_f1_not_clicks_val) = validate_predictions_on_validation_set(\n",
    "    validation_behavior_df, validation_news_df\n",
    ")\n",
    "\n",
    "# Display validation results\n",
    "print(f'Validation Precision (Clicked): {avg_precision_clicks_val:.4f}')\n",
    "print(f'Validation Recall (Clicked): {avg_recall_clicks_val:.4f}')\n",
    "print(f'Validation F1-Score (Clicked): {avg_f1_clicks_val:.4f}')\n",
    "\n",
    "print(f'Validation Precision (Not-Clicked): {avg_precision_not_clicks_val:.4f}')\n",
    "print(f'Validation Recall (Not-Clicked): {avg_recall_not_clicks_val:.4f}')\n",
    "print(f'Validation F1-Score (Not-Clicked): {avg_f1_not_clicks_val:.4f}')\n",
    "\n",
    "\n",
    "# ----------- Step 7: Testing Phase -----------\n",
    "\n",
    "def predict_clicks_for_user(user_id, displayed_news_list, top_n_per_component=5):\n",
    "    \"\"\"\n",
    "    Predict whether a user will click on the news articles in their displayed news list.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: ID of the user\n",
    "    - displayed_news_list: List of news articles displayed to the user (from Displayed News List)\n",
    "    \n",
    "    Returns:\n",
    "    - clicked_news_ids: List of news IDs predicted to be clicked by the user\n",
    "    - not_clicked_news_ids: List of news IDs predicted to not be clicked by the user\n",
    "    \"\"\"\n",
    "    clicked_news_ids = []\n",
    "    not_clicked_news_ids = []\n",
    "    \n",
    "    # Go through each news ID in the displayed news list and predict whether the user will click\n",
    "    for news_id in displayed_news_list:\n",
    "        # For each news article, we can predict the click using our hybrid recommendation function\n",
    "        recommendations = hybrid_recommendation(user_id=user_id, news_id=news_id, top_n_per_component=top_n_per_component)\n",
    "        \n",
    "        # If the news_id is part of the top recommendations, assume it is clicked\n",
    "        if news_id in recommendations:\n",
    "            clicked_news_ids.append(news_id)\n",
    "        else:\n",
    "            not_clicked_news_ids.append(news_id)\n",
    "    \n",
    "    return clicked_news_ids, not_clicked_news_ids\n",
    "\n",
    "def test_click_predictions(test_behavior_df):\n",
    "    \"\"\"\n",
    "    Apply the click prediction process to all users in the test dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_behavior_df: DataFrame containing the test behavior dataset\n",
    "    \n",
    "    Returns:\n",
    "    - Updated test_behavior_df with predicted Clicked News IDs and Not-Clicked News IDs.\n",
    "    \"\"\"\n",
    "    for index, row in test_behavior_df.iterrows():\n",
    "        user_id = row['User ID']\n",
    "        displayed_news_list = row['Displayed News List'].split(',')  # Convert displayed news list to a list\n",
    "        \n",
    "        # Predict clicks for this user and their displayed news\n",
    "        clicked_news_ids, not_clicked_news_ids = predict_clicks_for_user(user_id, displayed_news_list)\n",
    "        \n",
    "        # Update the behavior dataframe with the predicted clicks\n",
    "        test_behavior_df.at[index, 'Clicked News IDs'] = ','.join(clicked_news_ids) if clicked_news_ids else ''\n",
    "        test_behavior_df.at[index, 'Not-Clicked News IDs'] = ','.join(not_clicked_news_ids) if not_clicked_news_ids else ''\n",
    "    \n",
    "    return test_behavior_df\n",
    "\n",
    "# Preprocess Test Set\n",
    "test_news_df, test_behavior_df, behavior_spark_df_test, cosine_sim_matrix_test = preprocess_datasets(\n",
    "    test_news_df, test_behavior_df, vectorizer, word2vec_model, kmeans_news\n",
    ")\n",
    "\n",
    "# Run the prediction process for all users in the test set\n",
    "updated_test_behavior_df = test_click_predictions(test_behavior_df)\n",
    "\n",
    "# Save the updated test behavior dataset with the predictions\n",
    "updated_test_behavior_df.to_csv('/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project/Data/MINDlarge_test/Behavior Predictions Test Results/updated_test_behavior_dataset.csv', index=False)\n",
    "\n",
    "print(\"Testing completed and predictions saved to 'updated_test_behavior_dataset.csv'.\")\n",
    "\n",
    "\n",
    "# ----------- end -----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
