{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9494f90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded.\n",
      "Standardizing News IDs...\n",
      "Combining text columns...\n",
      "Step 1: Fine-tuning BERT for Masked Language Modeling...\n",
      "Converting dataset...\n",
      "Loading existing BERT tokenizer...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d363e210a964ac59674f33e83909209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed.\n",
      "Initializing data collator for MLM...\n",
      "Loading existing BERT model for Masked Language Modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Machine Learning models/Trained Models/Version7.7/bert_mlm_model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT fine-tuning completed successfully.\n",
      "Step 2: Generating embeddings...\n",
      "Loading BERT model from /Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Machine Learning models/Trained Models/Version7.7/bert_mlm_model...\n",
      "BERT model loaded successfully.\n",
      "Loading tokenizer from /Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Machine Learning models/Trained Models/Version7.7/bert_tokenizer...\n",
      "Tokenizer loaded successfully.\n",
      "Generating embeddings for the dataset...\n",
      "Generating embeddings in batches...\n",
      "Processed batch 1 of 2138\n",
      "Processed batch 2 of 2138\n",
      "Processed batch 3 of 2138\n",
      "Processed batch 4 of 2138\n",
      "Processed batch 5 of 2138\n",
      "Processed batch 6 of 2138\n",
      "Processed batch 7 of 2138\n",
      "Processed batch 8 of 2138\n",
      "Processed batch 9 of 2138\n",
      "Processed batch 10 of 2138\n",
      "Processed batch 11 of 2138\n",
      "Processed batch 12 of 2138\n",
      "Processed batch 13 of 2138\n",
      "Processed batch 14 of 2138\n",
      "Processed batch 15 of 2138\n",
      "Processed batch 16 of 2138\n",
      "Processed batch 17 of 2138\n",
      "Processed batch 18 of 2138\n",
      "Processed batch 19 of 2138\n",
      "Processed batch 20 of 2138\n",
      "Processed batch 21 of 2138\n",
      "Processed batch 22 of 2138\n",
      "Processed batch 23 of 2138\n",
      "Processed batch 24 of 2138\n",
      "Processed batch 25 of 2138\n",
      "Processed batch 26 of 2138\n",
      "Processed batch 27 of 2138\n",
      "Processed batch 28 of 2138\n",
      "Processed batch 29 of 2138\n",
      "Processed batch 30 of 2138\n",
      "Processed batch 31 of 2138\n",
      "Processed batch 32 of 2138\n",
      "Processed batch 33 of 2138\n",
      "Processed batch 34 of 2138\n",
      "Processed batch 35 of 2138\n",
      "Processed batch 36 of 2138\n",
      "Processed batch 37 of 2138\n",
      "Processed batch 38 of 2138\n",
      "Processed batch 39 of 2138\n",
      "Processed batch 40 of 2138\n",
      "Processed batch 41 of 2138\n",
      "Processed batch 42 of 2138\n",
      "Processed batch 43 of 2138\n",
      "Processed batch 44 of 2138\n",
      "Processed batch 45 of 2138\n",
      "Processed batch 46 of 2138\n",
      "Processed batch 47 of 2138\n",
      "Processed batch 48 of 2138\n",
      "Processed batch 49 of 2138\n",
      "Processed batch 50 of 2138\n",
      "Processed batch 51 of 2138\n",
      "Processed batch 52 of 2138\n",
      "Processed batch 53 of 2138\n",
      "Processed batch 54 of 2138\n",
      "Processed batch 55 of 2138\n",
      "Processed batch 56 of 2138\n",
      "Processed batch 57 of 2138\n",
      "Processed batch 58 of 2138\n",
      "Processed batch 59 of 2138\n",
      "Processed batch 60 of 2138\n",
      "Processed batch 61 of 2138\n",
      "Processed batch 62 of 2138\n",
      "Processed batch 63 of 2138\n",
      "Processed batch 64 of 2138\n",
      "Processed batch 65 of 2138\n",
      "Processed batch 66 of 2138\n",
      "Processed batch 67 of 2138\n",
      "Processed batch 68 of 2138\n",
      "Processed batch 69 of 2138\n",
      "Processed batch 70 of 2138\n",
      "Processed batch 71 of 2138\n",
      "Processed batch 72 of 2138\n",
      "Processed batch 73 of 2138\n",
      "Processed batch 74 of 2138\n",
      "Processed batch 75 of 2138\n",
      "Processed batch 76 of 2138\n",
      "Processed batch 77 of 2138\n",
      "Processed batch 78 of 2138\n",
      "Processed batch 79 of 2138\n",
      "Processed batch 80 of 2138\n",
      "Processed batch 81 of 2138\n",
      "Processed batch 82 of 2138\n",
      "Processed batch 83 of 2138\n",
      "Processed batch 84 of 2138\n",
      "Processed batch 85 of 2138\n",
      "Processed batch 86 of 2138\n",
      "Processed batch 87 of 2138\n",
      "Processed batch 88 of 2138\n",
      "Processed batch 89 of 2138\n",
      "Processed batch 90 of 2138\n",
      "Processed batch 91 of 2138\n",
      "Processed batch 92 of 2138\n",
      "Processed batch 93 of 2138\n",
      "Processed batch 94 of 2138\n",
      "Processed batch 95 of 2138\n",
      "Processed batch 96 of 2138\n",
      "Processed batch 97 of 2138\n",
      "Processed batch 98 of 2138\n",
      "Processed batch 99 of 2138\n",
      "Processed batch 100 of 2138\n",
      "Processed batch 101 of 2138\n",
      "Processed batch 102 of 2138\n",
      "Processed batch 103 of 2138\n",
      "Processed batch 104 of 2138\n",
      "Processed batch 105 of 2138\n",
      "Processed batch 106 of 2138\n",
      "Processed batch 107 of 2138\n",
      "Processed batch 108 of 2138\n",
      "Processed batch 109 of 2138\n",
      "Processed batch 110 of 2138\n",
      "Processed batch 111 of 2138\n",
      "Processed batch 112 of 2138\n",
      "Processed batch 113 of 2138\n",
      "Processed batch 114 of 2138\n",
      "Processed batch 115 of 2138\n",
      "Processed batch 116 of 2138\n",
      "Processed batch 117 of 2138\n",
      "Processed batch 118 of 2138\n",
      "Processed batch 119 of 2138\n",
      "Processed batch 120 of 2138\n",
      "Processed batch 121 of 2138\n",
      "Processed batch 122 of 2138\n",
      "Processed batch 123 of 2138\n",
      "Processed batch 124 of 2138\n",
      "Processed batch 125 of 2138\n",
      "Processed batch 126 of 2138\n",
      "Processed batch 127 of 2138\n",
      "Processed batch 128 of 2138\n",
      "Processed batch 129 of 2138\n",
      "Processed batch 130 of 2138\n",
      "Processed batch 131 of 2138\n",
      "Processed batch 132 of 2138\n",
      "Processed batch 133 of 2138\n",
      "Processed batch 134 of 2138\n",
      "Processed batch 135 of 2138\n",
      "Processed batch 136 of 2138\n",
      "Processed batch 137 of 2138\n",
      "Processed batch 138 of 2138\n",
      "Processed batch 139 of 2138\n",
      "Processed batch 140 of 2138\n",
      "Processed batch 141 of 2138\n",
      "Processed batch 142 of 2138\n",
      "Processed batch 143 of 2138\n",
      "Processed batch 144 of 2138\n",
      "Processed batch 145 of 2138\n",
      "Processed batch 146 of 2138\n",
      "Processed batch 147 of 2138\n",
      "Processed batch 148 of 2138\n",
      "Processed batch 149 of 2138\n",
      "Processed batch 150 of 2138\n",
      "Processed batch 151 of 2138\n",
      "Processed batch 152 of 2138\n",
      "Processed batch 153 of 2138\n",
      "Processed batch 154 of 2138\n",
      "Processed batch 155 of 2138\n",
      "Processed batch 156 of 2138\n",
      "Processed batch 157 of 2138\n",
      "Processed batch 158 of 2138\n",
      "Processed batch 159 of 2138\n",
      "Processed batch 160 of 2138\n",
      "Processed batch 161 of 2138\n",
      "Processed batch 162 of 2138\n",
      "Processed batch 163 of 2138\n",
      "Processed batch 164 of 2138\n",
      "Processed batch 165 of 2138\n",
      "Processed batch 166 of 2138\n",
      "Processed batch 167 of 2138\n",
      "Processed batch 168 of 2138\n",
      "Processed batch 169 of 2138\n",
      "Processed batch 170 of 2138\n",
      "Processed batch 171 of 2138\n",
      "Processed batch 172 of 2138\n",
      "Processed batch 173 of 2138\n",
      "Processed batch 174 of 2138\n",
      "Processed batch 175 of 2138\n",
      "Processed batch 176 of 2138\n",
      "Processed batch 177 of 2138\n",
      "Processed batch 178 of 2138\n",
      "Processed batch 179 of 2138\n",
      "Processed batch 180 of 2138\n",
      "Processed batch 181 of 2138\n",
      "Processed batch 182 of 2138\n",
      "Processed batch 183 of 2138\n",
      "Processed batch 184 of 2138\n",
      "Processed batch 185 of 2138\n",
      "Processed batch 186 of 2138\n",
      "Processed batch 187 of 2138\n",
      "Processed batch 188 of 2138\n",
      "Processed batch 189 of 2138\n",
      "Processed batch 190 of 2138\n",
      "Processed batch 191 of 2138\n",
      "Processed batch 192 of 2138\n",
      "Processed batch 193 of 2138\n",
      "Processed batch 194 of 2138\n",
      "Processed batch 195 of 2138\n",
      "Processed batch 196 of 2138\n",
      "Processed batch 197 of 2138\n",
      "Processed batch 198 of 2138\n",
      "Processed batch 199 of 2138\n",
      "Processed batch 200 of 2138\n",
      "Processed batch 201 of 2138\n",
      "Processed batch 202 of 2138\n",
      "Processed batch 203 of 2138\n",
      "Processed batch 204 of 2138\n",
      "Processed batch 205 of 2138\n",
      "Processed batch 206 of 2138\n",
      "Processed batch 207 of 2138\n",
      "Processed batch 208 of 2138\n",
      "Processed batch 209 of 2138\n",
      "Processed batch 210 of 2138\n",
      "Processed batch 211 of 2138\n",
      "Processed batch 212 of 2138\n",
      "Processed batch 213 of 2138\n",
      "Processed batch 214 of 2138\n",
      "Processed batch 215 of 2138\n",
      "Processed batch 216 of 2138\n",
      "Processed batch 217 of 2138\n",
      "Processed batch 218 of 2138\n",
      "Processed batch 219 of 2138\n",
      "Processed batch 220 of 2138\n",
      "Processed batch 221 of 2138\n",
      "Processed batch 222 of 2138\n",
      "Processed batch 223 of 2138\n",
      "Processed batch 224 of 2138\n",
      "Processed batch 225 of 2138\n",
      "Processed batch 226 of 2138\n",
      "Processed batch 227 of 2138\n",
      "Processed batch 228 of 2138\n",
      "Processed batch 229 of 2138\n",
      "Processed batch 230 of 2138\n",
      "Processed batch 231 of 2138\n",
      "Processed batch 232 of 2138\n",
      "Processed batch 233 of 2138\n",
      "Processed batch 234 of 2138\n",
      "Processed batch 235 of 2138\n",
      "Processed batch 236 of 2138\n",
      "Processed batch 237 of 2138\n",
      "Processed batch 238 of 2138\n",
      "Processed batch 239 of 2138\n",
      "Processed batch 240 of 2138\n",
      "Processed batch 241 of 2138\n",
      "Processed batch 242 of 2138\n",
      "Processed batch 243 of 2138\n",
      "Processed batch 244 of 2138\n",
      "Processed batch 245 of 2138\n",
      "Processed batch 246 of 2138\n",
      "Processed batch 247 of 2138\n",
      "Processed batch 248 of 2138\n",
      "Processed batch 249 of 2138\n",
      "Processed batch 250 of 2138\n",
      "Processed batch 251 of 2138\n",
      "Processed batch 252 of 2138\n",
      "Processed batch 253 of 2138\n",
      "Processed batch 254 of 2138\n",
      "Processed batch 255 of 2138\n",
      "Processed batch 256 of 2138\n",
      "Processed batch 257 of 2138\n",
      "Processed batch 258 of 2138\n",
      "Processed batch 259 of 2138\n",
      "Processed batch 260 of 2138\n",
      "Processed batch 261 of 2138\n",
      "Processed batch 262 of 2138\n",
      "Processed batch 263 of 2138\n",
      "Processed batch 264 of 2138\n",
      "Processed batch 265 of 2138\n",
      "Processed batch 266 of 2138\n",
      "Processed batch 267 of 2138\n",
      "Processed batch 268 of 2138\n",
      "Processed batch 269 of 2138\n",
      "Processed batch 270 of 2138\n",
      "Processed batch 271 of 2138\n",
      "Processed batch 272 of 2138\n",
      "Processed batch 273 of 2138\n",
      "Processed batch 274 of 2138\n",
      "Processed batch 275 of 2138\n",
      "Processed batch 276 of 2138\n",
      "Processed batch 277 of 2138\n",
      "Processed batch 278 of 2138\n",
      "Processed batch 279 of 2138\n",
      "Processed batch 280 of 2138\n",
      "Processed batch 281 of 2138\n",
      "Processed batch 282 of 2138\n",
      "Processed batch 283 of 2138\n",
      "Processed batch 284 of 2138\n",
      "Processed batch 285 of 2138\n",
      "Processed batch 286 of 2138\n",
      "Processed batch 287 of 2138\n",
      "Processed batch 288 of 2138\n",
      "Processed batch 289 of 2138\n",
      "Processed batch 290 of 2138\n",
      "Processed batch 291 of 2138\n",
      "Processed batch 292 of 2138\n",
      "Processed batch 293 of 2138\n",
      "Processed batch 294 of 2138\n",
      "Processed batch 295 of 2138\n",
      "Processed batch 296 of 2138\n",
      "Processed batch 297 of 2138\n",
      "Processed batch 298 of 2138\n",
      "Processed batch 299 of 2138\n",
      "Processed batch 300 of 2138\n",
      "Processed batch 301 of 2138\n",
      "Processed batch 302 of 2138\n",
      "Processed batch 303 of 2138\n",
      "Processed batch 304 of 2138\n",
      "Processed batch 305 of 2138\n",
      "Processed batch 306 of 2138\n",
      "Processed batch 307 of 2138\n",
      "Processed batch 308 of 2138\n",
      "Processed batch 309 of 2138\n",
      "Processed batch 310 of 2138\n",
      "Processed batch 311 of 2138\n",
      "Processed batch 312 of 2138\n",
      "Processed batch 313 of 2138\n",
      "Processed batch 314 of 2138\n",
      "Processed batch 315 of 2138\n",
      "Processed batch 316 of 2138\n",
      "Processed batch 317 of 2138\n",
      "Processed batch 318 of 2138\n",
      "Processed batch 319 of 2138\n",
      "Processed batch 320 of 2138\n",
      "Processed batch 321 of 2138\n",
      "Processed batch 322 of 2138\n",
      "Processed batch 323 of 2138\n",
      "Processed batch 324 of 2138\n",
      "Processed batch 325 of 2138\n",
      "Processed batch 326 of 2138\n",
      "Processed batch 327 of 2138\n",
      "Processed batch 328 of 2138\n",
      "Processed batch 329 of 2138\n",
      "Processed batch 330 of 2138\n",
      "Processed batch 331 of 2138\n",
      "Processed batch 332 of 2138\n",
      "Processed batch 333 of 2138\n",
      "Processed batch 334 of 2138\n",
      "Processed batch 335 of 2138\n",
      "Processed batch 336 of 2138\n",
      "Processed batch 337 of 2138\n",
      "Processed batch 338 of 2138\n",
      "Processed batch 339 of 2138\n",
      "Processed batch 340 of 2138\n",
      "Processed batch 341 of 2138\n",
      "Processed batch 342 of 2138\n",
      "Processed batch 343 of 2138\n",
      "Processed batch 344 of 2138\n",
      "Processed batch 345 of 2138\n",
      "Processed batch 346 of 2138\n",
      "Processed batch 347 of 2138\n",
      "Processed batch 348 of 2138\n",
      "Processed batch 349 of 2138\n",
      "Processed batch 350 of 2138\n",
      "Processed batch 351 of 2138\n",
      "Processed batch 352 of 2138\n",
      "Processed batch 353 of 2138\n",
      "Processed batch 354 of 2138\n",
      "Processed batch 355 of 2138\n",
      "Processed batch 356 of 2138\n",
      "Processed batch 357 of 2138\n",
      "Processed batch 358 of 2138\n",
      "Processed batch 359 of 2138\n",
      "Processed batch 360 of 2138\n",
      "Processed batch 361 of 2138\n",
      "Processed batch 362 of 2138\n",
      "Processed batch 363 of 2138\n",
      "Processed batch 364 of 2138\n",
      "Processed batch 365 of 2138\n",
      "Processed batch 366 of 2138\n",
      "Processed batch 367 of 2138\n",
      "Processed batch 368 of 2138\n",
      "Processed batch 369 of 2138\n",
      "Processed batch 370 of 2138\n",
      "Processed batch 371 of 2138\n",
      "Processed batch 372 of 2138\n",
      "Processed batch 373 of 2138\n",
      "Processed batch 374 of 2138\n",
      "Processed batch 375 of 2138\n",
      "Processed batch 376 of 2138\n",
      "Processed batch 377 of 2138\n",
      "Processed batch 378 of 2138\n",
      "Processed batch 379 of 2138\n",
      "Processed batch 380 of 2138\n",
      "Processed batch 381 of 2138\n",
      "Processed batch 382 of 2138\n",
      "Processed batch 383 of 2138\n",
      "Processed batch 384 of 2138\n",
      "Processed batch 385 of 2138\n",
      "Processed batch 386 of 2138\n",
      "Processed batch 387 of 2138\n",
      "Processed batch 388 of 2138\n",
      "Processed batch 389 of 2138\n",
      "Processed batch 390 of 2138\n",
      "Processed batch 391 of 2138\n",
      "Processed batch 392 of 2138\n",
      "Processed batch 393 of 2138\n",
      "Processed batch 394 of 2138\n",
      "Processed batch 395 of 2138\n",
      "Processed batch 396 of 2138\n",
      "Processed batch 397 of 2138\n",
      "Processed batch 398 of 2138\n",
      "Processed batch 399 of 2138\n",
      "Processed batch 400 of 2138\n",
      "Processed batch 401 of 2138\n",
      "Processed batch 402 of 2138\n",
      "Processed batch 403 of 2138\n",
      "Processed batch 404 of 2138\n",
      "Processed batch 405 of 2138\n",
      "Processed batch 406 of 2138\n",
      "Processed batch 407 of 2138\n",
      "Processed batch 408 of 2138\n",
      "Processed batch 409 of 2138\n",
      "Processed batch 410 of 2138\n",
      "Processed batch 411 of 2138\n",
      "Processed batch 412 of 2138\n",
      "Processed batch 413 of 2138\n",
      "Processed batch 414 of 2138\n",
      "Processed batch 415 of 2138\n",
      "Processed batch 416 of 2138\n",
      "Processed batch 417 of 2138\n",
      "Processed batch 418 of 2138\n",
      "Processed batch 419 of 2138\n",
      "Processed batch 420 of 2138\n",
      "Processed batch 421 of 2138\n",
      "Processed batch 422 of 2138\n",
      "Processed batch 423 of 2138\n",
      "Processed batch 424 of 2138\n",
      "Processed batch 425 of 2138\n",
      "Processed batch 426 of 2138\n",
      "Processed batch 427 of 2138\n",
      "Processed batch 428 of 2138\n",
      "Processed batch 429 of 2138\n",
      "Processed batch 430 of 2138\n",
      "Processed batch 431 of 2138\n",
      "Processed batch 432 of 2138\n",
      "Processed batch 433 of 2138\n",
      "Processed batch 434 of 2138\n",
      "Processed batch 435 of 2138\n",
      "Processed batch 436 of 2138\n",
      "Processed batch 437 of 2138\n",
      "Processed batch 438 of 2138\n",
      "Processed batch 439 of 2138\n",
      "Processed batch 440 of 2138\n",
      "Processed batch 441 of 2138\n",
      "Processed batch 442 of 2138\n",
      "Processed batch 443 of 2138\n",
      "Processed batch 444 of 2138\n",
      "Processed batch 445 of 2138\n",
      "Processed batch 446 of 2138\n",
      "Processed batch 447 of 2138\n",
      "Processed batch 448 of 2138\n",
      "Processed batch 449 of 2138\n",
      "Processed batch 450 of 2138\n",
      "Processed batch 451 of 2138\n",
      "Processed batch 452 of 2138\n",
      "Processed batch 453 of 2138\n",
      "Processed batch 454 of 2138\n",
      "Processed batch 455 of 2138\n",
      "Processed batch 456 of 2138\n",
      "Processed batch 457 of 2138\n",
      "Processed batch 458 of 2138\n",
      "Processed batch 459 of 2138\n",
      "Processed batch 460 of 2138\n",
      "Processed batch 461 of 2138\n",
      "Processed batch 462 of 2138\n",
      "Processed batch 463 of 2138\n",
      "Processed batch 464 of 2138\n",
      "Processed batch 465 of 2138\n",
      "Processed batch 466 of 2138\n",
      "Processed batch 467 of 2138\n",
      "Processed batch 468 of 2138\n",
      "Processed batch 469 of 2138\n",
      "Processed batch 470 of 2138\n",
      "Processed batch 471 of 2138\n",
      "Processed batch 472 of 2138\n",
      "Processed batch 473 of 2138\n",
      "Processed batch 474 of 2138\n",
      "Processed batch 475 of 2138\n",
      "Processed batch 476 of 2138\n",
      "Processed batch 477 of 2138\n",
      "Processed batch 478 of 2138\n",
      "Processed batch 479 of 2138\n",
      "Processed batch 480 of 2138\n",
      "Processed batch 481 of 2138\n",
      "Processed batch 482 of 2138\n",
      "Processed batch 483 of 2138\n",
      "Processed batch 484 of 2138\n",
      "Processed batch 485 of 2138\n",
      "Processed batch 486 of 2138\n",
      "Processed batch 487 of 2138\n",
      "Processed batch 488 of 2138\n",
      "Processed batch 489 of 2138\n",
      "Processed batch 490 of 2138\n",
      "Processed batch 491 of 2138\n",
      "Processed batch 492 of 2138\n",
      "Processed batch 493 of 2138\n",
      "Processed batch 494 of 2138\n",
      "Processed batch 495 of 2138\n",
      "Processed batch 496 of 2138\n",
      "Processed batch 497 of 2138\n",
      "Processed batch 498 of 2138\n",
      "Processed batch 499 of 2138\n",
      "Processed batch 500 of 2138\n",
      "Processed batch 501 of 2138\n",
      "Processed batch 502 of 2138\n",
      "Processed batch 503 of 2138\n",
      "Processed batch 504 of 2138\n",
      "Processed batch 505 of 2138\n",
      "Processed batch 506 of 2138\n",
      "Processed batch 507 of 2138\n",
      "Processed batch 508 of 2138\n",
      "Processed batch 509 of 2138\n",
      "Processed batch 510 of 2138\n",
      "Processed batch 511 of 2138\n",
      "Processed batch 512 of 2138\n",
      "Processed batch 513 of 2138\n",
      "Processed batch 514 of 2138\n",
      "Processed batch 515 of 2138\n",
      "Processed batch 516 of 2138\n",
      "Processed batch 517 of 2138\n",
      "Processed batch 518 of 2138\n",
      "Processed batch 519 of 2138\n",
      "Processed batch 520 of 2138\n",
      "Processed batch 521 of 2138\n",
      "Processed batch 522 of 2138\n",
      "Processed batch 523 of 2138\n",
      "Processed batch 524 of 2138\n",
      "Processed batch 525 of 2138\n",
      "Processed batch 526 of 2138\n",
      "Processed batch 527 of 2138\n",
      "Processed batch 528 of 2138\n",
      "Processed batch 529 of 2138\n",
      "Processed batch 530 of 2138\n",
      "Processed batch 531 of 2138\n",
      "Processed batch 532 of 2138\n",
      "Processed batch 533 of 2138\n",
      "Processed batch 534 of 2138\n",
      "Processed batch 535 of 2138\n",
      "Processed batch 536 of 2138\n",
      "Processed batch 537 of 2138\n",
      "Processed batch 538 of 2138\n",
      "Processed batch 539 of 2138\n",
      "Processed batch 540 of 2138\n",
      "Processed batch 541 of 2138\n",
      "Processed batch 542 of 2138\n",
      "Processed batch 543 of 2138\n",
      "Processed batch 544 of 2138\n",
      "Processed batch 545 of 2138\n",
      "Processed batch 546 of 2138\n",
      "Processed batch 547 of 2138\n",
      "Processed batch 548 of 2138\n",
      "Processed batch 549 of 2138\n",
      "Processed batch 550 of 2138\n",
      "Processed batch 551 of 2138\n",
      "Processed batch 552 of 2138\n",
      "Processed batch 553 of 2138\n",
      "Processed batch 554 of 2138\n",
      "Processed batch 555 of 2138\n",
      "Processed batch 556 of 2138\n",
      "Processed batch 557 of 2138\n",
      "Processed batch 558 of 2138\n",
      "Processed batch 559 of 2138\n",
      "Processed batch 560 of 2138\n",
      "Processed batch 561 of 2138\n",
      "Processed batch 562 of 2138\n",
      "Processed batch 563 of 2138\n",
      "Processed batch 564 of 2138\n",
      "Processed batch 565 of 2138\n",
      "Processed batch 566 of 2138\n",
      "Processed batch 567 of 2138\n",
      "Processed batch 568 of 2138\n",
      "Processed batch 569 of 2138\n",
      "Processed batch 570 of 2138\n",
      "Processed batch 571 of 2138\n",
      "Processed batch 572 of 2138\n",
      "Processed batch 573 of 2138\n",
      "Processed batch 574 of 2138\n",
      "Processed batch 575 of 2138\n",
      "Processed batch 576 of 2138\n",
      "Processed batch 577 of 2138\n",
      "Processed batch 578 of 2138\n",
      "Processed batch 579 of 2138\n",
      "Processed batch 580 of 2138\n",
      "Processed batch 581 of 2138\n",
      "Processed batch 582 of 2138\n",
      "Processed batch 583 of 2138\n",
      "Processed batch 584 of 2138\n",
      "Processed batch 585 of 2138\n",
      "Processed batch 586 of 2138\n",
      "Processed batch 587 of 2138\n",
      "Processed batch 588 of 2138\n",
      "Processed batch 589 of 2138\n",
      "Processed batch 590 of 2138\n",
      "Processed batch 591 of 2138\n",
      "Processed batch 592 of 2138\n",
      "Processed batch 593 of 2138\n",
      "Processed batch 594 of 2138\n",
      "Processed batch 595 of 2138\n",
      "Processed batch 596 of 2138\n",
      "Processed batch 597 of 2138\n",
      "Processed batch 598 of 2138\n",
      "Processed batch 599 of 2138\n",
      "Processed batch 600 of 2138\n",
      "Processed batch 601 of 2138\n",
      "Processed batch 602 of 2138\n",
      "Processed batch 603 of 2138\n",
      "Processed batch 604 of 2138\n",
      "Processed batch 605 of 2138\n",
      "Processed batch 606 of 2138\n",
      "Processed batch 607 of 2138\n",
      "Processed batch 608 of 2138\n",
      "Processed batch 609 of 2138\n",
      "Processed batch 610 of 2138\n",
      "Processed batch 611 of 2138\n",
      "Processed batch 612 of 2138\n",
      "Processed batch 613 of 2138\n",
      "Processed batch 614 of 2138\n",
      "Processed batch 615 of 2138\n",
      "Processed batch 616 of 2138\n",
      "Processed batch 617 of 2138\n",
      "Processed batch 618 of 2138\n",
      "Processed batch 619 of 2138\n",
      "Processed batch 620 of 2138\n",
      "Processed batch 621 of 2138\n",
      "Processed batch 622 of 2138\n",
      "Processed batch 623 of 2138\n",
      "Processed batch 624 of 2138\n",
      "Processed batch 625 of 2138\n",
      "Processed batch 626 of 2138\n",
      "Processed batch 627 of 2138\n",
      "Processed batch 628 of 2138\n",
      "Processed batch 629 of 2138\n",
      "Processed batch 630 of 2138\n",
      "Processed batch 631 of 2138\n",
      "Processed batch 632 of 2138\n",
      "Processed batch 633 of 2138\n",
      "Processed batch 634 of 2138\n",
      "Processed batch 635 of 2138\n",
      "Processed batch 636 of 2138\n",
      "Processed batch 637 of 2138\n",
      "Processed batch 638 of 2138\n",
      "Processed batch 639 of 2138\n",
      "Processed batch 640 of 2138\n",
      "Processed batch 641 of 2138\n",
      "Processed batch 642 of 2138\n",
      "Processed batch 643 of 2138\n",
      "Processed batch 644 of 2138\n",
      "Processed batch 645 of 2138\n",
      "Processed batch 646 of 2138\n",
      "Processed batch 647 of 2138\n",
      "Processed batch 648 of 2138\n",
      "Processed batch 649 of 2138\n",
      "Processed batch 650 of 2138\n",
      "Processed batch 651 of 2138\n",
      "Processed batch 652 of 2138\n",
      "Processed batch 653 of 2138\n",
      "Processed batch 654 of 2138\n",
      "Processed batch 655 of 2138\n",
      "Processed batch 656 of 2138\n",
      "Processed batch 657 of 2138\n",
      "Processed batch 658 of 2138\n",
      "Processed batch 659 of 2138\n",
      "Processed batch 660 of 2138\n",
      "Processed batch 661 of 2138\n",
      "Processed batch 662 of 2138\n",
      "Processed batch 663 of 2138\n",
      "Processed batch 664 of 2138\n",
      "Processed batch 665 of 2138\n",
      "Processed batch 666 of 2138\n",
      "Processed batch 667 of 2138\n",
      "Processed batch 668 of 2138\n",
      "Processed batch 669 of 2138\n",
      "Processed batch 670 of 2138\n",
      "Processed batch 671 of 2138\n",
      "Processed batch 672 of 2138\n",
      "Processed batch 673 of 2138\n",
      "Processed batch 674 of 2138\n",
      "Processed batch 675 of 2138\n",
      "Processed batch 676 of 2138\n",
      "Processed batch 677 of 2138\n",
      "Processed batch 678 of 2138\n",
      "Processed batch 679 of 2138\n",
      "Processed batch 680 of 2138\n",
      "Processed batch 681 of 2138\n",
      "Processed batch 682 of 2138\n",
      "Processed batch 683 of 2138\n",
      "Processed batch 684 of 2138\n",
      "Processed batch 685 of 2138\n",
      "Processed batch 686 of 2138\n",
      "Processed batch 687 of 2138\n",
      "Processed batch 688 of 2138\n",
      "Processed batch 689 of 2138\n",
      "Processed batch 690 of 2138\n",
      "Processed batch 691 of 2138\n",
      "Processed batch 692 of 2138\n",
      "Processed batch 693 of 2138\n",
      "Processed batch 694 of 2138\n",
      "Processed batch 695 of 2138\n",
      "Processed batch 696 of 2138\n",
      "Processed batch 697 of 2138\n",
      "Processed batch 698 of 2138\n",
      "Processed batch 699 of 2138\n",
      "Processed batch 700 of 2138\n",
      "Processed batch 701 of 2138\n",
      "Processed batch 702 of 2138\n",
      "Processed batch 703 of 2138\n",
      "Processed batch 704 of 2138\n",
      "Processed batch 705 of 2138\n",
      "Processed batch 706 of 2138\n",
      "Processed batch 707 of 2138\n",
      "Processed batch 708 of 2138\n",
      "Processed batch 709 of 2138\n",
      "Processed batch 710 of 2138\n",
      "Processed batch 711 of 2138\n",
      "Processed batch 712 of 2138\n",
      "Processed batch 713 of 2138\n",
      "Processed batch 714 of 2138\n",
      "Processed batch 715 of 2138\n",
      "Processed batch 716 of 2138\n",
      "Processed batch 717 of 2138\n",
      "Processed batch 718 of 2138\n",
      "Processed batch 719 of 2138\n",
      "Processed batch 720 of 2138\n",
      "Processed batch 721 of 2138\n",
      "Processed batch 722 of 2138\n",
      "Processed batch 723 of 2138\n",
      "Processed batch 724 of 2138\n",
      "Processed batch 725 of 2138\n",
      "Processed batch 726 of 2138\n",
      "Processed batch 727 of 2138\n",
      "Processed batch 728 of 2138\n",
      "Processed batch 729 of 2138\n",
      "Processed batch 730 of 2138\n",
      "Processed batch 731 of 2138\n",
      "Processed batch 732 of 2138\n",
      "Processed batch 733 of 2138\n",
      "Processed batch 734 of 2138\n",
      "Processed batch 735 of 2138\n",
      "Processed batch 736 of 2138\n",
      "Processed batch 737 of 2138\n",
      "Processed batch 738 of 2138\n",
      "Processed batch 739 of 2138\n",
      "Processed batch 740 of 2138\n",
      "Processed batch 741 of 2138\n",
      "Processed batch 742 of 2138\n",
      "Processed batch 743 of 2138\n",
      "Processed batch 744 of 2138\n",
      "Processed batch 745 of 2138\n",
      "Processed batch 746 of 2138\n",
      "Processed batch 747 of 2138\n",
      "Processed batch 748 of 2138\n",
      "Processed batch 749 of 2138\n",
      "Processed batch 750 of 2138\n",
      "Processed batch 751 of 2138\n",
      "Processed batch 752 of 2138\n",
      "Processed batch 753 of 2138\n",
      "Processed batch 754 of 2138\n",
      "Processed batch 755 of 2138\n",
      "Processed batch 756 of 2138\n",
      "Processed batch 757 of 2138\n",
      "Processed batch 758 of 2138\n",
      "Processed batch 759 of 2138\n",
      "Processed batch 760 of 2138\n",
      "Processed batch 761 of 2138\n",
      "Processed batch 762 of 2138\n",
      "Processed batch 763 of 2138\n",
      "Processed batch 764 of 2138\n",
      "Processed batch 765 of 2138\n",
      "Processed batch 766 of 2138\n",
      "Processed batch 767 of 2138\n",
      "Processed batch 768 of 2138\n",
      "Processed batch 769 of 2138\n",
      "Processed batch 770 of 2138\n",
      "Processed batch 771 of 2138\n",
      "Processed batch 772 of 2138\n",
      "Processed batch 773 of 2138\n",
      "Processed batch 774 of 2138\n",
      "Processed batch 775 of 2138\n",
      "Processed batch 776 of 2138\n",
      "Processed batch 777 of 2138\n",
      "Processed batch 778 of 2138\n",
      "Processed batch 779 of 2138\n",
      "Processed batch 780 of 2138\n",
      "Processed batch 781 of 2138\n",
      "Processed batch 782 of 2138\n",
      "Processed batch 783 of 2138\n",
      "Processed batch 784 of 2138\n",
      "Processed batch 785 of 2138\n",
      "Processed batch 786 of 2138\n",
      "Processed batch 787 of 2138\n",
      "Processed batch 788 of 2138\n",
      "Processed batch 789 of 2138\n",
      "Processed batch 790 of 2138\n",
      "Processed batch 791 of 2138\n",
      "Processed batch 792 of 2138\n",
      "Processed batch 793 of 2138\n",
      "Processed batch 794 of 2138\n",
      "Processed batch 795 of 2138\n",
      "Processed batch 796 of 2138\n",
      "Processed batch 797 of 2138\n",
      "Processed batch 798 of 2138\n",
      "Processed batch 799 of 2138\n",
      "Processed batch 800 of 2138\n",
      "Processed batch 801 of 2138\n",
      "Processed batch 802 of 2138\n",
      "Processed batch 803 of 2138\n",
      "Processed batch 804 of 2138\n",
      "Processed batch 805 of 2138\n",
      "Processed batch 806 of 2138\n",
      "Processed batch 807 of 2138\n",
      "Processed batch 808 of 2138\n",
      "Processed batch 809 of 2138\n",
      "Processed batch 810 of 2138\n",
      "Processed batch 811 of 2138\n",
      "Processed batch 812 of 2138\n",
      "Processed batch 813 of 2138\n",
      "Processed batch 814 of 2138\n",
      "Processed batch 815 of 2138\n",
      "Processed batch 816 of 2138\n",
      "Processed batch 817 of 2138\n",
      "Processed batch 818 of 2138\n",
      "Processed batch 819 of 2138\n",
      "Processed batch 820 of 2138\n",
      "Processed batch 821 of 2138\n",
      "Processed batch 822 of 2138\n",
      "Processed batch 823 of 2138\n",
      "Processed batch 824 of 2138\n",
      "Processed batch 825 of 2138\n",
      "Processed batch 826 of 2138\n",
      "Processed batch 827 of 2138\n",
      "Processed batch 828 of 2138\n",
      "Processed batch 829 of 2138\n",
      "Processed batch 830 of 2138\n",
      "Processed batch 831 of 2138\n",
      "Processed batch 832 of 2138\n",
      "Processed batch 833 of 2138\n",
      "Processed batch 834 of 2138\n",
      "Processed batch 835 of 2138\n",
      "Processed batch 836 of 2138\n",
      "Processed batch 837 of 2138\n",
      "Processed batch 838 of 2138\n",
      "Processed batch 839 of 2138\n",
      "Processed batch 840 of 2138\n",
      "Processed batch 841 of 2138\n",
      "Processed batch 842 of 2138\n",
      "Processed batch 843 of 2138\n",
      "Processed batch 844 of 2138\n",
      "Processed batch 845 of 2138\n",
      "Processed batch 846 of 2138\n",
      "Processed batch 847 of 2138\n",
      "Processed batch 848 of 2138\n",
      "Processed batch 849 of 2138\n",
      "Processed batch 850 of 2138\n",
      "Processed batch 851 of 2138\n",
      "Processed batch 852 of 2138\n",
      "Processed batch 853 of 2138\n",
      "Processed batch 854 of 2138\n",
      "Processed batch 855 of 2138\n",
      "Processed batch 856 of 2138\n",
      "Processed batch 857 of 2138\n",
      "Processed batch 858 of 2138\n",
      "Processed batch 859 of 2138\n",
      "Processed batch 860 of 2138\n",
      "Processed batch 861 of 2138\n",
      "Processed batch 862 of 2138\n",
      "Processed batch 863 of 2138\n",
      "Processed batch 864 of 2138\n",
      "Processed batch 865 of 2138\n",
      "Processed batch 866 of 2138\n",
      "Processed batch 867 of 2138\n",
      "Processed batch 868 of 2138\n",
      "Processed batch 869 of 2138\n",
      "Processed batch 870 of 2138\n",
      "Processed batch 871 of 2138\n",
      "Processed batch 872 of 2138\n",
      "Processed batch 873 of 2138\n",
      "Processed batch 874 of 2138\n",
      "Processed batch 875 of 2138\n",
      "Processed batch 876 of 2138\n",
      "Processed batch 877 of 2138\n",
      "Processed batch 878 of 2138\n",
      "Processed batch 879 of 2138\n",
      "Processed batch 880 of 2138\n",
      "Processed batch 881 of 2138\n",
      "Processed batch 882 of 2138\n",
      "Processed batch 883 of 2138\n",
      "Processed batch 884 of 2138\n",
      "Processed batch 885 of 2138\n",
      "Processed batch 886 of 2138\n",
      "Processed batch 887 of 2138\n",
      "Processed batch 888 of 2138\n",
      "Processed batch 889 of 2138\n",
      "Processed batch 890 of 2138\n",
      "Processed batch 891 of 2138\n",
      "Processed batch 892 of 2138\n",
      "Processed batch 893 of 2138\n",
      "Processed batch 894 of 2138\n",
      "Processed batch 895 of 2138\n",
      "Processed batch 896 of 2138\n",
      "Processed batch 897 of 2138\n",
      "Processed batch 898 of 2138\n",
      "Processed batch 899 of 2138\n",
      "Processed batch 900 of 2138\n",
      "Processed batch 901 of 2138\n",
      "Processed batch 902 of 2138\n",
      "Processed batch 903 of 2138\n",
      "Processed batch 904 of 2138\n",
      "Processed batch 905 of 2138\n",
      "Processed batch 906 of 2138\n",
      "Processed batch 907 of 2138\n",
      "Processed batch 908 of 2138\n",
      "Processed batch 909 of 2138\n",
      "Processed batch 910 of 2138\n",
      "Processed batch 911 of 2138\n",
      "Processed batch 912 of 2138\n",
      "Processed batch 913 of 2138\n",
      "Processed batch 914 of 2138\n",
      "Processed batch 915 of 2138\n",
      "Processed batch 916 of 2138\n",
      "Processed batch 917 of 2138\n",
      "Processed batch 918 of 2138\n",
      "Processed batch 919 of 2138\n",
      "Processed batch 920 of 2138\n",
      "Processed batch 921 of 2138\n",
      "Processed batch 922 of 2138\n",
      "Processed batch 923 of 2138\n",
      "Processed batch 924 of 2138\n",
      "Processed batch 925 of 2138\n",
      "Processed batch 926 of 2138\n",
      "Processed batch 927 of 2138\n",
      "Processed batch 928 of 2138\n",
      "Processed batch 929 of 2138\n",
      "Processed batch 930 of 2138\n",
      "Processed batch 931 of 2138\n",
      "Processed batch 932 of 2138\n",
      "Processed batch 933 of 2138\n",
      "Processed batch 934 of 2138\n",
      "Processed batch 935 of 2138\n",
      "Processed batch 936 of 2138\n",
      "Processed batch 937 of 2138\n",
      "Processed batch 938 of 2138\n",
      "Processed batch 939 of 2138\n",
      "Processed batch 940 of 2138\n",
      "Processed batch 941 of 2138\n",
      "Processed batch 942 of 2138\n",
      "Processed batch 943 of 2138\n",
      "Processed batch 944 of 2138\n",
      "Processed batch 945 of 2138\n",
      "Processed batch 946 of 2138\n",
      "Processed batch 947 of 2138\n",
      "Processed batch 948 of 2138\n",
      "Processed batch 949 of 2138\n",
      "Processed batch 950 of 2138\n",
      "Processed batch 951 of 2138\n",
      "Processed batch 952 of 2138\n",
      "Processed batch 953 of 2138\n",
      "Processed batch 954 of 2138\n",
      "Processed batch 955 of 2138\n",
      "Processed batch 956 of 2138\n",
      "Processed batch 957 of 2138\n",
      "Processed batch 958 of 2138\n",
      "Processed batch 959 of 2138\n",
      "Processed batch 960 of 2138\n",
      "Processed batch 961 of 2138\n",
      "Processed batch 962 of 2138\n",
      "Processed batch 963 of 2138\n",
      "Processed batch 964 of 2138\n",
      "Processed batch 965 of 2138\n",
      "Processed batch 966 of 2138\n",
      "Processed batch 967 of 2138\n",
      "Processed batch 968 of 2138\n",
      "Processed batch 969 of 2138\n",
      "Processed batch 970 of 2138\n",
      "Processed batch 971 of 2138\n",
      "Processed batch 972 of 2138\n",
      "Processed batch 973 of 2138\n",
      "Processed batch 974 of 2138\n",
      "Processed batch 975 of 2138\n",
      "Processed batch 976 of 2138\n",
      "Processed batch 977 of 2138\n",
      "Processed batch 978 of 2138\n",
      "Processed batch 979 of 2138\n",
      "Processed batch 980 of 2138\n",
      "Processed batch 981 of 2138\n",
      "Processed batch 982 of 2138\n",
      "Processed batch 983 of 2138\n",
      "Processed batch 984 of 2138\n",
      "Processed batch 985 of 2138\n",
      "Processed batch 986 of 2138\n",
      "Processed batch 987 of 2138\n",
      "Processed batch 988 of 2138\n",
      "Processed batch 989 of 2138\n",
      "Processed batch 990 of 2138\n",
      "Processed batch 991 of 2138\n",
      "Processed batch 992 of 2138\n",
      "Processed batch 993 of 2138\n",
      "Processed batch 994 of 2138\n",
      "Processed batch 995 of 2138\n",
      "Processed batch 996 of 2138\n",
      "Processed batch 997 of 2138\n",
      "Processed batch 998 of 2138\n",
      "Processed batch 999 of 2138\n",
      "Processed batch 1000 of 2138\n",
      "Processed batch 1001 of 2138\n",
      "Processed batch 1002 of 2138\n",
      "Processed batch 1003 of 2138\n",
      "Processed batch 1004 of 2138\n",
      "Processed batch 1005 of 2138\n",
      "Processed batch 1006 of 2138\n",
      "Processed batch 1007 of 2138\n",
      "Processed batch 1008 of 2138\n",
      "Processed batch 1009 of 2138\n",
      "Processed batch 1010 of 2138\n",
      "Processed batch 1011 of 2138\n",
      "Processed batch 1012 of 2138\n",
      "Processed batch 1013 of 2138\n",
      "Processed batch 1014 of 2138\n",
      "Processed batch 1015 of 2138\n",
      "Processed batch 1016 of 2138\n",
      "Processed batch 1017 of 2138\n",
      "Processed batch 1018 of 2138\n",
      "Processed batch 1019 of 2138\n",
      "Processed batch 1020 of 2138\n",
      "Processed batch 1021 of 2138\n",
      "Processed batch 1022 of 2138\n",
      "Processed batch 1023 of 2138\n",
      "Processed batch 1024 of 2138\n",
      "Processed batch 1025 of 2138\n",
      "Processed batch 1026 of 2138\n",
      "Processed batch 1027 of 2138\n",
      "Processed batch 1028 of 2138\n",
      "Processed batch 1029 of 2138\n",
      "Processed batch 1030 of 2138\n",
      "Processed batch 1031 of 2138\n",
      "Processed batch 1032 of 2138\n",
      "Processed batch 1033 of 2138\n",
      "Processed batch 1034 of 2138\n",
      "Processed batch 1035 of 2138\n",
      "Processed batch 1036 of 2138\n",
      "Processed batch 1037 of 2138\n",
      "Processed batch 1038 of 2138\n",
      "Processed batch 1039 of 2138\n",
      "Processed batch 1040 of 2138\n",
      "Processed batch 1041 of 2138\n",
      "Processed batch 1042 of 2138\n",
      "Processed batch 1043 of 2138\n",
      "Processed batch 1044 of 2138\n",
      "Processed batch 1045 of 2138\n",
      "Processed batch 1046 of 2138\n",
      "Processed batch 1047 of 2138\n",
      "Processed batch 1048 of 2138\n",
      "Processed batch 1049 of 2138\n",
      "Processed batch 1050 of 2138\n",
      "Processed batch 1051 of 2138\n",
      "Processed batch 1052 of 2138\n",
      "Processed batch 1053 of 2138\n",
      "Processed batch 1054 of 2138\n",
      "Processed batch 1055 of 2138\n",
      "Processed batch 1056 of 2138\n",
      "Processed batch 1057 of 2138\n",
      "Processed batch 1058 of 2138\n",
      "Processed batch 1059 of 2138\n",
      "Processed batch 1060 of 2138\n",
      "Processed batch 1061 of 2138\n",
      "Processed batch 1062 of 2138\n",
      "Processed batch 1063 of 2138\n",
      "Processed batch 1064 of 2138\n",
      "Processed batch 1065 of 2138\n",
      "Processed batch 1066 of 2138\n",
      "Processed batch 1067 of 2138\n",
      "Processed batch 1068 of 2138\n",
      "Processed batch 1069 of 2138\n",
      "Processed batch 1070 of 2138\n",
      "Processed batch 1071 of 2138\n",
      "Processed batch 1072 of 2138\n",
      "Processed batch 1073 of 2138\n",
      "Processed batch 1074 of 2138\n",
      "Processed batch 1075 of 2138\n",
      "Processed batch 1076 of 2138\n",
      "Processed batch 1077 of 2138\n",
      "Processed batch 1078 of 2138\n",
      "Processed batch 1079 of 2138\n",
      "Processed batch 1080 of 2138\n",
      "Processed batch 1081 of 2138\n",
      "Processed batch 1082 of 2138\n",
      "Processed batch 1083 of 2138\n",
      "Processed batch 1084 of 2138\n",
      "Processed batch 1085 of 2138\n",
      "Processed batch 1086 of 2138\n",
      "Processed batch 1087 of 2138\n",
      "Processed batch 1088 of 2138\n",
      "Processed batch 1089 of 2138\n",
      "Processed batch 1090 of 2138\n",
      "Processed batch 1091 of 2138\n",
      "Processed batch 1092 of 2138\n",
      "Processed batch 1093 of 2138\n",
      "Processed batch 1094 of 2138\n",
      "Processed batch 1095 of 2138\n",
      "Processed batch 1096 of 2138\n",
      "Processed batch 1097 of 2138\n",
      "Processed batch 1098 of 2138\n",
      "Processed batch 1099 of 2138\n",
      "Processed batch 1100 of 2138\n",
      "Processed batch 1101 of 2138\n",
      "Processed batch 1102 of 2138\n",
      "Processed batch 1103 of 2138\n",
      "Processed batch 1104 of 2138\n",
      "Processed batch 1105 of 2138\n",
      "Processed batch 1106 of 2138\n",
      "Processed batch 1107 of 2138\n",
      "Processed batch 1108 of 2138\n",
      "Processed batch 1109 of 2138\n",
      "Processed batch 1110 of 2138\n",
      "Processed batch 1111 of 2138\n",
      "Processed batch 1112 of 2138\n",
      "Processed batch 1113 of 2138\n",
      "Processed batch 1114 of 2138\n",
      "Processed batch 1115 of 2138\n",
      "Processed batch 1116 of 2138\n",
      "Processed batch 1117 of 2138\n",
      "Processed batch 1118 of 2138\n",
      "Processed batch 1119 of 2138\n",
      "Processed batch 1120 of 2138\n",
      "Processed batch 1121 of 2138\n",
      "Processed batch 1122 of 2138\n",
      "Processed batch 1123 of 2138\n",
      "Processed batch 1124 of 2138\n",
      "Processed batch 1125 of 2138\n",
      "Processed batch 1126 of 2138\n",
      "Processed batch 1127 of 2138\n",
      "Processed batch 1128 of 2138\n",
      "Processed batch 1129 of 2138\n",
      "Processed batch 1130 of 2138\n",
      "Processed batch 1131 of 2138\n",
      "Processed batch 1132 of 2138\n",
      "Processed batch 1133 of 2138\n",
      "Processed batch 1134 of 2138\n",
      "Processed batch 1135 of 2138\n",
      "Processed batch 1136 of 2138\n",
      "Processed batch 1137 of 2138\n",
      "Processed batch 1138 of 2138\n",
      "Processed batch 1139 of 2138\n",
      "Processed batch 1140 of 2138\n",
      "Processed batch 1141 of 2138\n",
      "Processed batch 1142 of 2138\n",
      "Processed batch 1143 of 2138\n",
      "Processed batch 1144 of 2138\n",
      "Processed batch 1145 of 2138\n",
      "Processed batch 1146 of 2138\n",
      "Processed batch 1147 of 2138\n",
      "Processed batch 1148 of 2138\n",
      "Processed batch 1149 of 2138\n",
      "Processed batch 1150 of 2138\n",
      "Processed batch 1151 of 2138\n",
      "Processed batch 1152 of 2138\n",
      "Processed batch 1153 of 2138\n",
      "Processed batch 1154 of 2138\n",
      "Processed batch 1155 of 2138\n",
      "Processed batch 1156 of 2138\n",
      "Processed batch 1157 of 2138\n",
      "Processed batch 1158 of 2138\n",
      "Processed batch 1159 of 2138\n",
      "Processed batch 1160 of 2138\n",
      "Processed batch 1161 of 2138\n",
      "Processed batch 1162 of 2138\n",
      "Processed batch 1163 of 2138\n",
      "Processed batch 1164 of 2138\n",
      "Processed batch 1165 of 2138\n",
      "Processed batch 1166 of 2138\n",
      "Processed batch 1167 of 2138\n",
      "Processed batch 1168 of 2138\n",
      "Processed batch 1169 of 2138\n",
      "Processed batch 1170 of 2138\n",
      "Processed batch 1171 of 2138\n",
      "Processed batch 1172 of 2138\n",
      "Processed batch 1173 of 2138\n",
      "Processed batch 1174 of 2138\n",
      "Processed batch 1175 of 2138\n",
      "Processed batch 1176 of 2138\n",
      "Processed batch 1177 of 2138\n",
      "Processed batch 1178 of 2138\n",
      "Processed batch 1179 of 2138\n",
      "Processed batch 1180 of 2138\n",
      "Processed batch 1181 of 2138\n",
      "Processed batch 1182 of 2138\n",
      "Processed batch 1183 of 2138\n",
      "Processed batch 1184 of 2138\n",
      "Processed batch 1185 of 2138\n",
      "Processed batch 1186 of 2138\n",
      "Processed batch 1187 of 2138\n",
      "Processed batch 1188 of 2138\n",
      "Processed batch 1189 of 2138\n",
      "Processed batch 1190 of 2138\n",
      "Processed batch 1191 of 2138\n",
      "Processed batch 1192 of 2138\n",
      "Processed batch 1193 of 2138\n",
      "Processed batch 1194 of 2138\n",
      "Processed batch 1195 of 2138\n",
      "Processed batch 1196 of 2138\n",
      "Processed batch 1197 of 2138\n",
      "Processed batch 1198 of 2138\n",
      "Processed batch 1199 of 2138\n",
      "Processed batch 1200 of 2138\n",
      "Processed batch 1201 of 2138\n",
      "Processed batch 1202 of 2138\n",
      "Processed batch 1203 of 2138\n",
      "Processed batch 1204 of 2138\n",
      "Processed batch 1205 of 2138\n",
      "Processed batch 1206 of 2138\n",
      "Processed batch 1207 of 2138\n",
      "Processed batch 1208 of 2138\n",
      "Processed batch 1209 of 2138\n",
      "Processed batch 1210 of 2138\n",
      "Processed batch 1211 of 2138\n",
      "Processed batch 1212 of 2138\n",
      "Processed batch 1213 of 2138\n",
      "Processed batch 1214 of 2138\n",
      "Processed batch 1215 of 2138\n",
      "Processed batch 1216 of 2138\n",
      "Processed batch 1217 of 2138\n",
      "Processed batch 1218 of 2138\n",
      "Processed batch 1219 of 2138\n",
      "Processed batch 1220 of 2138\n",
      "Processed batch 1221 of 2138\n",
      "Processed batch 1222 of 2138\n",
      "Processed batch 1223 of 2138\n",
      "Processed batch 1224 of 2138\n",
      "Processed batch 1225 of 2138\n",
      "Processed batch 1226 of 2138\n",
      "Processed batch 1227 of 2138\n",
      "Processed batch 1228 of 2138\n",
      "Processed batch 1229 of 2138\n",
      "Processed batch 1230 of 2138\n",
      "Processed batch 1231 of 2138\n",
      "Processed batch 1232 of 2138\n",
      "Processed batch 1233 of 2138\n",
      "Processed batch 1234 of 2138\n",
      "Processed batch 1235 of 2138\n",
      "Processed batch 1236 of 2138\n",
      "Processed batch 1237 of 2138\n",
      "Processed batch 1238 of 2138\n",
      "Processed batch 1239 of 2138\n",
      "Processed batch 1240 of 2138\n",
      "Processed batch 1241 of 2138\n",
      "Processed batch 1242 of 2138\n",
      "Processed batch 1243 of 2138\n",
      "Processed batch 1244 of 2138\n",
      "Processed batch 1245 of 2138\n",
      "Processed batch 1246 of 2138\n",
      "Processed batch 1247 of 2138\n",
      "Processed batch 1248 of 2138\n",
      "Processed batch 1249 of 2138\n",
      "Processed batch 1250 of 2138\n",
      "Processed batch 1251 of 2138\n",
      "Processed batch 1252 of 2138\n",
      "Processed batch 1253 of 2138\n",
      "Processed batch 1254 of 2138\n",
      "Processed batch 1255 of 2138\n",
      "Processed batch 1256 of 2138\n",
      "Processed batch 1257 of 2138\n",
      "Processed batch 1258 of 2138\n",
      "Processed batch 1259 of 2138\n",
      "Processed batch 1260 of 2138\n",
      "Processed batch 1261 of 2138\n",
      "Processed batch 1262 of 2138\n",
      "Processed batch 1263 of 2138\n",
      "Processed batch 1264 of 2138\n",
      "Processed batch 1265 of 2138\n",
      "Processed batch 1266 of 2138\n",
      "Processed batch 1267 of 2138\n",
      "Processed batch 1268 of 2138\n",
      "Processed batch 1269 of 2138\n",
      "Processed batch 1270 of 2138\n",
      "Processed batch 1271 of 2138\n",
      "Processed batch 1272 of 2138\n",
      "Processed batch 1273 of 2138\n",
      "Processed batch 1274 of 2138\n",
      "Processed batch 1275 of 2138\n",
      "Processed batch 1276 of 2138\n",
      "Processed batch 1277 of 2138\n",
      "Processed batch 1278 of 2138\n",
      "Processed batch 1279 of 2138\n",
      "Processed batch 1280 of 2138\n",
      "Processed batch 1281 of 2138\n",
      "Processed batch 1282 of 2138\n",
      "Processed batch 1283 of 2138\n",
      "Processed batch 1284 of 2138\n",
      "Processed batch 1285 of 2138\n",
      "Processed batch 1286 of 2138\n",
      "Processed batch 1287 of 2138\n",
      "Processed batch 1288 of 2138\n",
      "Processed batch 1289 of 2138\n",
      "Processed batch 1290 of 2138\n",
      "Processed batch 1291 of 2138\n",
      "Processed batch 1292 of 2138\n",
      "Processed batch 1293 of 2138\n",
      "Processed batch 1294 of 2138\n",
      "Processed batch 1295 of 2138\n",
      "Processed batch 1296 of 2138\n",
      "Processed batch 1297 of 2138\n",
      "Processed batch 1298 of 2138\n",
      "Processed batch 1299 of 2138\n",
      "Processed batch 1300 of 2138\n",
      "Processed batch 1301 of 2138\n",
      "Processed batch 1302 of 2138\n",
      "Processed batch 1303 of 2138\n",
      "Processed batch 1304 of 2138\n",
      "Processed batch 1305 of 2138\n",
      "Processed batch 1306 of 2138\n",
      "Processed batch 1307 of 2138\n",
      "Processed batch 1308 of 2138\n",
      "Processed batch 1309 of 2138\n",
      "Processed batch 1310 of 2138\n",
      "Processed batch 1311 of 2138\n",
      "Processed batch 1312 of 2138\n",
      "Processed batch 1313 of 2138\n",
      "Processed batch 1314 of 2138\n",
      "Processed batch 1315 of 2138\n",
      "Processed batch 1316 of 2138\n",
      "Processed batch 1317 of 2138\n",
      "Processed batch 1318 of 2138\n",
      "Processed batch 1319 of 2138\n",
      "Processed batch 1320 of 2138\n",
      "Processed batch 1321 of 2138\n",
      "Processed batch 1322 of 2138\n",
      "Processed batch 1323 of 2138\n",
      "Processed batch 1324 of 2138\n",
      "Processed batch 1325 of 2138\n",
      "Processed batch 1326 of 2138\n",
      "Processed batch 1327 of 2138\n",
      "Processed batch 1328 of 2138\n",
      "Processed batch 1329 of 2138\n",
      "Processed batch 1330 of 2138\n",
      "Processed batch 1331 of 2138\n",
      "Processed batch 1332 of 2138\n",
      "Processed batch 1333 of 2138\n",
      "Processed batch 1334 of 2138\n",
      "Processed batch 1335 of 2138\n",
      "Processed batch 1336 of 2138\n",
      "Processed batch 1337 of 2138\n",
      "Processed batch 1338 of 2138\n",
      "Processed batch 1339 of 2138\n",
      "Processed batch 1340 of 2138\n",
      "Processed batch 1341 of 2138\n",
      "Processed batch 1342 of 2138\n",
      "Processed batch 1343 of 2138\n",
      "Processed batch 1344 of 2138\n",
      "Processed batch 1345 of 2138\n",
      "Processed batch 1346 of 2138\n",
      "Processed batch 1347 of 2138\n",
      "Processed batch 1348 of 2138\n",
      "Processed batch 1349 of 2138\n",
      "Processed batch 1350 of 2138\n",
      "Processed batch 1351 of 2138\n",
      "Processed batch 1352 of 2138\n",
      "Processed batch 1353 of 2138\n",
      "Processed batch 1354 of 2138\n",
      "Processed batch 1355 of 2138\n",
      "Processed batch 1356 of 2138\n",
      "Processed batch 1357 of 2138\n",
      "Processed batch 1358 of 2138\n",
      "Processed batch 1359 of 2138\n",
      "Processed batch 1360 of 2138\n",
      "Processed batch 1361 of 2138\n",
      "Processed batch 1362 of 2138\n",
      "Processed batch 1363 of 2138\n",
      "Processed batch 1364 of 2138\n",
      "Processed batch 1365 of 2138\n",
      "Processed batch 1366 of 2138\n",
      "Processed batch 1367 of 2138\n",
      "Processed batch 1368 of 2138\n",
      "Processed batch 1369 of 2138\n",
      "Processed batch 1370 of 2138\n",
      "Processed batch 1371 of 2138\n",
      "Processed batch 1372 of 2138\n",
      "Processed batch 1373 of 2138\n",
      "Processed batch 1374 of 2138\n",
      "Processed batch 1375 of 2138\n",
      "Processed batch 1376 of 2138\n",
      "Processed batch 1377 of 2138\n",
      "Processed batch 1378 of 2138\n",
      "Processed batch 1379 of 2138\n",
      "Processed batch 1380 of 2138\n",
      "Processed batch 1381 of 2138\n",
      "Processed batch 1382 of 2138\n",
      "Processed batch 1383 of 2138\n",
      "Processed batch 1384 of 2138\n",
      "Processed batch 1385 of 2138\n",
      "Processed batch 1386 of 2138\n",
      "Processed batch 1387 of 2138\n",
      "Processed batch 1388 of 2138\n",
      "Processed batch 1389 of 2138\n",
      "Processed batch 1390 of 2138\n",
      "Processed batch 1391 of 2138\n",
      "Processed batch 1392 of 2138\n",
      "Processed batch 1393 of 2138\n",
      "Processed batch 1394 of 2138\n",
      "Processed batch 1395 of 2138\n",
      "Processed batch 1396 of 2138\n",
      "Processed batch 1397 of 2138\n",
      "Processed batch 1398 of 2138\n",
      "Processed batch 1399 of 2138\n",
      "Processed batch 1400 of 2138\n",
      "Processed batch 1401 of 2138\n",
      "Processed batch 1402 of 2138\n",
      "Processed batch 1403 of 2138\n",
      "Processed batch 1404 of 2138\n",
      "Processed batch 1405 of 2138\n",
      "Processed batch 1406 of 2138\n",
      "Processed batch 1407 of 2138\n",
      "Processed batch 1408 of 2138\n",
      "Processed batch 1409 of 2138\n",
      "Processed batch 1410 of 2138\n",
      "Processed batch 1411 of 2138\n",
      "Processed batch 1412 of 2138\n",
      "Processed batch 1413 of 2138\n",
      "Processed batch 1414 of 2138\n",
      "Processed batch 1415 of 2138\n",
      "Processed batch 1416 of 2138\n",
      "Processed batch 1417 of 2138\n",
      "Processed batch 1418 of 2138\n",
      "Processed batch 1419 of 2138\n",
      "Processed batch 1420 of 2138\n",
      "Processed batch 1421 of 2138\n",
      "Processed batch 1422 of 2138\n",
      "Processed batch 1423 of 2138\n",
      "Processed batch 1424 of 2138\n",
      "Processed batch 1425 of 2138\n",
      "Processed batch 1426 of 2138\n",
      "Processed batch 1427 of 2138\n",
      "Processed batch 1428 of 2138\n",
      "Processed batch 1429 of 2138\n",
      "Processed batch 1430 of 2138\n",
      "Processed batch 1431 of 2138\n",
      "Processed batch 1432 of 2138\n",
      "Processed batch 1433 of 2138\n",
      "Processed batch 1434 of 2138\n",
      "Processed batch 1435 of 2138\n",
      "Processed batch 1436 of 2138\n",
      "Processed batch 1437 of 2138\n",
      "Processed batch 1438 of 2138\n",
      "Processed batch 1439 of 2138\n",
      "Processed batch 1440 of 2138\n",
      "Processed batch 1441 of 2138\n",
      "Processed batch 1442 of 2138\n",
      "Processed batch 1443 of 2138\n",
      "Processed batch 1444 of 2138\n",
      "Processed batch 1445 of 2138\n",
      "Processed batch 1446 of 2138\n",
      "Processed batch 1447 of 2138\n",
      "Processed batch 1448 of 2138\n",
      "Processed batch 1449 of 2138\n",
      "Processed batch 1450 of 2138\n",
      "Processed batch 1451 of 2138\n",
      "Processed batch 1452 of 2138\n",
      "Processed batch 1453 of 2138\n",
      "Processed batch 1454 of 2138\n",
      "Processed batch 1455 of 2138\n",
      "Processed batch 1456 of 2138\n",
      "Processed batch 1457 of 2138\n",
      "Processed batch 1458 of 2138\n",
      "Processed batch 1459 of 2138\n",
      "Processed batch 1460 of 2138\n",
      "Processed batch 1461 of 2138\n",
      "Processed batch 1462 of 2138\n",
      "Processed batch 1463 of 2138\n",
      "Processed batch 1464 of 2138\n",
      "Processed batch 1465 of 2138\n",
      "Processed batch 1466 of 2138\n",
      "Processed batch 1467 of 2138\n",
      "Processed batch 1468 of 2138\n",
      "Processed batch 1469 of 2138\n",
      "Processed batch 1470 of 2138\n",
      "Processed batch 1471 of 2138\n",
      "Processed batch 1472 of 2138\n",
      "Processed batch 1473 of 2138\n",
      "Processed batch 1474 of 2138\n",
      "Processed batch 1475 of 2138\n",
      "Processed batch 1476 of 2138\n",
      "Processed batch 1477 of 2138\n",
      "Processed batch 1478 of 2138\n",
      "Processed batch 1479 of 2138\n",
      "Processed batch 1480 of 2138\n",
      "Processed batch 1481 of 2138\n",
      "Processed batch 1482 of 2138\n",
      "Processed batch 1483 of 2138\n",
      "Processed batch 1484 of 2138\n",
      "Processed batch 1485 of 2138\n",
      "Processed batch 1486 of 2138\n",
      "Processed batch 1487 of 2138\n",
      "Processed batch 1488 of 2138\n",
      "Processed batch 1489 of 2138\n",
      "Processed batch 1490 of 2138\n",
      "Processed batch 1491 of 2138\n",
      "Processed batch 1492 of 2138\n",
      "Processed batch 1493 of 2138\n",
      "Processed batch 1494 of 2138\n",
      "Processed batch 1495 of 2138\n",
      "Processed batch 1496 of 2138\n",
      "Processed batch 1497 of 2138\n",
      "Processed batch 1498 of 2138\n",
      "Processed batch 1499 of 2138\n",
      "Processed batch 1500 of 2138\n",
      "Processed batch 1501 of 2138\n",
      "Processed batch 1502 of 2138\n",
      "Processed batch 1503 of 2138\n",
      "Processed batch 1504 of 2138\n",
      "Processed batch 1505 of 2138\n",
      "Processed batch 1506 of 2138\n",
      "Processed batch 1507 of 2138\n",
      "Processed batch 1508 of 2138\n",
      "Processed batch 1509 of 2138\n",
      "Processed batch 1510 of 2138\n",
      "Processed batch 1511 of 2138\n",
      "Processed batch 1512 of 2138\n",
      "Processed batch 1513 of 2138\n",
      "Processed batch 1514 of 2138\n",
      "Processed batch 1515 of 2138\n",
      "Processed batch 1516 of 2138\n",
      "Processed batch 1517 of 2138\n",
      "Processed batch 1518 of 2138\n",
      "Processed batch 1519 of 2138\n",
      "Processed batch 1520 of 2138\n",
      "Processed batch 1521 of 2138\n",
      "Processed batch 1522 of 2138\n",
      "Processed batch 1523 of 2138\n",
      "Processed batch 1524 of 2138\n",
      "Processed batch 1525 of 2138\n",
      "Processed batch 1526 of 2138\n",
      "Processed batch 1527 of 2138\n",
      "Processed batch 1528 of 2138\n",
      "Processed batch 1529 of 2138\n",
      "Processed batch 1530 of 2138\n",
      "Processed batch 1531 of 2138\n",
      "Processed batch 1532 of 2138\n",
      "Processed batch 1533 of 2138\n",
      "Processed batch 1534 of 2138\n",
      "Processed batch 1535 of 2138\n",
      "Processed batch 1536 of 2138\n",
      "Processed batch 1537 of 2138\n",
      "Processed batch 1538 of 2138\n",
      "Processed batch 1539 of 2138\n",
      "Processed batch 1540 of 2138\n",
      "Processed batch 1541 of 2138\n",
      "Processed batch 1542 of 2138\n",
      "Processed batch 1543 of 2138\n",
      "Processed batch 1544 of 2138\n",
      "Processed batch 1545 of 2138\n",
      "Processed batch 1546 of 2138\n",
      "Processed batch 1547 of 2138\n",
      "Processed batch 1548 of 2138\n",
      "Processed batch 1549 of 2138\n",
      "Processed batch 1550 of 2138\n",
      "Processed batch 1551 of 2138\n",
      "Processed batch 1552 of 2138\n",
      "Processed batch 1553 of 2138\n",
      "Processed batch 1554 of 2138\n",
      "Processed batch 1555 of 2138\n",
      "Processed batch 1556 of 2138\n",
      "Processed batch 1557 of 2138\n",
      "Processed batch 1558 of 2138\n",
      "Processed batch 1559 of 2138\n",
      "Processed batch 1560 of 2138\n",
      "Processed batch 1561 of 2138\n",
      "Processed batch 1562 of 2138\n",
      "Processed batch 1563 of 2138\n",
      "Processed batch 1564 of 2138\n",
      "Processed batch 1565 of 2138\n",
      "Processed batch 1566 of 2138\n",
      "Processed batch 1567 of 2138\n",
      "Processed batch 1568 of 2138\n",
      "Processed batch 1569 of 2138\n",
      "Processed batch 1570 of 2138\n",
      "Processed batch 1571 of 2138\n",
      "Processed batch 1572 of 2138\n",
      "Processed batch 1573 of 2138\n",
      "Processed batch 1574 of 2138\n",
      "Processed batch 1575 of 2138\n",
      "Processed batch 1576 of 2138\n",
      "Processed batch 1577 of 2138\n",
      "Processed batch 1578 of 2138\n",
      "Processed batch 1579 of 2138\n",
      "Processed batch 1580 of 2138\n",
      "Processed batch 1581 of 2138\n",
      "Processed batch 1582 of 2138\n",
      "Processed batch 1583 of 2138\n",
      "Processed batch 1584 of 2138\n",
      "Processed batch 1585 of 2138\n",
      "Processed batch 1586 of 2138\n",
      "Processed batch 1587 of 2138\n",
      "Processed batch 1588 of 2138\n",
      "Processed batch 1589 of 2138\n",
      "Processed batch 1590 of 2138\n",
      "Processed batch 1591 of 2138\n",
      "Processed batch 1592 of 2138\n",
      "Processed batch 1593 of 2138\n",
      "Processed batch 1594 of 2138\n",
      "Processed batch 1595 of 2138\n",
      "Processed batch 1596 of 2138\n",
      "Processed batch 1597 of 2138\n",
      "Processed batch 1598 of 2138\n",
      "Processed batch 1599 of 2138\n",
      "Processed batch 1600 of 2138\n",
      "Processed batch 1601 of 2138\n",
      "Processed batch 1602 of 2138\n",
      "Processed batch 1603 of 2138\n",
      "Processed batch 1604 of 2138\n",
      "Processed batch 1605 of 2138\n",
      "Processed batch 1606 of 2138\n",
      "Processed batch 1607 of 2138\n",
      "Processed batch 1608 of 2138\n",
      "Processed batch 1609 of 2138\n",
      "Processed batch 1610 of 2138\n",
      "Processed batch 1611 of 2138\n",
      "Processed batch 1612 of 2138\n",
      "Processed batch 1613 of 2138\n",
      "Processed batch 1614 of 2138\n",
      "Processed batch 1615 of 2138\n",
      "Processed batch 1616 of 2138\n",
      "Processed batch 1617 of 2138\n",
      "Processed batch 1618 of 2138\n",
      "Processed batch 1619 of 2138\n",
      "Processed batch 1620 of 2138\n",
      "Processed batch 1621 of 2138\n",
      "Processed batch 1622 of 2138\n",
      "Processed batch 1623 of 2138\n",
      "Processed batch 1624 of 2138\n",
      "Processed batch 1625 of 2138\n",
      "Processed batch 1626 of 2138\n",
      "Processed batch 1627 of 2138\n",
      "Processed batch 1628 of 2138\n",
      "Processed batch 1629 of 2138\n",
      "Processed batch 1630 of 2138\n",
      "Processed batch 1631 of 2138\n",
      "Processed batch 1632 of 2138\n",
      "Processed batch 1633 of 2138\n",
      "Processed batch 1634 of 2138\n",
      "Processed batch 1635 of 2138\n",
      "Processed batch 1636 of 2138\n",
      "Processed batch 1637 of 2138\n",
      "Processed batch 1638 of 2138\n",
      "Processed batch 1639 of 2138\n",
      "Processed batch 1640 of 2138\n",
      "Processed batch 1641 of 2138\n",
      "Processed batch 1642 of 2138\n",
      "Processed batch 1643 of 2138\n",
      "Processed batch 1644 of 2138\n",
      "Processed batch 1645 of 2138\n",
      "Processed batch 1646 of 2138\n",
      "Processed batch 1647 of 2138\n",
      "Processed batch 1648 of 2138\n",
      "Processed batch 1649 of 2138\n",
      "Processed batch 1650 of 2138\n",
      "Processed batch 1651 of 2138\n",
      "Processed batch 1652 of 2138\n",
      "Processed batch 1653 of 2138\n",
      "Processed batch 1654 of 2138\n",
      "Processed batch 1655 of 2138\n",
      "Processed batch 1656 of 2138\n",
      "Processed batch 1657 of 2138\n",
      "Processed batch 1658 of 2138\n",
      "Processed batch 1659 of 2138\n",
      "Processed batch 1660 of 2138\n",
      "Processed batch 1661 of 2138\n",
      "Processed batch 1662 of 2138\n",
      "Processed batch 1663 of 2138\n",
      "Processed batch 1664 of 2138\n",
      "Processed batch 1665 of 2138\n",
      "Processed batch 1666 of 2138\n",
      "Processed batch 1667 of 2138\n",
      "Processed batch 1668 of 2138\n",
      "Processed batch 1669 of 2138\n",
      "Processed batch 1670 of 2138\n",
      "Processed batch 1671 of 2138\n",
      "Processed batch 1672 of 2138\n",
      "Processed batch 1673 of 2138\n",
      "Processed batch 1674 of 2138\n",
      "Processed batch 1675 of 2138\n",
      "Processed batch 1676 of 2138\n",
      "Processed batch 1677 of 2138\n",
      "Processed batch 1678 of 2138\n",
      "Processed batch 1679 of 2138\n",
      "Processed batch 1680 of 2138\n",
      "Processed batch 1681 of 2138\n",
      "Processed batch 1682 of 2138\n",
      "Processed batch 1683 of 2138\n",
      "Processed batch 1684 of 2138\n",
      "Processed batch 1685 of 2138\n",
      "Processed batch 1686 of 2138\n",
      "Processed batch 1687 of 2138\n",
      "Processed batch 1688 of 2138\n",
      "Processed batch 1689 of 2138\n",
      "Processed batch 1690 of 2138\n",
      "Processed batch 1691 of 2138\n",
      "Processed batch 1692 of 2138\n",
      "Processed batch 1693 of 2138\n",
      "Processed batch 1694 of 2138\n",
      "Processed batch 1695 of 2138\n",
      "Processed batch 1696 of 2138\n",
      "Processed batch 1697 of 2138\n",
      "Processed batch 1698 of 2138\n",
      "Processed batch 1699 of 2138\n",
      "Processed batch 1700 of 2138\n",
      "Processed batch 1701 of 2138\n",
      "Processed batch 1702 of 2138\n",
      "Processed batch 1703 of 2138\n",
      "Processed batch 1704 of 2138\n",
      "Processed batch 1705 of 2138\n",
      "Processed batch 1706 of 2138\n",
      "Processed batch 1707 of 2138\n",
      "Processed batch 1708 of 2138\n",
      "Processed batch 1709 of 2138\n",
      "Processed batch 1710 of 2138\n",
      "Processed batch 1711 of 2138\n",
      "Processed batch 1712 of 2138\n",
      "Processed batch 1713 of 2138\n",
      "Processed batch 1714 of 2138\n",
      "Processed batch 1715 of 2138\n",
      "Processed batch 1716 of 2138\n",
      "Processed batch 1717 of 2138\n",
      "Processed batch 1718 of 2138\n",
      "Processed batch 1719 of 2138\n",
      "Processed batch 1720 of 2138\n",
      "Processed batch 1721 of 2138\n",
      "Processed batch 1722 of 2138\n",
      "Processed batch 1723 of 2138\n",
      "Processed batch 1724 of 2138\n",
      "Processed batch 1725 of 2138\n",
      "Processed batch 1726 of 2138\n",
      "Processed batch 1727 of 2138\n",
      "Processed batch 1728 of 2138\n",
      "Processed batch 1729 of 2138\n",
      "Processed batch 1730 of 2138\n",
      "Processed batch 1731 of 2138\n",
      "Processed batch 1732 of 2138\n",
      "Processed batch 1733 of 2138\n",
      "Processed batch 1734 of 2138\n",
      "Processed batch 1735 of 2138\n",
      "Processed batch 1736 of 2138\n",
      "Processed batch 1737 of 2138\n",
      "Processed batch 1738 of 2138\n",
      "Processed batch 1739 of 2138\n",
      "Processed batch 1740 of 2138\n",
      "Processed batch 1741 of 2138\n",
      "Processed batch 1742 of 2138\n",
      "Processed batch 1743 of 2138\n",
      "Processed batch 1744 of 2138\n",
      "Processed batch 1745 of 2138\n",
      "Processed batch 1746 of 2138\n",
      "Processed batch 1747 of 2138\n",
      "Processed batch 1748 of 2138\n",
      "Processed batch 1749 of 2138\n",
      "Processed batch 1750 of 2138\n",
      "Processed batch 1751 of 2138\n",
      "Processed batch 1752 of 2138\n",
      "Processed batch 1753 of 2138\n",
      "Processed batch 1754 of 2138\n",
      "Processed batch 1755 of 2138\n",
      "Processed batch 1756 of 2138\n",
      "Processed batch 1757 of 2138\n",
      "Processed batch 1758 of 2138\n",
      "Processed batch 1759 of 2138\n",
      "Processed batch 1760 of 2138\n",
      "Processed batch 1761 of 2138\n",
      "Processed batch 1762 of 2138\n",
      "Processed batch 1763 of 2138\n",
      "Processed batch 1764 of 2138\n",
      "Processed batch 1765 of 2138\n",
      "Processed batch 1766 of 2138\n",
      "Processed batch 1767 of 2138\n",
      "Processed batch 1768 of 2138\n",
      "Processed batch 1769 of 2138\n",
      "Processed batch 1770 of 2138\n",
      "Processed batch 1771 of 2138\n",
      "Processed batch 1772 of 2138\n",
      "Processed batch 1773 of 2138\n",
      "Processed batch 1774 of 2138\n",
      "Processed batch 1775 of 2138\n",
      "Processed batch 1776 of 2138\n",
      "Processed batch 1777 of 2138\n",
      "Processed batch 1778 of 2138\n",
      "Processed batch 1779 of 2138\n",
      "Processed batch 1780 of 2138\n",
      "Processed batch 1781 of 2138\n",
      "Processed batch 1782 of 2138\n",
      "Processed batch 1783 of 2138\n",
      "Processed batch 1784 of 2138\n",
      "Processed batch 1785 of 2138\n",
      "Processed batch 1786 of 2138\n",
      "Processed batch 1787 of 2138\n",
      "Processed batch 1788 of 2138\n",
      "Processed batch 1789 of 2138\n",
      "Processed batch 1790 of 2138\n",
      "Processed batch 1791 of 2138\n",
      "Processed batch 1792 of 2138\n",
      "Processed batch 1793 of 2138\n",
      "Processed batch 1794 of 2138\n",
      "Processed batch 1795 of 2138\n",
      "Processed batch 1796 of 2138\n",
      "Processed batch 1797 of 2138\n",
      "Processed batch 1798 of 2138\n",
      "Processed batch 1799 of 2138\n",
      "Processed batch 1800 of 2138\n",
      "Processed batch 1801 of 2138\n",
      "Processed batch 1802 of 2138\n",
      "Processed batch 1803 of 2138\n",
      "Processed batch 1804 of 2138\n",
      "Processed batch 1805 of 2138\n",
      "Processed batch 1806 of 2138\n",
      "Processed batch 1807 of 2138\n",
      "Processed batch 1808 of 2138\n",
      "Processed batch 1809 of 2138\n",
      "Processed batch 1810 of 2138\n",
      "Processed batch 1811 of 2138\n",
      "Processed batch 1812 of 2138\n",
      "Processed batch 1813 of 2138\n",
      "Processed batch 1814 of 2138\n",
      "Processed batch 1815 of 2138\n",
      "Processed batch 1816 of 2138\n",
      "Processed batch 1817 of 2138\n",
      "Processed batch 1818 of 2138\n",
      "Processed batch 1819 of 2138\n",
      "Processed batch 1820 of 2138\n",
      "Processed batch 1821 of 2138\n",
      "Processed batch 1822 of 2138\n",
      "Processed batch 1823 of 2138\n",
      "Processed batch 1824 of 2138\n",
      "Processed batch 1825 of 2138\n",
      "Processed batch 1826 of 2138\n",
      "Processed batch 1827 of 2138\n",
      "Processed batch 1828 of 2138\n",
      "Processed batch 1829 of 2138\n",
      "Processed batch 1830 of 2138\n",
      "Processed batch 1831 of 2138\n",
      "Processed batch 1832 of 2138\n",
      "Processed batch 1833 of 2138\n",
      "Processed batch 1834 of 2138\n",
      "Processed batch 1835 of 2138\n",
      "Processed batch 1836 of 2138\n",
      "Processed batch 1837 of 2138\n",
      "Processed batch 1838 of 2138\n",
      "Processed batch 1839 of 2138\n",
      "Processed batch 1840 of 2138\n",
      "Processed batch 1841 of 2138\n",
      "Processed batch 1842 of 2138\n",
      "Processed batch 1843 of 2138\n",
      "Processed batch 1844 of 2138\n",
      "Processed batch 1845 of 2138\n",
      "Processed batch 1846 of 2138\n",
      "Processed batch 1847 of 2138\n",
      "Processed batch 1848 of 2138\n",
      "Processed batch 1849 of 2138\n",
      "Processed batch 1850 of 2138\n",
      "Processed batch 1851 of 2138\n",
      "Processed batch 1852 of 2138\n",
      "Processed batch 1853 of 2138\n",
      "Processed batch 1854 of 2138\n",
      "Processed batch 1855 of 2138\n",
      "Processed batch 1856 of 2138\n",
      "Processed batch 1857 of 2138\n",
      "Processed batch 1858 of 2138\n",
      "Processed batch 1859 of 2138\n",
      "Processed batch 1860 of 2138\n",
      "Processed batch 1861 of 2138\n",
      "Processed batch 1862 of 2138\n",
      "Processed batch 1863 of 2138\n",
      "Processed batch 1864 of 2138\n",
      "Processed batch 1865 of 2138\n",
      "Processed batch 1866 of 2138\n",
      "Processed batch 1867 of 2138\n",
      "Processed batch 1868 of 2138\n",
      "Processed batch 1869 of 2138\n",
      "Processed batch 1870 of 2138\n",
      "Processed batch 1871 of 2138\n",
      "Processed batch 1872 of 2138\n",
      "Processed batch 1873 of 2138\n",
      "Processed batch 1874 of 2138\n",
      "Processed batch 1875 of 2138\n",
      "Processed batch 1876 of 2138\n",
      "Processed batch 1877 of 2138\n",
      "Processed batch 1878 of 2138\n",
      "Processed batch 1879 of 2138\n",
      "Processed batch 1880 of 2138\n",
      "Processed batch 1881 of 2138\n",
      "Processed batch 1882 of 2138\n",
      "Processed batch 1883 of 2138\n",
      "Processed batch 1884 of 2138\n",
      "Processed batch 1885 of 2138\n",
      "Processed batch 1886 of 2138\n",
      "Processed batch 1887 of 2138\n",
      "Processed batch 1888 of 2138\n",
      "Processed batch 1889 of 2138\n",
      "Processed batch 1890 of 2138\n",
      "Processed batch 1891 of 2138\n",
      "Processed batch 1892 of 2138\n",
      "Processed batch 1893 of 2138\n",
      "Processed batch 1894 of 2138\n",
      "Processed batch 1895 of 2138\n",
      "Processed batch 1896 of 2138\n",
      "Processed batch 1897 of 2138\n",
      "Processed batch 1898 of 2138\n",
      "Processed batch 1899 of 2138\n",
      "Processed batch 1900 of 2138\n",
      "Processed batch 1901 of 2138\n",
      "Processed batch 1902 of 2138\n",
      "Processed batch 1903 of 2138\n",
      "Processed batch 1904 of 2138\n",
      "Processed batch 1905 of 2138\n",
      "Processed batch 1906 of 2138\n",
      "Processed batch 1907 of 2138\n",
      "Processed batch 1908 of 2138\n",
      "Processed batch 1909 of 2138\n",
      "Processed batch 1910 of 2138\n",
      "Processed batch 1911 of 2138\n",
      "Processed batch 1912 of 2138\n",
      "Processed batch 1913 of 2138\n",
      "Processed batch 1914 of 2138\n",
      "Processed batch 1915 of 2138\n",
      "Processed batch 1916 of 2138\n",
      "Processed batch 1917 of 2138\n",
      "Processed batch 1918 of 2138\n",
      "Processed batch 1919 of 2138\n",
      "Processed batch 1920 of 2138\n",
      "Processed batch 1921 of 2138\n",
      "Processed batch 1922 of 2138\n",
      "Processed batch 1923 of 2138\n",
      "Processed batch 1924 of 2138\n",
      "Processed batch 1925 of 2138\n",
      "Processed batch 1926 of 2138\n",
      "Processed batch 1927 of 2138\n",
      "Processed batch 1928 of 2138\n",
      "Processed batch 1929 of 2138\n",
      "Processed batch 1930 of 2138\n",
      "Processed batch 1931 of 2138\n",
      "Processed batch 1932 of 2138\n",
      "Processed batch 1933 of 2138\n",
      "Processed batch 1934 of 2138\n",
      "Processed batch 1935 of 2138\n",
      "Processed batch 1936 of 2138\n",
      "Processed batch 1937 of 2138\n",
      "Processed batch 1938 of 2138\n",
      "Processed batch 1939 of 2138\n",
      "Processed batch 1940 of 2138\n",
      "Processed batch 1941 of 2138\n",
      "Processed batch 1942 of 2138\n",
      "Processed batch 1943 of 2138\n",
      "Processed batch 1944 of 2138\n",
      "Processed batch 1945 of 2138\n",
      "Processed batch 1946 of 2138\n",
      "Processed batch 1947 of 2138\n",
      "Processed batch 1948 of 2138\n",
      "Processed batch 1949 of 2138\n",
      "Processed batch 1950 of 2138\n",
      "Processed batch 1951 of 2138\n",
      "Processed batch 1952 of 2138\n",
      "Processed batch 1953 of 2138\n",
      "Processed batch 1954 of 2138\n",
      "Processed batch 1955 of 2138\n",
      "Processed batch 1956 of 2138\n",
      "Processed batch 1957 of 2138\n",
      "Processed batch 1958 of 2138\n",
      "Processed batch 1959 of 2138\n",
      "Processed batch 1960 of 2138\n",
      "Processed batch 1961 of 2138\n",
      "Processed batch 1962 of 2138\n",
      "Processed batch 1963 of 2138\n",
      "Processed batch 1964 of 2138\n",
      "Processed batch 1965 of 2138\n",
      "Processed batch 1966 of 2138\n",
      "Processed batch 1967 of 2138\n",
      "Processed batch 1968 of 2138\n",
      "Processed batch 1969 of 2138\n",
      "Processed batch 1970 of 2138\n",
      "Processed batch 1971 of 2138\n",
      "Processed batch 1972 of 2138\n",
      "Processed batch 1973 of 2138\n",
      "Processed batch 1974 of 2138\n",
      "Processed batch 1975 of 2138\n",
      "Processed batch 1976 of 2138\n",
      "Processed batch 1977 of 2138\n",
      "Processed batch 1978 of 2138\n",
      "Processed batch 1979 of 2138\n",
      "Processed batch 1980 of 2138\n",
      "Processed batch 1981 of 2138\n",
      "Processed batch 1982 of 2138\n",
      "Processed batch 1983 of 2138\n",
      "Processed batch 1984 of 2138\n",
      "Processed batch 1985 of 2138\n",
      "Processed batch 1986 of 2138\n",
      "Processed batch 1987 of 2138\n",
      "Processed batch 1988 of 2138\n",
      "Processed batch 1989 of 2138\n",
      "Processed batch 1990 of 2138\n",
      "Processed batch 1991 of 2138\n",
      "Processed batch 1992 of 2138\n",
      "Processed batch 1993 of 2138\n",
      "Processed batch 1994 of 2138\n",
      "Processed batch 1995 of 2138\n",
      "Processed batch 1996 of 2138\n",
      "Processed batch 1997 of 2138\n",
      "Processed batch 1998 of 2138\n",
      "Processed batch 1999 of 2138\n",
      "Processed batch 2000 of 2138\n",
      "Processed batch 2001 of 2138\n",
      "Processed batch 2002 of 2138\n",
      "Processed batch 2003 of 2138\n",
      "Processed batch 2004 of 2138\n",
      "Processed batch 2005 of 2138\n",
      "Processed batch 2006 of 2138\n",
      "Processed batch 2007 of 2138\n",
      "Processed batch 2008 of 2138\n",
      "Processed batch 2009 of 2138\n",
      "Processed batch 2010 of 2138\n",
      "Processed batch 2011 of 2138\n",
      "Processed batch 2012 of 2138\n",
      "Processed batch 2013 of 2138\n",
      "Processed batch 2014 of 2138\n",
      "Processed batch 2015 of 2138\n",
      "Processed batch 2016 of 2138\n",
      "Processed batch 2017 of 2138\n",
      "Processed batch 2018 of 2138\n",
      "Processed batch 2019 of 2138\n",
      "Processed batch 2020 of 2138\n",
      "Processed batch 2021 of 2138\n",
      "Processed batch 2022 of 2138\n",
      "Processed batch 2023 of 2138\n",
      "Processed batch 2024 of 2138\n",
      "Processed batch 2025 of 2138\n",
      "Processed batch 2026 of 2138\n",
      "Processed batch 2027 of 2138\n",
      "Processed batch 2028 of 2138\n",
      "Processed batch 2029 of 2138\n",
      "Processed batch 2030 of 2138\n",
      "Processed batch 2031 of 2138\n",
      "Processed batch 2032 of 2138\n",
      "Processed batch 2033 of 2138\n",
      "Processed batch 2034 of 2138\n",
      "Processed batch 2035 of 2138\n",
      "Processed batch 2036 of 2138\n",
      "Processed batch 2037 of 2138\n",
      "Processed batch 2038 of 2138\n",
      "Processed batch 2039 of 2138\n",
      "Processed batch 2040 of 2138\n",
      "Processed batch 2041 of 2138\n",
      "Processed batch 2042 of 2138\n",
      "Processed batch 2043 of 2138\n",
      "Processed batch 2044 of 2138\n",
      "Processed batch 2045 of 2138\n",
      "Processed batch 2046 of 2138\n",
      "Processed batch 2047 of 2138\n",
      "Processed batch 2048 of 2138\n",
      "Processed batch 2049 of 2138\n",
      "Processed batch 2050 of 2138\n",
      "Processed batch 2051 of 2138\n",
      "Processed batch 2052 of 2138\n",
      "Processed batch 2053 of 2138\n",
      "Processed batch 2054 of 2138\n",
      "Processed batch 2055 of 2138\n",
      "Processed batch 2056 of 2138\n",
      "Processed batch 2057 of 2138\n",
      "Processed batch 2058 of 2138\n",
      "Processed batch 2059 of 2138\n",
      "Processed batch 2060 of 2138\n",
      "Processed batch 2061 of 2138\n",
      "Processed batch 2062 of 2138\n",
      "Processed batch 2063 of 2138\n",
      "Processed batch 2064 of 2138\n",
      "Processed batch 2065 of 2138\n",
      "Processed batch 2066 of 2138\n",
      "Processed batch 2067 of 2138\n",
      "Processed batch 2068 of 2138\n",
      "Processed batch 2069 of 2138\n",
      "Processed batch 2070 of 2138\n",
      "Processed batch 2071 of 2138\n",
      "Processed batch 2072 of 2138\n",
      "Processed batch 2073 of 2138\n",
      "Processed batch 2074 of 2138\n",
      "Processed batch 2075 of 2138\n",
      "Processed batch 2076 of 2138\n",
      "Processed batch 2077 of 2138\n",
      "Processed batch 2078 of 2138\n",
      "Processed batch 2079 of 2138\n",
      "Processed batch 2080 of 2138\n",
      "Processed batch 2081 of 2138\n",
      "Processed batch 2082 of 2138\n",
      "Processed batch 2083 of 2138\n",
      "Processed batch 2084 of 2138\n",
      "Processed batch 2085 of 2138\n",
      "Processed batch 2086 of 2138\n",
      "Processed batch 2087 of 2138\n",
      "Processed batch 2088 of 2138\n",
      "Processed batch 2089 of 2138\n",
      "Processed batch 2090 of 2138\n",
      "Processed batch 2091 of 2138\n",
      "Processed batch 2092 of 2138\n",
      "Processed batch 2093 of 2138\n",
      "Processed batch 2094 of 2138\n",
      "Processed batch 2095 of 2138\n",
      "Processed batch 2096 of 2138\n",
      "Processed batch 2097 of 2138\n",
      "Processed batch 2098 of 2138\n",
      "Processed batch 2099 of 2138\n",
      "Processed batch 2100 of 2138\n",
      "Processed batch 2101 of 2138\n",
      "Processed batch 2102 of 2138\n",
      "Processed batch 2103 of 2138\n",
      "Processed batch 2104 of 2138\n",
      "Processed batch 2105 of 2138\n",
      "Processed batch 2106 of 2138\n",
      "Processed batch 2107 of 2138\n",
      "Processed batch 2108 of 2138\n",
      "Processed batch 2109 of 2138\n",
      "Processed batch 2110 of 2138\n",
      "Processed batch 2111 of 2138\n",
      "Processed batch 2112 of 2138\n",
      "Processed batch 2113 of 2138\n",
      "Processed batch 2114 of 2138\n",
      "Processed batch 2115 of 2138\n",
      "Processed batch 2116 of 2138\n",
      "Processed batch 2117 of 2138\n",
      "Processed batch 2118 of 2138\n",
      "Processed batch 2119 of 2138\n",
      "Processed batch 2120 of 2138\n",
      "Processed batch 2121 of 2138\n",
      "Processed batch 2122 of 2138\n",
      "Processed batch 2123 of 2138\n",
      "Processed batch 2124 of 2138\n",
      "Processed batch 2125 of 2138\n",
      "Processed batch 2126 of 2138\n",
      "Processed batch 2127 of 2138\n",
      "Processed batch 2128 of 2138\n",
      "Processed batch 2129 of 2138\n",
      "Processed batch 2130 of 2138\n",
      "Processed batch 2131 of 2138\n",
      "Processed batch 2132 of 2138\n",
      "Processed batch 2133 of 2138\n",
      "Processed batch 2134 of 2138\n",
      "Processed batch 2135 of 2138\n",
      "Processed batch 2136 of 2138\n",
      "Processed batch 2137 of 2138\n",
      "Processed batch 2138 of 2138\n",
      "Embeddings generation completed.\n",
      "Saving embeddings...\n",
      "Embeddings saved to /Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset successfully.\n",
      "Loading existing sentiment model...\n",
      "Loading existing emotion model...\n",
      "Loading existing OneHotEncoder...\n",
      "Applying one-hot encoding to features...\n",
      "Loading existing K-means model...\n",
      "Data and embeddings saved to /Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set directories for saving models and loading datasets\n",
    "model_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Machine Learning models/Trained Models/Version7.7'\n",
    "data_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset/News_cleaned.csv'\n",
    "output_data_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset'\n",
    "output_data_path = os.path.join(output_data_dir, 'News_Features_Engineered_Dataset.parquet')\n",
    "embedding_output_path = os.path.join(output_data_dir, 'news_embeddings_BERT_and_Kmeans.npy')\n",
    "\n",
    "# Ensure model directory exists\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Allowing for parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Set the device to MPS if available, otherwise use CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Helper function to standardize News ID format\n",
    "def standardize_news_id(nid):\n",
    "    \"\"\"Standardize News ID by removing extra spaces.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \"\", nid) if isinstance(nid, str) else nid\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv(data_path)\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# Standardize News ID\n",
    "print(\"Standardizing News IDs...\")\n",
    "data['News ID'] = data['News ID'].apply(standardize_news_id)\n",
    "\n",
    "# Combine text columns into a single representation for each article\n",
    "print(\"Combining text columns...\")\n",
    "data[\"text\"] = (\n",
    "    data[\"Category\"] + \" \" +\n",
    "    data[\"Subcategory\"] + \" \" +\n",
    "    data[\"Title\"] + \" \" +\n",
    "    data[\"Abstract\"]\n",
    ")\n",
    "\n",
    "# Fine-Tune BERT for MLM\n",
    "def fine_tune_bert_for_mlm(data, model_dir):\n",
    "    \"\"\"\n",
    "    Fine-tune BERT for Masked Language Modeling (MLM).\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data containing a \"text\" column.\n",
    "        model_dir (str): Directory to save tokenizer and model.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Fine-tuning BERT for Masked Language Modeling...\")\n",
    "    \n",
    "    # Convert dataset to Hugging Face Dataset format\n",
    "    print(\"Converting dataset...\")\n",
    "    dataset = Dataset.from_pandas(data[[\"text\"]])\n",
    "\n",
    "    # Load or train tokenizer\n",
    "    tokenizer_path = os.path.join(model_dir, \"bert_tokenizer\")\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        print(\"Loading existing BERT tokenizer...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "    else:\n",
    "        print(\"Training new BERT tokenizer...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        tokenizer.save_pretrained(tokenizer_path)\n",
    "        print(f\"BERT tokenizer trained and saved to {tokenizer_path}\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    print(\"Tokenization completed.\")\n",
    "\n",
    "    # Data collator for MLM\n",
    "    print(\"Initializing data collator for MLM...\")\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "    # Load or use existing model\n",
    "    model_path = os.path.join(model_dir, \"bert_mlm_model\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing BERT model for Masked Language Modeling...\")\n",
    "        model = BertForMaskedLM.from_pretrained(model_path)\n",
    "    else:\n",
    "        print(\"Training new BERT model for Masked Language Modeling...\")\n",
    "        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Training arguments\n",
    "        print(\"Setting up training arguments...\")\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_dir,\n",
    "            eval_strategy=\"no\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            save_steps=10_000,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=500\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        print(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        print(\"Saving trained BERT model...\")\n",
    "        model.save_pretrained(model_path)\n",
    "        print(f\"BERT model trained and saved to {model_path}\")\n",
    "\n",
    "    print(\"BERT fine-tuning completed successfully.\")\n",
    "    \n",
    "fine_tune_bert_for_mlm(data, model_dir)\n",
    "\n",
    "# Generate Embeddings\n",
    "def generate_embeddings(data, model_dir, output_data_dir, device, output_dim=768):\n",
    "    \"\"\"\n",
    "    Generate 768-dimensional embeddings for text using a fine-tuned BERT model.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data containing a \"text\" column.\n",
    "        model_dir (str): Directory containing the fine-tuned model.\n",
    "        output_data_dir (str): Directory to save the generated embeddings.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        output_dim (int): Dimension of the output embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Generating embeddings...\")\n",
    "    \n",
    "    # Load BERT model and tokenizer\n",
    "    bert_model_path = os.path.join(model_dir, \"bert_mlm_model\")\n",
    "    print(f\"Loading BERT model from {bert_model_path}...\")\n",
    "    bert_model = BertModel.from_pretrained(bert_model_path).to(device)\n",
    "    print(\"BERT model loaded successfully.\")\n",
    "\n",
    "    tokenizer_path = os.path.join(model_dir, \"bert_tokenizer\")\n",
    "    print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "    # Batch processing for embedding generation\n",
    "    def generate_embeddings_batch(texts, model, tokenizer, device, batch_size=32):\n",
    "        model.eval()\n",
    "        embeddings = []\n",
    "        print(\"Generating embeddings in batches...\")\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                inputs = tokenizer(\n",
    "                    batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "                ).to(device)\n",
    "                outputs = model(**inputs)\n",
    "                embeddings.append(outputs.pooler_output.cpu().numpy())\n",
    "                print(f\"Processed batch {i // batch_size + 1} of {len(texts) // batch_size + 1}\")\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    print(\"Generating embeddings for the dataset...\")\n",
    "    embeddings = generate_embeddings_batch(data[\"text\"].tolist(), bert_model, tokenizer, device)\n",
    "    print(\"Embeddings generation completed.\")\n",
    "\n",
    "    # Save embeddings\n",
    "    print(\"Saving embeddings...\")\n",
    "    data[\"BERT-Embeddings\"] = embeddings.tolist()\n",
    "    np.save(os.path.join(output_data_dir, \"news_embeddings.npy\"), embeddings)\n",
    "    data[[\"News ID\", \"BERT-Embeddings\"]].to_parquet(os.path.join(output_data_dir, \"news_BERT_Only_embeddings.parquet\"), index=False)\n",
    "    print(f\"Embeddings saved to {output_data_dir} successfully.\")\n",
    "\n",
    "generate_embeddings(data, model_dir, output_data_dir, device, output_dim=768)\n",
    "\n",
    "# Step 3: Sentiment and Emotion Analysis\n",
    "sentiment_model_dir = os.path.join(model_dir, \"sentiment_model\")\n",
    "if not os.path.exists(sentiment_model_dir):\n",
    "    print(\"Training and saving sentiment model...\")\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        revision=\"714eb0f\",\n",
    "        device=0 if torch.backends.mps.is_available() else -1\n",
    "    )\n",
    "    sentiment_pipeline.model.save_pretrained(sentiment_model_dir)\n",
    "    sentiment_pipeline.tokenizer.save_pretrained(sentiment_model_dir)\n",
    "    print(\"Sentiment model trained and saved.\")\n",
    "else:\n",
    "    print(\"Loading existing sentiment model...\")\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=sentiment_model_dir,\n",
    "        device=0 if torch.backends.mps.is_available() else -1\n",
    "    )\n",
    "\n",
    "data['Sentiment'] = data.apply(\n",
    "    lambda row: sentiment_pipeline(row['Title'][:512] + \" \" + row['Abstract'][:512])[0]['label'], axis=1\n",
    ")\n",
    "\n",
    "emotion_model_dir = os.path.join(model_dir, \"emotion_model\")\n",
    "if not os.path.exists(emotion_model_dir):\n",
    "    print(\"Training and saving emotion model...\")\n",
    "    emotion_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "        device=0 if torch.backends.mps.is_available() else -1\n",
    "    )\n",
    "    emotion_pipeline.model.save_pretrained(emotion_model_dir)\n",
    "    emotion_pipeline.tokenizer.save_pretrained(emotion_model_dir)\n",
    "    print(\"Emotion model trained and saved.\")\n",
    "else:\n",
    "    print(\"Loading existing emotion model...\")\n",
    "    emotion_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=emotion_model_dir,\n",
    "        device=0 if torch.backends.mps.is_available() else -1\n",
    "    )\n",
    "\n",
    "data['Emotion'] = data.apply(\n",
    "    lambda row: emotion_pipeline(row['Title'][:512] + \" \" + row['Abstract'][:512])[0]['label'], axis=1\n",
    ")\n",
    "\n",
    "# Step 4: One-Hot Encoding and K-means Clustering\n",
    "encoder_path = os.path.join(model_dir, 'one_hot_encoder.pkl')\n",
    "\n",
    "# Combine features as they are\n",
    "combined_features = np.column_stack((\n",
    "    data['Category'].astype(str), \n",
    "    data['Subcategory'].astype(str), \n",
    "    data['Sentiment'].astype(str), \n",
    "    data['Emotion'].astype(str)\n",
    "))\n",
    "\n",
    "# Train or load the one-hot encoder\n",
    "if os.path.exists(encoder_path):\n",
    "    print(\"Loading existing OneHotEncoder...\")\n",
    "    encoder = joblib.load(encoder_path)\n",
    "else:\n",
    "    print(\"Training new OneHotEncoder...\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoder.fit(combined_features)\n",
    "    joblib.dump(encoder, encoder_path)\n",
    "    print(f\"OneHotEncoder trained and saved to {encoder_path}\")\n",
    "\n",
    "# Apply one-hot encoding\n",
    "print(\"Applying one-hot encoding to features...\")\n",
    "one_hot_encoded_features = encoder.transform(combined_features)\n",
    "\n",
    "# Proceed with K-means clustering\n",
    "kmeans_path = os.path.join(model_dir, 'kmeans_model.pkl')\n",
    "if os.path.exists(kmeans_path):\n",
    "    print(\"Loading existing K-means model...\")\n",
    "    kmeans = joblib.load(kmeans_path)\n",
    "else:\n",
    "    print(\"Training K-means model...\")\n",
    "    max_k = 100\n",
    "    best_k = 2\n",
    "    best_score = -1\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans_candidate = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=0)\n",
    "        cluster_labels = kmeans_candidate.fit_predict(one_hot_encoded_features)\n",
    "        score = silhouette_score(one_hot_encoded_features, cluster_labels)\n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "    kmeans = MiniBatchKMeans(n_clusters=best_k, batch_size=1000, random_state=0)\n",
    "    kmeans.fit(one_hot_encoded_features)\n",
    "    joblib.dump(kmeans, kmeans_path)\n",
    "    print(\"K-means model trained and saved.\")\n",
    "\n",
    "# Assign clusters to data\n",
    "data['K-means Clusters'] = kmeans.predict(one_hot_encoded_features)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Add K-means embeddings to data\n",
    "data['K-means Embeddings'] = data['K-means Clusters'].apply(lambda x: centroids[x].tolist())\n",
    "\n",
    "# Save final data and embeddings\n",
    "data[['News ID', 'BERT-Embeddings', 'Sentiment', 'Emotion', 'K-means Clusters', 'K-means Embeddings']].to_parquet(output_data_path, index=False)\n",
    "news_embeddings = {\n",
    "    \"News ID\": data['News ID'].tolist(),\n",
    "    \"BERT-Embeddings\": np.array(data['BERT-Embeddings'].tolist()),\n",
    "    \"K-means Embeddings\": np.array(data['K-means Embeddings'].tolist())\n",
    "}\n",
    "np.save(embedding_output_path, news_embeddings)\n",
    "\n",
    "print(f\"Data and embeddings saved to {output_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69048153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types before conversion:\n",
      " User ID                 object\n",
      "Displayed News List     object\n",
      "Clicked News IDs        object\n",
      "Not-Clicked News IDs    object\n",
      "dtype: object\n",
      "No concatenated IDs found in 'Displayed News List'.\n",
      "No concatenated IDs found in 'Clicked News IDs'.\n",
      "No concatenated IDs found in 'Not-Clicked News IDs'.\n",
      "Data types after conversion:\n",
      " User ID                 object\n",
      "Displayed News List     object\n",
      "Clicked News IDs        object\n",
      "Not-Clicked News IDs    object\n",
      "dtype: object\n",
      "Processed Behavior Dataset:\n",
      "    User ID                                Displayed News List  \\\n",
      "0  U134050  N12246,N128820,N119226,N4065,N67770,N33446,N10...   \n",
      "1  U254959  N34011,N9375,N67397,N7936,N118985,N109453,N103...   \n",
      "2  U499841  N63858,N26834,N6379,N85484,N15229,N65119,N1047...   \n",
      "3  U107107  N12959,N8085,N18389,N3758,N9740,N90543,N129790...   \n",
      "4  U492344  N109183,N48453,N85005,N45706,N98923,N46069,N35...   \n",
      "5  U657892                       N66666,N88230,N105366,N67497   \n",
      "6  U441763         N68325,N31649,N28616,N84454,N64490,N104550   \n",
      "7  U170615  N13193,N82882,N57676,N72331,N37175,N110161,N10...   \n",
      "8  U114779  N14678,N35786,N91030,N76664,N40228,N47866,N757...   \n",
      "9  U224919  N69106,N87211,N71082,N47193,N13423,N49262,N395...   \n",
      "\n",
      "                            Clicked News IDs  \\\n",
      "0                                     N69938   \n",
      "1  N18468,N34044,N86067,N105956,N3663,N46641   \n",
      "2                                     N49978   \n",
      "3                             N122944,N54368   \n",
      "4                                    N122640   \n",
      "5                                     N58465   \n",
      "6                                     N92905   \n",
      "7                            N118880,N121138   \n",
      "8                                     N18468   \n",
      "9                                     N92905   \n",
      "\n",
      "                                Not-Clicked News IDs  \n",
      "0  N91737,N30206,N54368,N117802,N18190,N122944,N1...  \n",
      "1  N119999,N24958,N104054,N33901,N9250,N33378,N11...  \n",
      "2  N18190,N89764,N91737,N54368,N29160,N117802,N12...  \n",
      "3  N18190,N55801,N59297,N128045,N29160,N122640,N6...  \n",
      "4  N64785,N82503,N32993,N122944,N29160,N62800,N72...  \n",
      "5  N64785,N130076,N30532,N29016,N30582,N40742,N10...  \n",
      "6  N2110,N123209,N112536,N118908,N119999,N91737,N...  \n",
      "7  N83732,N56565,N129740,N29160,N105956,N123209,N...  \n",
      "8          N21018,N89764,N46641,N58760,N2110,N129416  \n",
      "9  N39770,N89441,N46641,N92476,N24165,N110308,N44...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the behavior dataset\n",
    "behavior_data_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset/cleaned_behavior_dataset.csv'\n",
    "behavior_data = pd.read_csv(behavior_data_path)\n",
    "\n",
    "# Columns to retain\n",
    "columns_to_keep = ['User ID', 'Displayed News List', 'Clicked News IDs', 'Not-Clicked News IDs']\n",
    "\n",
    "# Drop all columns except the ones specified\n",
    "behavior_data = behavior_data[columns_to_keep]\n",
    "\n",
    "# Check and display the data types of each column before processing\n",
    "print(\"Data types before conversion:\\n\", behavior_data.dtypes)\n",
    "\n",
    "# Function to convert to comma-separated and handle concatenated news IDs\n",
    "def convert_to_comma_separated(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"  # Handle NaN values\n",
    "    # Remove brackets, quotes, spaces, and any unwanted characters\n",
    "    value = re.sub(r\"[\\[\\]'\\\" ]\", \"\", value)\n",
    "    # Add comma between concatenated IDs using regex to find patterns like 'N[digits]N[digits]'\n",
    "    value = re.sub(r\"(N\\d+)(?=N\\d+)\", r\"\\1,\", value)\n",
    "    return value\n",
    "\n",
    "# Apply conversion to 'Displayed News List', 'Clicked News IDs', and 'Not-Clicked News IDs'\n",
    "behavior_data['Displayed News List'] = behavior_data['Displayed News List'].apply(convert_to_comma_separated)\n",
    "behavior_data['Clicked News IDs'] = behavior_data['Clicked News IDs'].apply(convert_to_comma_separated)\n",
    "behavior_data['Not-Clicked News IDs'] = behavior_data['Not-Clicked News IDs'].apply(convert_to_comma_separated)\n",
    "\n",
    "# Validation function to confirm no concatenated IDs remain\n",
    "def check_concatenated_ids(column):\n",
    "    pattern = r\"N\\d+N\\d+\"  # Pattern to detect concatenated IDs\n",
    "    concatenated_ids = behavior_data[column].str.contains(pattern, regex=True).sum()\n",
    "    return concatenated_ids == 0\n",
    "\n",
    "# Check for concatenated IDs in all relevant columns\n",
    "columns_to_check = ['Displayed News List', 'Clicked News IDs', 'Not-Clicked News IDs']\n",
    "for col in columns_to_check:\n",
    "    if check_concatenated_ids(col):\n",
    "        print(f\"No concatenated IDs found in '{col}'.\")\n",
    "    else:\n",
    "        print(f\"Concatenated IDs detected in '{col}' after processing.\")\n",
    "\n",
    "# Ensure 'Clicked News IDs' and 'Not-Clicked News IDs' have the same data type as 'Displayed News List'\n",
    "displayed_news_type = behavior_data['Displayed News List'].dtype\n",
    "behavior_data['Clicked News IDs'] = behavior_data['Clicked News IDs'].astype(displayed_news_type)\n",
    "behavior_data['Not-Clicked News IDs'] = behavior_data['Not-Clicked News IDs'].astype(displayed_news_type)\n",
    "\n",
    "# Check and display the data types after conversion to confirm changes\n",
    "print(\"Data types after conversion:\\n\", behavior_data.dtypes)\n",
    "\n",
    "# Display the first few rows to confirm the dataset structure\n",
    "print(\"Processed Behavior Dataset:\\n\", behavior_data.head(10))\n",
    "\n",
    "# Save the processed dataset if needed\n",
    "output_behavior_data_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset/processed_behavior_dataset.csv'\n",
    "behavior_data.to_csv(output_behavior_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a597a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f48fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading saved user profiles...\n",
      "Calculating default profile for unseen users...\n",
      "Step 1: Loading news embeddings.\n",
      "Number of errors encountered during embedding validation: 0\n",
      "  News ID                                    BERT-Embeddings Sentiment  \\\n",
      "0  N88753  [-0.03282312, -0.12860948, 0.24515766, -0.2115...  POSITIVE   \n",
      "1  N23144  [0.15914527, -0.22159758, -0.027823929, -0.175...  NEGATIVE   \n",
      "2  N93187  [-0.11768901, -0.13728267, 0.26656207, -0.3962...  NEGATIVE   \n",
      "3  N75236  [0.18441504, -0.23967472, 0.03682767, -0.11220...  NEGATIVE   \n",
      "4  N99744  [0.13281766, -0.33782175, 0.054901987, 0.10067...  NEGATIVE   \n",
      "\n",
      "   Emotion  K-means Clusters  \\\n",
      "0    anger                10   \n",
      "1      joy                66   \n",
      "2     fear                37   \n",
      "3  sadness                72   \n",
      "4      joy                66   \n",
      "\n",
      "                                  K-means Embeddings  \n",
      "0  [0.05628373, 0.00539707, 0.07555898, 0.0485736...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "BERT Embedding Size: 768\n",
      "K-means Embedding Size: 304\n",
      "Step 2: Concatenate embeddings and news embeddings map generation.\n",
      "Padded embeddings from size 1072 to 1088.\n",
      "Shape of concatenated embeddings: (68395, 1072)\n",
      "Shape of padded embeddings: (68395, 1088)\n",
      "Sample news embeddings map: [('N88753', tensor([-0.0328, -0.1286,  0.2452,  ...,  0.0000,  0.0000,  0.0000])), ('N23144', tensor([ 0.1591, -0.2216, -0.0278,  ...,  0.0000,  0.0000,  0.0000])), ('N93187', tensor([-0.1177, -0.1373,  0.2666,  ...,  0.0000,  0.0000,  0.0000])), ('N75236', tensor([ 0.1844, -0.2397,  0.0368,  ...,  0.0000,  0.0000,  0.0000])), ('N99744', tensor([ 0.1328, -0.3378,  0.0549,  ...,  0.0000,  0.0000,  0.0000]))]\n",
      "Step 3: Loading behavior validation data...\n",
      "First 5 rows after processing:\n",
      "   User ID                                  Clicked News IDs  \\\n",
      "0  U134050                                          [N69938]   \n",
      "1  U254959  [N18468, N34044, N86067, N105956, N3663, N46641]   \n",
      "2  U499841                                          [N49978]   \n",
      "3  U107107                                 [N122944, N54368]   \n",
      "4  U492344                                         [N122640]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Not-Clicked News IDs  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [N91737, N30206, N54368, N117802, N18190, N122944, N18356, N123209, N46894, N29160, N49978, N89764, N30582, N58465, N35304]   \n",
      "1  [N119999, N24958, N104054, N33901, N9250, N33378, N110967, N14612, N52388, N53018, N19586, N51573, N70459, N30206, N96351, N56505, N130076, N27478, N91865, N20399, N23721, N53661, N92199, N17488, N30532, N67588, N66002, N48883, N7382, N2756, N93879, N18356, N125192, N32694, N3475, N87236, N27669, N83491, N129416, N95108, N18258, N95862, N123968, N44493, N38042, N107521, N92476, N122775, N58541, N110308, N62456, N92905, N108805, N42208, N28518, N110025, N79671, N83374, N57663, N66180, N69938, N9471, N125720, N70024, N8515, N87928, N111798, N79372, N89348, N48833, N37648, N118908, N34548, N11405, N126472, N56999, N28067, N73774, N79048, N89764, N52371, N43000, N29160, N103830, N42518, N35304, N63206, N2110, N71001, N21018, N72054, N70038, N126696, N26342, N86352, N18405, N73833, N121267, N40742, N10516, ...]   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [N18190, N89764, N91737, N54368, N29160, N117802, N120089]   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [N18190, N55801, N59297, N128045, N29160, N122640, N69938, N104644]   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [N64785, N82503, N32993, N122944, N29160, N62800, N72977, N117695, N18190, N110709, N40742, N80770, N128045, N27669, N127357, N104054, N54368, N55801, N40795, N14719, N15210, N35304, N9989, N14612, N4378, N130076, N48360, N100425, N39770, N98252, N16716, N241, N3623, N104644, N14857, N52388, N18258, N87470, N59297, N106619, N82573, N69938]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Displayed News List  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [N30206, N69938, N29160, N18190, N18356, N117802, N35304, N49978, N30582, N122944, N54368, N89764, N58465, N91737, N123209, N46894]  \n",
      "1  [N69938, N87236, N14612, N121267, N96351, N52388, N107521, N83491, N85420, N92905, N9250, N33901, N70459, N40742, N86067, N58541, N116564, N112536, N17488, N126472, N42518, N34044, N70024, N89764, N110025, N95108, N56602, N56999, N83374, N21018, N92199, N48883, N70038, N33378, N43000, N91737, N29160, N95862, N62456, N18468, N105956, N24958, N22562, N118908, N23452, N32694, N110967, N125720, N9471, N53661, N53018, N125192, N18258, N104054, N35304, N71171, N71001, N56505, N8515, N19586, N126696, N18356, N100425, N30206, N42208, N92476, N27478, N28518, N48833, N63206, N87928, N92462, N10516, N27669, N51573, N89348, N122775, N130076, N44493, N73833, N86352, N57663, N73774, N11405, N3475, N129416, N38042, N66180, N3663, N66002, N93879, N111798, N110308, N79372, N108805, N52371, N119999, N23721, N67588, N79048, ...]  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [N117802, N89764, N91737, N120089, N54368, N18190, N29160, N49978]  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [N55801, N54368, N104644, N122640, N69938, N122944, N128045, N59297, N18190, N29160]  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [N59297, N35304, N15210, N117695, N4378, N14719, N40795, N54368, N62800, N98252, N87470, N40742, N106619, N9989, N122944, N80770, N110709, N82503, N241, N27669, N104054, N69938, N130076, N100425, N128045, N48360, N29160, N52388, N14612, N16716, N72977, N18190, N127357, N39770, N55801, N64785, N14857, N32993, N122640, N3623, N18258, N82573, N104644]  \n",
      "Validation data size after filtering: 365201 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Directory paths\n",
    "model_dir = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Machine Learning models/Trained Models/Version7.7'\n",
    "validation_data_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset/News_Features_Engineered_Dataset.parquet'\n",
    "behavior_validation_path = '/Users/n7/Desktop/ie University SAMBD Acadamics/Capstone Project Revised/Data/MINDlarge_dev/Cleaned Dataset/processed_behavior_dataset.csv'\n",
    "user_profiles_path = os.path.join(model_dir, 'user_profiles.pkl')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'best_model_epoch_13.pt')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load user profiles\n",
    "print(\"Loading saved user profiles...\")\n",
    "with open(user_profiles_path, 'rb') as f:\n",
    "    user_profiles = pickle.load(f)\n",
    "\n",
    "# Generate default profile for unseen users\n",
    "print(\"Calculating default profile for unseen users...\")\n",
    "all_preferences = [profile['preference_profile'] for profile in user_profiles.values()]\n",
    "all_non_preferences = [profile['non_preference_profile'] for profile in user_profiles.values()]\n",
    "\n",
    "default_preference = torch.stack(all_preferences).mean(dim=0)\n",
    "default_non_preference = torch.stack(all_non_preferences).mean(dim=0)\n",
    "default_user_profile = {'preference_profile': default_preference, 'non_preference_profile': default_non_preference}\n",
    "\n",
    "# Helper function to standardize News ID format\n",
    "def standardize_news_id(nid):\n",
    "    \"\"\"Standardize News ID by removing extra spaces.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \"\", nid) if isinstance(nid, str) else nid\n",
    "\n",
    "# Helper function to standardize User ID format\n",
    "def standardize_user_id(uid):\n",
    "    \"\"\"Standardize User ID by removing extra spaces and applying a consistent format.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \"\", str(uid)) if isinstance(uid, (str, int)) else uid\n",
    "\n",
    "# Step 1: Load news embeddings\n",
    "print(\"Step 1: Loading news embeddings.\")\n",
    "\n",
    "def load_parquet_embeddings_with_error_checking(validation_data_path):\n",
    "    \"\"\"\n",
    "    Load embeddings from a Parquet file, validate them, and count any errors during processing.\n",
    "\n",
    "    Args:\n",
    "        validation_data_path (str): Path to the Parquet file containing saved embeddings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing News ID, BERT-Embeddings, and K-means Embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize error count\n",
    "    error_count = 0\n",
    "\n",
    "    # Load the Parquet file\n",
    "    try:\n",
    "        embeddings_df = pd.read_parquet(validation_data_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file {validation_data_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load Parquet file: {e}\")\n",
    "\n",
    "    # Check for required columns\n",
    "    required_columns = [\"News ID\", \"BERT-Embeddings\", \"K-means Embeddings\"]\n",
    "    for column in required_columns:\n",
    "        if column not in embeddings_df.columns:\n",
    "            raise KeyError(f\"Missing expected column in the Parquet file: {column}\")\n",
    "\n",
    "    # Helper function to validate and convert embeddings to float32\n",
    "    def validate_and_convert_embedding(embedding, expected_size):\n",
    "        nonlocal error_count\n",
    "        try:\n",
    "            array = np.array(embedding, dtype=np.float32)  # Convert to float32\n",
    "            if array.size != expected_size:\n",
    "                raise ValueError(f\"Unexpected size: {array.size}\")\n",
    "            return array\n",
    "        except (ValueError, TypeError):\n",
    "            error_count += 1\n",
    "            return np.zeros(expected_size, dtype=np.float32)  # Default to zero array on error\n",
    "\n",
    "    # Expected sizes\n",
    "    bert_embeddings = embeddings_df[\"BERT-Embeddings\"].iloc[0]\n",
    "    kmeans_embeddings = embeddings_df[\"K-means Embeddings\"].iloc[0]\n",
    "    bert_size = len(bert_embeddings) if isinstance(bert_embeddings, (list, np.ndarray)) else 768\n",
    "    kmeans_size = len(kmeans_embeddings) if isinstance(kmeans_embeddings, (list, np.ndarray)) else 304\n",
    "\n",
    "    # Validate embeddings and convert to float32\n",
    "    embeddings_df[\"BERT-Embeddings\"] = embeddings_df[\"BERT-Embeddings\"].apply(lambda x: validate_and_convert_embedding(x, bert_size))\n",
    "    embeddings_df[\"K-means Embeddings\"] = embeddings_df[\"K-means Embeddings\"].apply(lambda x: validate_and_convert_embedding(x, kmeans_size))\n",
    "\n",
    "    # Print the error count\n",
    "    print(f\"Number of errors encountered during embedding validation: {error_count}\")\n",
    "    return embeddings_df\n",
    "\n",
    "# Load the embeddings\n",
    "data = load_parquet_embeddings_with_error_checking(validation_data_path)\n",
    "\n",
    "# Standardize News IDs\n",
    "data['News ID'] = data['News ID'].apply(standardize_news_id)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Concatenate BERT-Embeddings (768 dimensions) and K-means Embeddings (304 dimensions)\n",
    "bert_embedding_size = len(data['BERT-Embeddings'].iloc[0])  # Infer size from the first embedding\n",
    "kmeans_embedding_size = len(data['K-means Embeddings'].iloc[0])  # Infer size from the first embedding\n",
    "\n",
    "# Validate embedding dimensions before concatenation\n",
    "print(f\"BERT Embedding Size: {bert_embedding_size}\")\n",
    "print(f\"K-means Embedding Size: {kmeans_embedding_size}\")\n",
    "\n",
    "# Ensure all embeddings are of expected dimensions\n",
    "assert all(len(embedding) == bert_embedding_size for embedding in data['BERT-Embeddings']), \\\n",
    "    \"Inconsistent BERT embedding size detected.\"\n",
    "assert all(len(embedding) == kmeans_embedding_size for embedding in data['K-means Embeddings']), \\\n",
    "    \"Inconsistent K-means embedding size detected.\"\n",
    "\n",
    "def pad_embeddings(matrix, target_size=1088):\n",
    "    \"\"\"\n",
    "    Pad the concatenated embeddings matrix to a fixed size along the second dimension.\n",
    "    Adds zero-padding to the end of each row if the size is less than the target size (default: 1088).\n",
    "\n",
    "    Parameters:\n",
    "    - matrix (np.ndarray): Input concatenated embeddings matrix.\n",
    "    - target_size (int, optional): Desired size along the second dimension (default: 1088).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Padded matrix with dimensions [matrix.shape[0], target_size].\n",
    "    \"\"\"\n",
    "    current_size = matrix.shape[1]\n",
    "    if current_size < target_size:\n",
    "        # Calculate the padding size\n",
    "        padding_size = target_size - current_size\n",
    "        # Apply zero-padding along the second dimension\n",
    "        padded_matrix = np.pad(matrix, ((0, 0), (0, padding_size)), mode='constant', constant_values=0)\n",
    "        print(f\"Padded embeddings from size {current_size} to {target_size}.\")\n",
    "    else:\n",
    "        padded_matrix = matrix  # No padding needed if already at or above target size\n",
    "        print(f\"No padding applied. Embeddings already of size {current_size}.\")\n",
    "    return padded_matrix\n",
    "\n",
    "# Step 2: Concatenate embeddings and news embeddings map generation\n",
    "print(\"Step 2: Concatenate embeddings and news embeddings map generation.\")\n",
    "embedding_matrix = np.hstack([\n",
    "    np.vstack(data['BERT-Embeddings'].values),  # Convert BERT-Embeddings to matrix form\n",
    "    np.vstack(data['K-means Embeddings'].values)  # Convert K-means Embeddings to matrix form\n",
    "])\n",
    "\n",
    "# Pad concatenated embeddings to a fixed size (e.g., 1088 dimensions)\n",
    "target_size = 1088\n",
    "padded_embedding_matrix = pad_embeddings(embedding_matrix, target_size)\n",
    "\n",
    "# Convert to tensors\n",
    "embedding_tensor = torch.tensor(padded_embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Add the tensor embeddings to the dataset\n",
    "data['combined_embedding'] = [embedding_tensor[i] for i in range(embedding_tensor.size(0))]\n",
    "\n",
    "# Create news embeddings map\n",
    "news_embeddings_map = {row['News ID']: row['combined_embedding'] for _, row in data.iterrows()}\n",
    "\n",
    "# Print information for verification\n",
    "print(f\"Shape of concatenated embeddings: {embedding_matrix.shape}\")\n",
    "print(f\"Shape of padded embeddings: {padded_embedding_matrix.shape}\")\n",
    "print(f\"Sample news embeddings map: {list(news_embeddings_map.items())[:5]}\")\n",
    "\n",
    "# Step 3: Load behavior validation data\n",
    "print(\"Step 3: Loading behavior validation data...\")\n",
    "behavior_df = pd.read_csv(behavior_validation_path)\n",
    "\n",
    "# Standardize User IDs\n",
    "behavior_df['User ID'] = behavior_df['User ID'].apply(standardize_user_id)\n",
    "\n",
    "# Standardize and process 'Clicked News IDs' and 'Not-Clicked News IDs'\n",
    "behavior_df['Clicked News IDs'] = behavior_df['Clicked News IDs'].apply(\n",
    "    lambda x: [standardize_news_id(nid) for nid in x.split(',')] if isinstance(x, str) else []\n",
    ")\n",
    "behavior_df['Not-Clicked News IDs'] = behavior_df['Not-Clicked News IDs'].apply(\n",
    "    lambda x: [standardize_news_id(nid) for nid in x.split(',')] if isinstance(x, str) else []\n",
    ")\n",
    "\n",
    "# Drop rows where 'Clicked News IDs' or 'Not-Clicked News IDs' are empty\n",
    "behavior_df = behavior_df[\n",
    "    (behavior_df['Clicked News IDs'].map(len) > 0) & (behavior_df['Not-Clicked News IDs'].map(len) > 0)\n",
    "]\n",
    "\n",
    "# Combine 'Clicked News IDs' and 'Not-Clicked News IDs' into a randomly ordered 'Displayed News List',\n",
    "behavior_df['Displayed News List'] = behavior_df.apply(\n",
    "    lambda row: np.random.permutation(\n",
    "        row['Clicked News IDs'] + row['Not-Clicked News IDs']\n",
    "    ).tolist(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Standardize 'Displayed News List'\n",
    "behavior_df['Displayed News List'] = behavior_df['Displayed News List'].apply(\n",
    "    lambda x: [standardize_news_id(nid) for nid in x] if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "# Set display options to avoid truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Prevent truncation of column content\n",
    "pd.set_option(\"display.max_rows\", None)  # Prevent truncation of rows\n",
    "\n",
    "# Print first 5 rows for verification\n",
    "print(\"First 5 rows after processing:\")\n",
    "print(behavior_df[['User ID', 'Clicked News IDs', 'Not-Clicked News IDs', 'Displayed News List']].head())\n",
    "\n",
    "# Print size of validation data after filtering\n",
    "print(f\"Validation data size after filtering: {len(behavior_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed645aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cosine similarity for evaluation...\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [1:02:16<00:00, 101.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News IDs Found: 13310314\n",
      "News IDs Not Found: 352128\n",
      "\n",
      "Evaluation Metrics for Clicked Predictions:\n",
      "Precision: 0.0872\n",
      "Recall: 0.5243\n",
      "F1 Score: 0.1286\n",
      "Accuracy: 0.0872\n",
      "\n",
      "Evaluation Metrics for Not-Clicked Predictions:\n",
      "Precision: 0.8772\n",
      "Recall: 0.9447\n",
      "F1 Score: 0.9055\n",
      "Accuracy: 0.8772\n",
      "\n",
      "Logic Part Counters:\n",
      "High Confidence Clicked Predictions: 0\n",
      "High Confidence Not-Clicked Predictions: 0\n",
      "Confidence Margin Resolved Predictions: 13102119\n",
      "Low Confidence Default to Not-Clicked Predictions: 208195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 10000  # Dynamically adjustable\n",
    "\n",
    "# Step 3: Load evaluation matrix using cosine similarity\n",
    "print(\"Using cosine similarity for evaluation...\")\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    return cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "\n",
    "def evaluate_predictions(predicted, actual):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against actual clicked/not-clicked news IDs.\n",
    "    \"\"\"\n",
    "    if len(actual) == 0:\n",
    "        return None\n",
    "    if len(predicted) == 0:\n",
    "        # Handle cases with no predictions\n",
    "        precision, recall, f1, accuracy = 0.0, 0.0, 0.0, 0.0\n",
    "    else:\n",
    "        y_true = [1 if pred in actual else 0 for pred in predicted]\n",
    "        y_pred = [1] * len(predicted)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1, \"Accuracy\": accuracy}\n",
    "\n",
    "def parallel_metric_calculation(metrics_list):\n",
    "    \"\"\"\n",
    "    Calculate average metrics across all batches.\n",
    "    \"\"\"\n",
    "    avg_metrics = defaultdict(float)\n",
    "    total = len(metrics_list)\n",
    "    if total == 0:\n",
    "        return {\"Precision\": 0.0, \"Recall\": 0.0, \"F1 Score\": 0.0, \"Accuracy\": 0.0}\n",
    "    for metrics in metrics_list:\n",
    "        for key, value in metrics.items():\n",
    "            avg_metrics[key] += value\n",
    "    for key in avg_metrics.keys():\n",
    "        avg_metrics[key] /= total\n",
    "    return avg_metrics\n",
    "\n",
    "# Step 4: Validation loop\n",
    "print(\"Starting validation...\")\n",
    "clicked_metrics = []\n",
    "not_clicked_metrics = []\n",
    "news_ids_found = 0\n",
    "news_ids_not_found = 0\n",
    "\n",
    "# Counters for each logic part\n",
    "high_conf_clicked = 0\n",
    "high_conf_not_clicked = 0\n",
    "confidence_margin_resolved = 0\n",
    "low_conf_not_clicked = 0\n",
    "\n",
    "for batch_start in tqdm(range(0, len(behavior_df), BATCH_SIZE)):\n",
    "    batch_df = behavior_df.iloc[batch_start:batch_start + BATCH_SIZE]\n",
    "    for _, row in batch_df.iterrows():\n",
    "        user_id = row['User ID']\n",
    "        displayed_news_ids = row['Displayed News List']\n",
    "        clicked_news_ids = row['Clicked News IDs']\n",
    "        not_clicked_news_ids = row['Not-Clicked News IDs']\n",
    "\n",
    "        user_profile = user_profiles.get(user_id, default_user_profile)\n",
    "        preference_profile = user_profile['preference_profile']\n",
    "        non_preference_profile = user_profile['non_preference_profile']\n",
    "        predicted_clicked, predicted_not_clicked = [], []\n",
    "\n",
    "        for news_id in displayed_news_ids:\n",
    "            news_embedding = news_embeddings_map.get(news_id)\n",
    "            if news_embedding is None:\n",
    "                news_ids_not_found += 1\n",
    "                continue\n",
    "            news_ids_found += 1\n",
    "\n",
    "            # Get cosine similarity scores\n",
    "            preference_score = calculate_cosine_similarity(preference_profile, news_embedding)\n",
    "            non_preference_score = calculate_cosine_similarity(non_preference_profile, news_embedding)\n",
    "\n",
    "            # Compare scores and classify\n",
    "            if preference_score > 0.6 and non_preference_score < 0.45:\n",
    "                predicted_clicked.append(news_id)  # High confidence clicked\n",
    "                high_conf_clicked += 1\n",
    "            elif non_preference_score > 0.6 and preference_score < 0.45:\n",
    "                predicted_not_clicked.append(news_id)  # High confidence not-clicked\n",
    "                high_conf_not_clicked += 1\n",
    "            else:\n",
    "                confidence_diff = abs(preference_score - non_preference_score)\n",
    "                if confidence_diff > 0.0001:\n",
    "                    if preference_score > non_preference_score:\n",
    "                        predicted_clicked.append(news_id)  # Confidence margin resolved as clicked\n",
    "                        confidence_margin_resolved += 1\n",
    "                    else:\n",
    "                        predicted_not_clicked.append(news_id)  # Confidence margin resolved as not-clicked\n",
    "                        confidence_margin_resolved += 1\n",
    "                else:\n",
    "                    predicted_not_clicked.append(news_id)  # Low confidence, default to not-clicked\n",
    "                    low_conf_not_clicked += 1\n",
    "\n",
    "        if clicked_news_ids:\n",
    "            clicked_eval = evaluate_predictions(predicted_clicked, clicked_news_ids)\n",
    "            if clicked_eval:\n",
    "                clicked_metrics.append(clicked_eval)\n",
    "        if not_clicked_news_ids:\n",
    "            not_clicked_eval = evaluate_predictions(predicted_not_clicked, not_clicked_news_ids)\n",
    "            if not_clicked_eval:\n",
    "                not_clicked_metrics.append(not_clicked_eval)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    avg_clicked_metrics = executor.submit(parallel_metric_calculation, clicked_metrics).result()\n",
    "    avg_not_clicked_metrics = executor.submit(parallel_metric_calculation, not_clicked_metrics).result()\n",
    "\n",
    "# Step 5: Printing results\n",
    "print(f\"News IDs Found: {news_ids_found}\")\n",
    "print(f\"News IDs Not Found: {news_ids_not_found}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Clicked Predictions:\")\n",
    "for metric, value in avg_clicked_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Not-Clicked Predictions:\")\n",
    "for metric, value in avg_not_clicked_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Print the counters for each logic part\n",
    "print(\"\\nLogic Part Counters:\")\n",
    "print(f\"High Confidence Clicked Predictions: {high_conf_clicked}\")\n",
    "print(f\"High Confidence Not-Clicked Predictions: {high_conf_not_clicked}\")\n",
    "print(f\"Confidence Margin Resolved Predictions: {confidence_margin_resolved}\")\n",
    "print(f\"Low Confidence Default to Not-Clicked Predictions: {low_conf_not_clicked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e76b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News ID: N91737\n",
      "Preference Score: 0.9180\n",
      "Non-Preference Score: 0.2875\n",
      "News ID: N69938\n",
      "Preference Score: 0.9850\n",
      "Non-Preference Score: 0.9844\n",
      "News ID: N104054\n",
      "Preference Score: 0.7706\n",
      "Non-Preference Score: 0.0576\n",
      "News ID: N86067\n",
      "Preference Score: 0.0826\n",
      "Non-Preference Score: 0.0188\n",
      "News ID: N46641\n",
      "Preference Score: 0.1459\n",
      "Non-Preference Score: 0.0437\n",
      "News ID: N3663\n",
      "Preference Score: 0.9833\n",
      "Non-Preference Score: 0.0041\n",
      "News ID: N18468\n",
      "Preference Score: 0.1422\n",
      "Non-Preference Score: 0.0338\n",
      "News ID: N33901\n",
      "Preference Score: 0.0753\n",
      "Non-Preference Score: 0.0283\n",
      "News ID: N24958\n",
      "Preference Score: 0.9013\n",
      "Non-Preference Score: 0.0345\n",
      "News ID: N105956\n",
      "Preference Score: 0.8275\n",
      "Non-Preference Score: 0.0548\n",
      "News ID: N9250\n",
      "Preference Score: 0.0769\n",
      "Non-Preference Score: 0.0049\n",
      "News ID: N34044\n",
      "Preference Score: 0.0666\n",
      "Non-Preference Score: 0.0275\n",
      "News ID: N119999\n",
      "Preference Score: 0.9780\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N18190\n",
      "Preference Score: 0.9163\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N54368\n",
      "Preference Score: 0.0519\n",
      "Non-Preference Score: 0.1982\n",
      "News ID: N122944\n",
      "Preference Score: 0.9985\n",
      "Non-Preference Score: 0.0114\n",
      "News ID: N18190\n",
      "Preference Score: 0.9934\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N55801\n",
      "Preference Score: 0.8240\n",
      "Non-Preference Score: 0.0090\n",
      "News ID: N64785\n",
      "Preference Score: 0.2980\n",
      "Non-Preference Score: 0.0326\n",
      "News ID: N58465\n",
      "Preference Score: 0.9724\n",
      "Non-Preference Score: 0.0250\n",
      "News ID: N64785\n",
      "Preference Score: 0.8436\n",
      "Non-Preference Score: 0.0311\n",
      "News ID: N2110\n",
      "Preference Score: 0.7575\n",
      "Non-Preference Score: 0.0039\n",
      "News ID: N92905\n",
      "Preference Score: 0.7783\n",
      "Non-Preference Score: 0.0201\n",
      "News ID: N121138\n",
      "Preference Score: 0.1036\n",
      "Non-Preference Score: 0.0529\n",
      "News ID: N118880\n",
      "Preference Score: 0.0451\n",
      "Non-Preference Score: 0.1136\n",
      "News ID: N83732\n",
      "Preference Score: 0.0336\n",
      "Non-Preference Score: 0.0817\n",
      "News ID: N56565\n",
      "Preference Score: 0.0595\n",
      "Non-Preference Score: 0.0583\n",
      "News ID: N21018\n",
      "Preference Score: 0.8492\n",
      "Non-Preference Score: 0.0351\n",
      "News ID: N18468\n",
      "Preference Score: 0.8433\n",
      "Non-Preference Score: 0.0192\n",
      "News ID: N39770\n",
      "Preference Score: 0.9993\n",
      "Non-Preference Score: 0.7237\n",
      "News ID: N92905\n",
      "Preference Score: 0.9994\n",
      "Non-Preference Score: 0.1081\n",
      "News ID: N126696\n",
      "Preference Score: 0.2614\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N130076\n",
      "Preference Score: 0.9292\n",
      "Non-Preference Score: 0.0009\n",
      "News ID: N55801\n",
      "Preference Score: 0.4771\n",
      "Non-Preference Score: 0.0120\n",
      "News ID: N54368\n",
      "Preference Score: 0.5603\n",
      "Non-Preference Score: 0.0613\n",
      "News ID: N120086\n",
      "Preference Score: 0.8635\n",
      "Non-Preference Score: 0.0852\n",
      "News ID: N34044\n",
      "Preference Score: 0.6537\n",
      "Non-Preference Score: 0.0225\n",
      "News ID: N97684\n",
      "Preference Score: 0.7162\n",
      "Non-Preference Score: 0.3400\n",
      "News ID: N27669\n",
      "Preference Score: 0.6615\n",
      "Non-Preference Score: 0.0013\n",
      "News ID: N50568\n",
      "Preference Score: 0.9515\n",
      "Non-Preference Score: 0.0639\n",
      "News ID: N72977\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N12384\n",
      "Preference Score: 0.9970\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N54368\n",
      "Preference Score: 0.8853\n",
      "Non-Preference Score: 0.3752\n",
      "News ID: N123209\n",
      "Preference Score: 0.8682\n",
      "Non-Preference Score: 0.0866\n",
      "News ID: N111050\n",
      "Preference Score: 0.0603\n",
      "Non-Preference Score: 0.0411\n",
      "News ID: N100425\n",
      "Preference Score: 0.8765\n",
      "Non-Preference Score: 0.0115\n",
      "News ID: N72977\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0033\n",
      "News ID: N46894\n",
      "Preference Score: 0.9414\n",
      "Non-Preference Score: 0.0094\n",
      "News ID: N123209\n",
      "Preference Score: 0.8260\n",
      "Non-Preference Score: 0.0804\n",
      "News ID: N122944\n",
      "Preference Score: 0.5761\n",
      "Non-Preference Score: 0.0049\n",
      "News ID: N53018\n",
      "Preference Score: 0.9572\n",
      "Non-Preference Score: 0.4769\n",
      "News ID: N117802\n",
      "Preference Score: 0.9883\n",
      "Non-Preference Score: 0.0119\n",
      "News ID: N30206\n",
      "Preference Score: 0.9755\n",
      "Non-Preference Score: 0.0220\n",
      "News ID: N40742\n",
      "Preference Score: 0.9884\n",
      "Non-Preference Score: 0.9653\n",
      "News ID: N59297\n",
      "Preference Score: 0.7752\n",
      "Non-Preference Score: 0.0235\n",
      "News ID: N29160\n",
      "Preference Score: 0.4501\n",
      "Non-Preference Score: 0.0335\n",
      "News ID: N69938\n",
      "Preference Score: 0.8702\n",
      "Non-Preference Score: 0.1495\n",
      "News ID: N91958\n",
      "Preference Score: 0.9099\n",
      "Non-Preference Score: 0.0322\n",
      "News ID: N130076\n",
      "Preference Score: 0.8020\n",
      "Non-Preference Score: 0.0031\n",
      "News ID: N56784\n",
      "Preference Score: 0.4508\n",
      "Non-Preference Score: 0.0414\n",
      "News ID: N19831\n",
      "Preference Score: 0.7685\n",
      "Non-Preference Score: 0.0015\n",
      "News ID: N2110\n",
      "Preference Score: 0.6822\n",
      "Non-Preference Score: 0.0031\n",
      "News ID: N18468\n",
      "Preference Score: 0.8569\n",
      "Non-Preference Score: 0.0299\n",
      "News ID: N29160\n",
      "Preference Score: 0.9650\n",
      "Non-Preference Score: 0.0119\n",
      "News ID: N119999\n",
      "Preference Score: 0.8825\n",
      "Non-Preference Score: 0.0029\n",
      "News ID: N117802\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.0146\n",
      "News ID: N91737\n",
      "Preference Score: 0.9130\n",
      "Non-Preference Score: 0.1050\n",
      "News ID: N41089\n",
      "Preference Score: 0.9053\n",
      "Non-Preference Score: 0.0277\n",
      "News ID: N126404\n",
      "Preference Score: 0.9342\n",
      "Non-Preference Score: 0.0054\n",
      "News ID: N29160\n",
      "Preference Score: 0.1147\n",
      "Non-Preference Score: 0.0150\n",
      "News ID: N9989\n",
      "Preference Score: 0.1814\n",
      "Non-Preference Score: 0.0090\n",
      "News ID: N52019\n",
      "Preference Score: 0.0425\n",
      "Non-Preference Score: 0.0539\n",
      "News ID: N60732\n",
      "Preference Score: 0.8940\n",
      "Non-Preference Score: 0.0599\n",
      "News ID: N18623\n",
      "Preference Score: 0.5253\n",
      "Non-Preference Score: 0.1410\n",
      "News ID: N14675\n",
      "Preference Score: 0.0195\n",
      "Non-Preference Score: 0.0109\n",
      "News ID: N55801\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0373\n",
      "News ID: N54368\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0493\n",
      "News ID: N34044\n",
      "Preference Score: 0.4710\n",
      "Non-Preference Score: 0.0342\n",
      "News ID: N92462\n",
      "Preference Score: 0.3277\n",
      "Non-Preference Score: 0.0842\n",
      "News ID: N126696\n",
      "Preference Score: 0.4874\n",
      "Non-Preference Score: 0.0026\n",
      "News ID: N23721\n",
      "Preference Score: 0.7426\n",
      "Non-Preference Score: 0.0721\n",
      "News ID: N91865\n",
      "Preference Score: 0.0968\n",
      "Non-Preference Score: 0.0257\n",
      "News ID: N123968\n",
      "Preference Score: 0.6809\n",
      "Non-Preference Score: 0.0245\n",
      "News ID: N89764\n",
      "Preference Score: 0.9706\n",
      "Non-Preference Score: 0.0220\n",
      "News ID: N119999\n",
      "Preference Score: 0.9719\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N21018\n",
      "Preference Score: 0.9835\n",
      "Non-Preference Score: 0.2777\n",
      "News ID: N46555\n",
      "Preference Score: 0.9162\n",
      "Non-Preference Score: 0.4767\n",
      "News ID: N117802\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.0546\n",
      "News ID: N52464\n",
      "Preference Score: 0.9256\n",
      "Non-Preference Score: 0.3290\n",
      "News ID: N21018\n",
      "Preference Score: 0.7239\n",
      "Non-Preference Score: 0.0290\n",
      "News ID: N2110\n",
      "Preference Score: 0.3701\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N122944\n",
      "Preference Score: 0.9698\n",
      "Non-Preference Score: 0.0091\n",
      "News ID: N118623\n",
      "Preference Score: 0.8223\n",
      "Non-Preference Score: 0.0546\n",
      "News ID: N66180\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.9520\n",
      "News ID: N83491\n",
      "Preference Score: 0.9732\n",
      "Non-Preference Score: 0.2994\n",
      "News ID: N80468\n",
      "Preference Score: 0.6728\n",
      "Non-Preference Score: 0.0502\n",
      "News ID: N110967\n",
      "Preference Score: 0.9995\n",
      "Non-Preference Score: 0.6647\n",
      "News ID: N121284\n",
      "Preference Score: 0.8693\n",
      "Non-Preference Score: 0.0435\n",
      "News ID: N26534\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N19831\n",
      "Preference Score: 0.1221\n",
      "Non-Preference Score: 0.0051\n",
      "News ID: N123462\n",
      "Preference Score: 0.1971\n",
      "Non-Preference Score: 0.0692\n",
      "News ID: N28863\n",
      "Preference Score: 0.4539\n",
      "Non-Preference Score: 0.0129\n",
      "News ID: N26553\n",
      "Preference Score: 0.2339\n",
      "Non-Preference Score: 0.0095\n",
      "News ID: N94999\n",
      "Preference Score: 0.0586\n",
      "Non-Preference Score: 0.0433\n",
      "News ID: N51163\n",
      "Preference Score: 0.6247\n",
      "Non-Preference Score: 0.1209\n",
      "News ID: N114950\n",
      "Preference Score: 0.4312\n",
      "Non-Preference Score: 0.0521\n",
      "News ID: N21356\n",
      "Preference Score: 0.3156\n",
      "Non-Preference Score: 0.0619\n",
      "News ID: N121138\n",
      "Preference Score: 0.2227\n",
      "Non-Preference Score: 0.0621\n",
      "News ID: N44909\n",
      "Preference Score: 0.9994\n",
      "Non-Preference Score: 0.9793\n",
      "News ID: N54900\n",
      "Preference Score: 0.9878\n",
      "Non-Preference Score: 0.0139\n",
      "News ID: N110025\n",
      "Preference Score: 0.8486\n",
      "Non-Preference Score: 0.0921\n",
      "News ID: N11405\n",
      "Preference Score: 0.8160\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N53018\n",
      "Preference Score: 0.9119\n",
      "Non-Preference Score: 0.5914\n",
      "News ID: N18468\n",
      "Preference Score: 0.0862\n",
      "Non-Preference Score: 0.0354\n",
      "News ID: N73833\n",
      "Preference Score: 0.0747\n",
      "Non-Preference Score: 0.0620\n",
      "News ID: N33901\n",
      "Preference Score: 0.2212\n",
      "Non-Preference Score: 0.0356\n",
      "News ID: N55801\n",
      "Preference Score: 0.8687\n",
      "Non-Preference Score: 0.0061\n",
      "News ID: N69938\n",
      "Preference Score: 0.9423\n",
      "Non-Preference Score: 0.7763\n",
      "News ID: N48360\n",
      "Preference Score: 0.1586\n",
      "Non-Preference Score: 0.0112\n",
      "News ID: N52388\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N29160\n",
      "Preference Score: 0.6592\n",
      "Non-Preference Score: 0.0484\n",
      "News ID: N95541\n",
      "Preference Score: 0.7730\n",
      "Non-Preference Score: 0.2162\n",
      "News ID: N100425\n",
      "Preference Score: 0.9929\n",
      "Non-Preference Score: 0.0013\n",
      "News ID: N97674\n",
      "Preference Score: 0.9657\n",
      "Non-Preference Score: 0.1763\n",
      "News ID: N37648\n",
      "Preference Score: 0.8726\n",
      "Non-Preference Score: 0.0353\n",
      "News ID: N107199\n",
      "Preference Score: 0.9903\n",
      "Non-Preference Score: 0.0202\n",
      "News ID: N29160\n",
      "Preference Score: 0.4175\n",
      "Non-Preference Score: 0.0698\n",
      "News ID: N53018\n",
      "Preference Score: 0.0867\n",
      "Non-Preference Score: 0.4225\n",
      "News ID: N129416\n",
      "Preference Score: 0.7255\n",
      "Non-Preference Score: 0.9924\n",
      "News ID: N18190\n",
      "Preference Score: 0.9517\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N69938\n",
      "Preference Score: 0.9232\n",
      "Non-Preference Score: 0.2552\n",
      "News ID: N21018\n",
      "Preference Score: 0.1113\n",
      "Non-Preference Score: 0.0304\n",
      "News ID: N52464\n",
      "Preference Score: 0.0444\n",
      "Non-Preference Score: 0.6526\n",
      "News ID: N20417\n",
      "Preference Score: 0.8715\n",
      "Non-Preference Score: 0.7034\n",
      "News ID: N70240\n",
      "Preference Score: 0.1996\n",
      "Non-Preference Score: 0.0171\n",
      "News ID: N19831\n",
      "Preference Score: 0.8123\n",
      "Non-Preference Score: 0.0009\n",
      "News ID: N47891\n",
      "Preference Score: 0.5384\n",
      "Non-Preference Score: 0.0814\n",
      "News ID: N29160\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.0470\n",
      "News ID: N53018\n",
      "Preference Score: 0.9801\n",
      "Non-Preference Score: 0.7351\n",
      "News ID: N13918\n",
      "Preference Score: 0.8831\n",
      "Non-Preference Score: 0.0391\n",
      "News ID: N40742\n",
      "Preference Score: 0.9933\n",
      "Non-Preference Score: 0.5177\n",
      "News ID: N129416\n",
      "Preference Score: 0.9952\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N7082\n",
      "Preference Score: 0.9985\n",
      "Non-Preference Score: 0.0102\n",
      "News ID: N27768\n",
      "Preference Score: 0.0188\n",
      "Non-Preference Score: 0.0046\n",
      "News ID: N95108\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0244\n",
      "News ID: N18110\n",
      "Preference Score: 0.9996\n",
      "Non-Preference Score: 0.0050\n",
      "News ID: N34044\n",
      "Preference Score: 0.9573\n",
      "Non-Preference Score: 0.0336\n",
      "News ID: N89764\n",
      "Preference Score: 0.0616\n",
      "Non-Preference Score: 0.0340\n",
      "News ID: N54368\n",
      "Preference Score: 0.5603\n",
      "Non-Preference Score: 0.0613\n",
      "News ID: N128045\n",
      "Preference Score: 0.9942\n",
      "Non-Preference Score: 0.0415\n",
      "News ID: N54368\n",
      "Preference Score: 0.7842\n",
      "Non-Preference Score: 0.0691\n",
      "News ID: N52464\n",
      "Preference Score: 0.8964\n",
      "Non-Preference Score: 0.5376\n",
      "News ID: N14719\n",
      "Preference Score: 0.9668\n",
      "Non-Preference Score: 0.0059\n",
      "News ID: N111050\n",
      "Preference Score: 0.8865\n",
      "Non-Preference Score: 0.4534\n",
      "News ID: N21033\n",
      "Preference Score: 0.8661\n",
      "Non-Preference Score: 0.0197\n",
      "News ID: N2110\n",
      "Preference Score: 0.9840\n",
      "Non-Preference Score: 0.0034\n",
      "News ID: N56999\n",
      "Preference Score: 0.9843\n",
      "Non-Preference Score: 0.0265\n",
      "News ID: N103036\n",
      "Preference Score: 0.8239\n",
      "Non-Preference Score: 0.0434\n",
      "News ID: N80770\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0336\n",
      "News ID: N122944\n",
      "Preference Score: 0.9691\n",
      "Non-Preference Score: 0.0022\n",
      "News ID: N49048\n",
      "Preference Score: 0.9506\n",
      "Non-Preference Score: 0.0534\n",
      "News ID: N34628\n",
      "Preference Score: 0.8899\n",
      "Non-Preference Score: 0.0417\n",
      "News ID: N100456\n",
      "Preference Score: 0.9110\n",
      "Non-Preference Score: 0.0746\n",
      "News ID: N90206\n",
      "Preference Score: 0.8997\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N29016\n",
      "Preference Score: 0.8256\n",
      "Non-Preference Score: 0.0114\n",
      "News ID: N94217\n",
      "Preference Score: 0.6658\n",
      "Non-Preference Score: 0.8494\n",
      "News ID: N19586\n",
      "Preference Score: 0.9126\n",
      "Non-Preference Score: 0.0084\n",
      "News ID: N54368\n",
      "Preference Score: 0.8264\n",
      "Non-Preference Score: 0.0856\n",
      "News ID: N17106\n",
      "Preference Score: 0.9968\n",
      "Non-Preference Score: 0.0413\n",
      "News ID: N105956\n",
      "Preference Score: 0.7854\n",
      "Non-Preference Score: 0.0336\n",
      "News ID: N13432\n",
      "Preference Score: 0.1651\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N18405\n",
      "Preference Score: 0.9062\n",
      "Non-Preference Score: 0.0248\n",
      "News ID: N89764\n",
      "Preference Score: 0.0829\n",
      "Non-Preference Score: 0.0265\n",
      "News ID: N18468\n",
      "Preference Score: 0.0779\n",
      "Non-Preference Score: 0.0270\n",
      "News ID: N69938\n",
      "Preference Score: 0.6821\n",
      "Non-Preference Score: 0.1177\n",
      "News ID: N30582\n",
      "Preference Score: 0.4563\n",
      "Non-Preference Score: 0.0379\n",
      "News ID: N89764\n",
      "Preference Score: 0.0616\n",
      "Non-Preference Score: 0.0340\n",
      "News ID: N70459\n",
      "Preference Score: 0.4641\n",
      "Non-Preference Score: 0.0396\n",
      "News ID: N100425\n",
      "Preference Score: 0.0682\n",
      "Non-Preference Score: 0.0048\n",
      "News ID: N58465\n",
      "Preference Score: 0.8341\n",
      "Non-Preference Score: 0.0636\n",
      "News ID: N72977\n",
      "Preference Score: 0.8569\n",
      "Non-Preference Score: 0.0011\n",
      "News ID: N18190\n",
      "Preference Score: 0.7298\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N47891\n",
      "Preference Score: 0.1806\n",
      "Non-Preference Score: 0.0769\n",
      "News ID: N46894\n",
      "Preference Score: 0.5238\n",
      "Non-Preference Score: 0.0052\n",
      "News ID: N32286\n",
      "Preference Score: 0.9946\n",
      "Non-Preference Score: 0.2657\n",
      "News ID: N27669\n",
      "Preference Score: 0.6615\n",
      "Non-Preference Score: 0.0013\n",
      "News ID: N123209\n",
      "Preference Score: 0.0769\n",
      "Non-Preference Score: 0.0724\n",
      "News ID: N37648\n",
      "Preference Score: 0.8632\n",
      "Non-Preference Score: 0.0424\n",
      "News ID: N30206\n",
      "Preference Score: 0.5413\n",
      "Non-Preference Score: 0.0332\n",
      "News ID: N70871\n",
      "Preference Score: 0.8773\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N129416\n",
      "Preference Score: 0.9976\n",
      "Non-Preference Score: 0.0011\n",
      "News ID: N72977\n",
      "Preference Score: 0.9167\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N91865\n",
      "Preference Score: 0.0804\n",
      "Non-Preference Score: 0.0207\n",
      "News ID: N33901\n",
      "Preference Score: 0.0456\n",
      "Non-Preference Score: 0.0168\n",
      "News ID: N19586\n",
      "Preference Score: 0.0357\n",
      "Non-Preference Score: 0.0180\n",
      "News ID: N29160\n",
      "Preference Score: 0.0604\n",
      "Non-Preference Score: 0.0134\n",
      "News ID: N17106\n",
      "Preference Score: 0.9913\n",
      "Non-Preference Score: 0.1270\n",
      "News ID: N54368\n",
      "Preference Score: 0.0691\n",
      "Non-Preference Score: 0.0435\n",
      "News ID: N30758\n",
      "Preference Score: 0.1685\n",
      "Non-Preference Score: 0.0069\n",
      "News ID: N122944\n",
      "Preference Score: 0.9826\n",
      "Non-Preference Score: 0.0397\n",
      "News ID: N89764\n",
      "Preference Score: 0.0641\n",
      "Non-Preference Score: 0.0572\n",
      "News ID: N100425\n",
      "Preference Score: 0.9966\n",
      "Non-Preference Score: 0.5204\n",
      "News ID: N96083\n",
      "Preference Score: 0.5865\n",
      "Non-Preference Score: 0.1097\n",
      "News ID: N46555\n",
      "Preference Score: 0.9767\n",
      "Non-Preference Score: 0.6544\n",
      "News ID: N21096\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0393\n",
      "News ID: N127357\n",
      "Preference Score: 0.8398\n",
      "Non-Preference Score: 0.0190\n",
      "News ID: N100425\n",
      "Preference Score: 0.6002\n",
      "Non-Preference Score: 0.0024\n",
      "News ID: N89764\n",
      "Preference Score: 0.0616\n",
      "Non-Preference Score: 0.0340\n",
      "News ID: N19455\n",
      "Preference Score: 0.8673\n",
      "Non-Preference Score: 0.6224\n",
      "News ID: N109802\n",
      "Preference Score: 0.7740\n",
      "Non-Preference Score: 0.3348\n",
      "News ID: N126404\n",
      "Preference Score: 0.9778\n",
      "Non-Preference Score: 0.0101\n",
      "News ID: N91865\n",
      "Preference Score: 0.0968\n",
      "Non-Preference Score: 0.0257\n",
      "News ID: N110079\n",
      "Preference Score: 0.1767\n",
      "Non-Preference Score: 0.0474\n",
      "News ID: N38555\n",
      "Preference Score: 0.9951\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N44453\n",
      "Preference Score: 0.8840\n",
      "Non-Preference Score: 0.0245\n",
      "News ID: N15424\n",
      "Preference Score: 0.9945\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N64109\n",
      "Preference Score: 0.9384\n",
      "Non-Preference Score: 0.0781\n",
      "News ID: N2059\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0078\n",
      "News ID: N80456\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0013\n",
      "News ID: N57663\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0192\n",
      "News ID: N122944\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0011\n",
      "News ID: N96351\n",
      "Preference Score: 0.6441\n",
      "Non-Preference Score: 0.0399\n",
      "News ID: N67588\n",
      "Preference Score: 0.9986\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N91737\n",
      "Preference Score: 0.9063\n",
      "Non-Preference Score: 0.0787\n",
      "News ID: N89764\n",
      "Preference Score: 0.7515\n",
      "Non-Preference Score: 0.0206\n",
      "News ID: N112536\n",
      "Preference Score: 0.0660\n",
      "Non-Preference Score: 0.0143\n",
      "News ID: N100425\n",
      "Preference Score: 0.9075\n",
      "Non-Preference Score: 0.0052\n",
      "News ID: N126832\n",
      "Preference Score: 0.6219\n",
      "Non-Preference Score: 0.4898\n",
      "News ID: N123209\n",
      "Preference Score: 0.7640\n",
      "Non-Preference Score: 0.1327\n",
      "News ID: N79044\n",
      "Preference Score: 0.8030\n",
      "Non-Preference Score: 0.0933\n",
      "News ID: N82222\n",
      "Preference Score: 0.0609\n",
      "Non-Preference Score: 0.0091\n",
      "News ID: N54368\n",
      "Preference Score: 0.9176\n",
      "Non-Preference Score: 0.1044\n",
      "News ID: N66180\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.9168\n",
      "News ID: N65373\n",
      "Preference Score: 0.8945\n",
      "Non-Preference Score: 0.0689\n",
      "News ID: N87192\n",
      "Preference Score: 0.9691\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N25621\n",
      "Preference Score: 0.9309\n",
      "Non-Preference Score: 0.0827\n",
      "News ID: N30582\n",
      "Preference Score: 0.9739\n",
      "Non-Preference Score: 0.5609\n",
      "News ID: N12384\n",
      "Preference Score: 0.9985\n",
      "Non-Preference Score: 0.0050\n",
      "News ID: N69938\n",
      "Preference Score: 0.8937\n",
      "Non-Preference Score: 0.9318\n",
      "News ID: N29160\n",
      "Preference Score: 0.8985\n",
      "Non-Preference Score: 0.2565\n",
      "News ID: N128045\n",
      "Preference Score: 0.9824\n",
      "Non-Preference Score: 0.6780\n",
      "News ID: N122944\n",
      "Preference Score: 0.9909\n",
      "Non-Preference Score: 0.0168\n",
      "News ID: N129416\n",
      "Preference Score: 0.8416\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N46641\n",
      "Preference Score: 0.0567\n",
      "Non-Preference Score: 0.0463\n",
      "News ID: N87192\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0049\n",
      "News ID: N28863\n",
      "Preference Score: 0.9207\n",
      "Non-Preference Score: 0.0634\n",
      "News ID: N99846\n",
      "Preference Score: 0.9607\n",
      "Non-Preference Score: 0.0130\n",
      "News ID: N17038\n",
      "Preference Score: 0.9329\n",
      "Non-Preference Score: 0.0154\n",
      "News ID: N3737\n",
      "Preference Score: 0.0226\n",
      "Non-Preference Score: 0.0393\n",
      "News ID: N44453\n",
      "Preference Score: 0.0348\n",
      "Non-Preference Score: 0.0202\n",
      "News ID: N123462\n",
      "Preference Score: 0.1970\n",
      "Non-Preference Score: 0.0837\n",
      "News ID: N90986\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0023\n",
      "News ID: N103830\n",
      "Preference Score: 0.9901\n",
      "Non-Preference Score: 0.5420\n",
      "News ID: N19586\n",
      "Preference Score: 0.9997\n",
      "Non-Preference Score: 0.0139\n",
      "News ID: N26553\n",
      "Preference Score: 0.6713\n",
      "Non-Preference Score: 0.0028\n",
      "News ID: N3663\n",
      "Preference Score: 0.6909\n",
      "Non-Preference Score: 0.0029\n",
      "News ID: N18190\n",
      "Preference Score: 0.7298\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N54368\n",
      "Preference Score: 0.5603\n",
      "Non-Preference Score: 0.0613\n",
      "News ID: N18190\n",
      "Preference Score: 0.9728\n",
      "Non-Preference Score: 0.0007\n",
      "News ID: N54368\n",
      "Preference Score: 0.8953\n",
      "Non-Preference Score: 0.2694\n",
      "News ID: N12575\n",
      "Preference Score: 0.9760\n",
      "Non-Preference Score: 0.9229\n",
      "News ID: N87236\n",
      "Preference Score: 0.9298\n",
      "Non-Preference Score: 0.9190\n",
      "News ID: N33539\n",
      "Preference Score: 0.9855\n",
      "Non-Preference Score: 0.0045\n",
      "News ID: N13142\n",
      "Preference Score: 0.6264\n",
      "Non-Preference Score: 0.0421\n",
      "News ID: N35304\n",
      "Preference Score: 0.8947\n",
      "Non-Preference Score: 0.0025\n",
      "News ID: N32286\n",
      "Preference Score: 0.1298\n",
      "Non-Preference Score: 0.1041\n",
      "News ID: N59134\n",
      "Preference Score: 0.0467\n",
      "Non-Preference Score: 0.0129\n",
      "News ID: N98252\n",
      "Preference Score: 0.0448\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N29160\n",
      "Preference Score: 0.0504\n",
      "Non-Preference Score: 0.0181\n",
      "News ID: N15861\n",
      "Preference Score: 0.7850\n",
      "Non-Preference Score: 0.0388\n",
      "News ID: N30206\n",
      "Preference Score: 0.4497\n",
      "Non-Preference Score: 0.0205\n",
      "News ID: N29160\n",
      "Preference Score: 0.0710\n",
      "Non-Preference Score: 0.2710\n",
      "News ID: N91737\n",
      "Preference Score: 0.9158\n",
      "Non-Preference Score: 0.0888\n",
      "News ID: N119999\n",
      "Preference Score: 0.7940\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N69938\n",
      "Preference Score: 0.9318\n",
      "Non-Preference Score: 0.2267\n",
      "News ID: N55801\n",
      "Preference Score: 0.0408\n",
      "Non-Preference Score: 0.0167\n",
      "News ID: N129416\n",
      "Preference Score: 0.9996\n",
      "Non-Preference Score: 0.0711\n",
      "News ID: N53018\n",
      "Preference Score: 0.9239\n",
      "Non-Preference Score: 0.8696\n",
      "News ID: N120089\n",
      "Preference Score: 0.9995\n",
      "Non-Preference Score: 0.9254\n",
      "News ID: N58760\n",
      "Preference Score: 0.4006\n",
      "Non-Preference Score: 0.0305\n",
      "News ID: N44294\n",
      "Preference Score: 0.0458\n",
      "Non-Preference Score: 0.0653\n",
      "News ID: N54368\n",
      "Preference Score: 0.9962\n",
      "Non-Preference Score: 0.1866\n",
      "News ID: N62800\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0664\n",
      "News ID: N54368\n",
      "Preference Score: 0.5603\n",
      "Non-Preference Score: 0.0613\n",
      "News ID: N125410\n",
      "Preference Score: 0.8846\n",
      "Non-Preference Score: 0.9116\n",
      "News ID: N122944\n",
      "Preference Score: 0.9991\n",
      "Non-Preference Score: 0.0021\n",
      "News ID: N80770\n",
      "Preference Score: 0.8497\n",
      "Non-Preference Score: 0.0730\n",
      "News ID: N96351\n",
      "Preference Score: 0.6114\n",
      "Non-Preference Score: 0.1959\n",
      "News ID: N13432\n",
      "Preference Score: 0.8549\n",
      "Non-Preference Score: 0.0453\n",
      "News ID: N76665\n",
      "Preference Score: 0.8802\n",
      "Non-Preference Score: 0.0363\n",
      "News ID: N104665\n",
      "Preference Score: 0.4929\n",
      "Non-Preference Score: 0.0043\n",
      "News ID: N35236\n",
      "Preference Score: 0.9974\n",
      "Non-Preference Score: 0.9823\n",
      "News ID: N14675\n",
      "Preference Score: 0.0585\n",
      "Non-Preference Score: 0.0113\n",
      "News ID: N25621\n",
      "Preference Score: 0.9183\n",
      "Non-Preference Score: 0.0453\n",
      "News ID: N18190\n",
      "Preference Score: 0.7298\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N129416\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0019\n",
      "News ID: N39770\n",
      "Preference Score: 0.0648\n",
      "Non-Preference Score: 0.0754\n",
      "News ID: N29160\n",
      "Preference Score: 0.9659\n",
      "Non-Preference Score: 0.0080\n",
      "News ID: N2110\n",
      "Preference Score: 0.9604\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N85089\n",
      "Preference Score: 0.4446\n",
      "Non-Preference Score: 0.1086\n",
      "News ID: N127357\n",
      "Preference Score: 0.6778\n",
      "Non-Preference Score: 0.0832\n",
      "News ID: N123209\n",
      "Preference Score: 0.1097\n",
      "Non-Preference Score: 0.0775\n",
      "News ID: N54368\n",
      "Preference Score: 0.0564\n",
      "Non-Preference Score: 0.0450\n",
      "News ID: N51163\n",
      "Preference Score: 0.1864\n",
      "Non-Preference Score: 0.0800\n",
      "News ID: N103830\n",
      "Preference Score: 0.4206\n",
      "Non-Preference Score: 0.0255\n",
      "News ID: N39770\n",
      "Preference Score: 0.3957\n",
      "Non-Preference Score: 0.7765\n",
      "News ID: N18190\n",
      "Preference Score: 0.9536\n",
      "Non-Preference Score: 0.4411\n",
      "News ID: N29160\n",
      "Preference Score: 0.1175\n",
      "Non-Preference Score: 0.0187\n",
      "News ID: N129416\n",
      "Preference Score: 0.8416\n",
      "Non-Preference Score: 0.0010\n",
      "News ID: N29160\n",
      "Preference Score: 0.0536\n",
      "Non-Preference Score: 0.0107\n",
      "News ID: N18468\n",
      "Preference Score: 0.0744\n",
      "Non-Preference Score: 0.0176\n",
      "News ID: N2110\n",
      "Preference Score: 0.0859\n",
      "Non-Preference Score: 0.0013\n",
      "News ID: N21018\n",
      "Preference Score: 0.7946\n",
      "Non-Preference Score: 0.0375\n",
      "News ID: N120089\n",
      "Preference Score: 0.9963\n",
      "Non-Preference Score: 0.0279\n",
      "News ID: N92462\n",
      "Preference Score: 0.8761\n",
      "Non-Preference Score: 0.0505\n",
      "News ID: N117802\n",
      "Preference Score: 0.4902\n",
      "Non-Preference Score: 0.0079\n",
      "News ID: N86067\n",
      "Preference Score: 0.0881\n",
      "Non-Preference Score: 0.0129\n",
      "News ID: N116877\n",
      "Preference Score: 0.1236\n",
      "Non-Preference Score: 0.0115\n",
      "News ID: N91737\n",
      "Preference Score: 0.0989\n",
      "Non-Preference Score: 0.0624\n",
      "News ID: N69938\n",
      "Preference Score: 0.0816\n",
      "Non-Preference Score: 0.0607\n",
      "News ID: N92476\n",
      "Preference Score: 0.0529\n",
      "Non-Preference Score: 0.0689\n",
      "News ID: N18258\n",
      "Preference Score: 0.8688\n",
      "Non-Preference Score: 0.0539\n",
      "News ID: N82503\n",
      "Preference Score: 0.8924\n",
      "Non-Preference Score: 0.0424\n",
      "News ID: N54368\n",
      "Preference Score: 0.9151\n",
      "Non-Preference Score: 0.0420\n",
      "News ID: N29160\n",
      "Preference Score: 0.8726\n",
      "Non-Preference Score: 0.0405\n",
      "News ID: N129416\n",
      "Preference Score: 0.8723\n",
      "Non-Preference Score: 0.9690\n",
      "News ID: N2110\n",
      "Preference Score: 0.6619\n",
      "Non-Preference Score: 0.8495\n",
      "News ID: N70883\n",
      "Preference Score: 0.0535\n",
      "Non-Preference Score: 0.0003\n",
      "News ID: N109802\n",
      "Preference Score: 0.7641\n",
      "Non-Preference Score: 0.6206\n",
      "News ID: N85943\n",
      "Preference Score: 0.1312\n",
      "Non-Preference Score: 0.0369\n",
      "News ID: N108495\n",
      "Preference Score: 0.0545\n",
      "Non-Preference Score: 0.0116\n",
      "News ID: N116564\n",
      "Preference Score: 0.0757\n",
      "Non-Preference Score: 0.0695\n",
      "News ID: N16716\n",
      "Preference Score: 0.0348\n",
      "Non-Preference Score: 0.0039\n",
      "News ID: N96351\n",
      "Preference Score: 0.5458\n",
      "Non-Preference Score: 0.1242\n",
      "News ID: N18356\n",
      "Preference Score: 0.1052\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N44453\n",
      "Preference Score: 0.8840\n",
      "Non-Preference Score: 0.0480\n",
      "News ID: N80105\n",
      "Preference Score: 0.6297\n",
      "Non-Preference Score: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News ID: N18623\n",
      "Preference Score: 0.0333\n",
      "Non-Preference Score: 0.0871\n",
      "News ID: N121375\n",
      "Preference Score: 0.0973\n",
      "Non-Preference Score: 0.1217\n",
      "News ID: N99846\n",
      "Preference Score: 0.1133\n",
      "Non-Preference Score: 0.0003\n",
      "News ID: N92905\n",
      "Preference Score: 0.1126\n",
      "Non-Preference Score: 0.0115\n",
      "News ID: N29160\n",
      "Preference Score: 0.0765\n",
      "Non-Preference Score: 0.2266\n",
      "News ID: N56602\n",
      "Preference Score: 0.0968\n",
      "Non-Preference Score: 0.0109\n",
      "News ID: N122944\n",
      "Preference Score: 0.9994\n",
      "Non-Preference Score: 0.9225\n",
      "News ID: N55801\n",
      "Preference Score: 0.9836\n",
      "Non-Preference Score: 0.8323\n",
      "News ID: N21356\n",
      "Preference Score: 0.5230\n",
      "Non-Preference Score: 0.0737\n",
      "News ID: N18258\n",
      "Preference Score: 0.5309\n",
      "Non-Preference Score: 0.0518\n",
      "News ID: N53018\n",
      "Preference Score: 0.9680\n",
      "Non-Preference Score: 0.5961\n",
      "News ID: N129416\n",
      "Preference Score: 0.9979\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N29160\n",
      "Preference Score: 0.6389\n",
      "Non-Preference Score: 0.0258\n",
      "News ID: N62456\n",
      "Preference Score: 0.8467\n",
      "Non-Preference Score: 0.0497\n",
      "News ID: N2110\n",
      "Preference Score: 0.2641\n",
      "Non-Preference Score: 0.0736\n",
      "News ID: N53018\n",
      "Preference Score: 0.6513\n",
      "Non-Preference Score: 0.0476\n",
      "News ID: N129416\n",
      "Preference Score: 0.9992\n",
      "Non-Preference Score: 0.6851\n",
      "News ID: N120086\n",
      "Preference Score: 0.0431\n",
      "Non-Preference Score: 0.0322\n",
      "News ID: N4148\n",
      "Preference Score: 0.6542\n",
      "Non-Preference Score: 0.6811\n",
      "News ID: N54368\n",
      "Preference Score: 0.0794\n",
      "Non-Preference Score: 0.1262\n",
      "News ID: N59619\n",
      "Preference Score: 0.3624\n",
      "Non-Preference Score: 0.0891\n",
      "News ID: N100425\n",
      "Preference Score: 0.9880\n",
      "Non-Preference Score: 0.0103\n",
      "News ID: N40742\n",
      "Preference Score: 0.9746\n",
      "Non-Preference Score: 0.9694\n",
      "News ID: N70024\n",
      "Preference Score: 0.8468\n",
      "Non-Preference Score: 0.8933\n",
      "News ID: N30582\n",
      "Preference Score: 0.8605\n",
      "Non-Preference Score: 0.3073\n",
      "News ID: N123209\n",
      "Preference Score: 0.5946\n",
      "Non-Preference Score: 0.0817\n",
      "News ID: N89764\n",
      "Preference Score: 0.9352\n",
      "Non-Preference Score: 0.0146\n",
      "News ID: N72054\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0006\n",
      "News ID: N27478\n",
      "Preference Score: 0.9853\n",
      "Non-Preference Score: 0.0016\n",
      "News ID: N3663\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.0009\n",
      "News ID: N86352\n",
      "Preference Score: 0.3111\n",
      "Non-Preference Score: 0.0035\n",
      "News ID: N52464\n",
      "Preference Score: 0.4245\n",
      "Non-Preference Score: 0.0982\n",
      "News ID: N128045\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0192\n",
      "News ID: N47891\n",
      "Preference Score: 0.9457\n",
      "Non-Preference Score: 0.0946\n",
      "News ID: N29160\n",
      "Preference Score: 0.9940\n",
      "Non-Preference Score: 0.0090\n",
      "News ID: N37490\n",
      "Preference Score: 0.9378\n",
      "Non-Preference Score: 0.0015\n",
      "News ID: N29160\n",
      "Preference Score: 0.8182\n",
      "Non-Preference Score: 0.0904\n",
      "News ID: N7728\n",
      "Preference Score: 0.8415\n",
      "Non-Preference Score: 0.7640\n",
      "News ID: N123209\n",
      "Preference Score: 0.8398\n",
      "Non-Preference Score: 0.0769\n",
      "News ID: N125192\n",
      "Preference Score: 0.3263\n",
      "Non-Preference Score: 0.0310\n",
      "News ID: N92905\n",
      "Preference Score: 0.0402\n",
      "Non-Preference Score: 0.0105\n",
      "News ID: N35736\n",
      "Preference Score: 0.9999\n",
      "Non-Preference Score: 0.3344\n",
      "News ID: N91865\n",
      "Preference Score: 0.9635\n",
      "Non-Preference Score: 0.0250\n",
      "News ID: N44493\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N25814\n",
      "Preference Score: 0.9992\n",
      "Non-Preference Score: 0.0003\n",
      "News ID: N105956\n",
      "Preference Score: 0.8728\n",
      "Non-Preference Score: 0.7785\n",
      "News ID: N21018\n",
      "Preference Score: 0.7736\n",
      "Non-Preference Score: 0.0671\n",
      "News ID: N21018\n",
      "Preference Score: 0.8906\n",
      "Non-Preference Score: 0.0608\n",
      "News ID: N18468\n",
      "Preference Score: 0.4748\n",
      "Non-Preference Score: 0.0763\n",
      "News ID: N129416\n",
      "Preference Score: 1.0000\n",
      "Non-Preference Score: 0.0003\n",
      "News ID: N2110\n",
      "Preference Score: 0.9579\n",
      "Non-Preference Score: 0.0004\n",
      "News ID: N117802\n",
      "Preference Score: 0.9668\n",
      "Non-Preference Score: 0.0122\n",
      "News ID: N123209\n",
      "Preference Score: 0.0769\n",
      "Non-Preference Score: 0.0724\n",
      "News ID: N92476\n",
      "Preference Score: 0.0639\n",
      "Non-Preference Score: 0.0486\n",
      "News ID: N58760\n",
      "Preference Score: 0.2010\n",
      "Non-Preference Score: 0.0300\n",
      "News ID: N129416\n",
      "Preference Score: 0.9924\n",
      "Non-Preference Score: 0.0005\n",
      "News ID: N14719\n",
      "Preference Score: 0.9997\n",
      "Non-Preference Score: 0.0079\n",
      "News ID: N18190\n",
      "Preference Score: 0.9998\n",
      "Non-Preference Score: 0.0003\n",
      "News ID: N100368\n",
      "Preference Score: 0.6065\n",
      "Non-Preference Score: 0.0228\n",
      "News ID: N46555\n",
      "Preference Score: 0.7638\n",
      "Non-Preference Score: 0.1261\n",
      "News ID: N28518\n",
      "Preference Score: 0.9951\n",
      "Non-Preference Score: 0.0361\n",
      "News ID: N121267\n",
      "Preference Score: 0.9257\n",
      "Non-Preference Score: 0.3981\n",
      "News ID: N26483\n",
      "Preference Score: 0.5930\n",
      "Non-Preference Score: 0.9996\n",
      "News ID: N48323\n",
      "Preference Score: 0.4782\n",
      "Non-Preference Score: 0.9984\n",
      "News ID: N18190\n",
      "Preference Score: 0.9973\n",
      "Non-Preference Score: 0.4129\n",
      "News ID: N54368\n",
      "Preference Score: 0.8562\n",
      "Non-Preference Score: 0.4609\n",
      "News ID: N8599\n",
      "Preference Score: 0.9208\n",
      "Non-Preference Score: 0.0012\n",
      "News ID: N126404\n",
      "Preference Score: 0.1301\n",
      "Non-Preference Score: 0.0035\n",
      "News ID: N79871\n",
      "Preference Score: 0.9917\n",
      "Non-Preference Score: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 148\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# Get model predictions\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     preference_score \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    144\u001b[0m         torch\u001b[38;5;241m.\u001b[39mstack([preference_profile, news_embedding])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    145\u001b[0m     )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    146\u001b[0m     non_preference_score \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    147\u001b[0m         torch\u001b[38;5;241m.\u001b[39mstack([non_preference_profile, news_embedding])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 148\u001b[0m     )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# Print the scores for debugging\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnews_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_HEADS = 4         # Match training model\n",
    "HIDDEN_DIM = 128      # Match training model\n",
    "EMBED_DIM = 1088      # Input embedding dimension\n",
    "PROJECTION_DIM = 128  # Dimension after projection layer\n",
    "BATCH_SIZE = 10000    # Dynamically adjustable\n",
    "\n",
    "model_checkpoint_path = os.path.join(model_dir, 'best_model_epoch_9.pt')\n",
    "\n",
    "\n",
    "# Step 3: Load pretrained model and define evaluation matrix\n",
    "class MultiHeadAttentionModel(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, projection_dim):\n",
    "        \"\"\"\n",
    "        Multi-head attention model with a projection layer for dimensionality reduction.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            hidden_dim (int): Dimension of hidden layers in the feed-forward network.\n",
    "            projection_dim (int): Dimension after projection layer.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttentionModel, self).__init__()\n",
    "        # Projection layer\n",
    "        self.projection = nn.Linear(embed_dim, projection_dim)\n",
    "\n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=projection_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Feed-forward layers\n",
    "        self.linear1 = nn.Linear(projection_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.linear3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "\n",
    "        # Activation and output layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_dim // 4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_profiles):\n",
    "        \"\"\"\n",
    "        Forward pass for evaluation.\n",
    "\n",
    "        Args:\n",
    "            user_profiles (Tensor): Input tensor of shape (batch_size, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted scores for user profiles.\n",
    "        \"\"\"\n",
    "        # Ensure input is 3D\n",
    "        user_profiles = user_profiles.unsqueeze(1) if user_profiles.dim() == 2 else user_profiles\n",
    "\n",
    "        # Step 1: Project embeddings to lower dimensions\n",
    "        projected_profiles = self.projection(user_profiles)\n",
    "\n",
    "        # Step 2: Multi-head attention\n",
    "        attn_output, _ = self.multihead_attention(projected_profiles, projected_profiles, projected_profiles)\n",
    "        attn_output = attn_output.mean(dim=1)\n",
    "\n",
    "        # Step 3: Feed-forward layers\n",
    "        x = self.relu(self.linear1(attn_output))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "\n",
    "        # Step 4: Output layer\n",
    "        return self.sigmoid(self.output_layer(x))\n",
    "\n",
    "\n",
    "# Load pretrained model for evaluation\n",
    "print(\"Loading pretrained model...\")\n",
    "model = MultiHeadAttentionModel(embed_dim=EMBED_DIM, num_heads=NUM_HEADS, hidden_dim=HIDDEN_DIM, projection_dim=PROJECTION_DIM).to(device)\n",
    "model.load_state_dict(torch.load(model_checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "def evaluate_predictions(predicted, actual):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against actual clicked/not-clicked news IDs.\n",
    "    \"\"\"\n",
    "    if len(actual) == 0:\n",
    "        return None\n",
    "    if len(predicted) == 0:\n",
    "        # Handle cases with no predictions\n",
    "        precision, recall, f1, accuracy = 0.0, 0.0, 0.0, 0.0\n",
    "    else:\n",
    "        y_true = [1 if pred in actual else 0 for pred in predicted]\n",
    "        y_pred = [1] * len(predicted)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1, \"Accuracy\": accuracy}\n",
    "\n",
    "def parallel_metric_calculation(metrics_list):\n",
    "    \"\"\"\n",
    "    Calculate average metrics across all batches.\n",
    "    \"\"\"\n",
    "    avg_metrics = defaultdict(float)\n",
    "    total = len(metrics_list)\n",
    "    if total == 0:\n",
    "        return {\"Precision\": 0.0, \"Recall\": 0.0, \"F1 Score\": 0.0, \"Accuracy\": 0.0}\n",
    "    for metrics in metrics_list:\n",
    "        for key, value in metrics.items():\n",
    "            avg_metrics[key] += value\n",
    "    for key in avg_metrics.keys():\n",
    "        avg_metrics[key] /= total\n",
    "    return avg_metrics\n",
    "\n",
    "# Step 4: Validation loop\n",
    "print(\"Starting validation...\")\n",
    "clicked_metrics = []\n",
    "not_clicked_metrics = []\n",
    "news_ids_found = 0\n",
    "news_ids_not_found = 0\n",
    "\n",
    "# Counters for each logic part\n",
    "high_conf_clicked = 0\n",
    "high_conf_not_clicked = 0\n",
    "confidence_margin_resolved = 0\n",
    "low_conf_not_clicked = 0\n",
    "\n",
    "for batch_start in tqdm(range(0, len(behavior_df), BATCH_SIZE)):\n",
    "    batch_df = behavior_df.iloc[batch_start:batch_start + BATCH_SIZE]\n",
    "    for _, row in batch_df.iterrows():\n",
    "        user_id = row['User ID']\n",
    "        displayed_news_ids = row['Displayed News List']\n",
    "        clicked_news_ids = row['Clicked News IDs']\n",
    "        not_clicked_news_ids = row['Not-Clicked News IDs']\n",
    "\n",
    "        user_profile = user_profiles.get(user_id, default_user_profile)\n",
    "        preference_profile = user_profile['preference_profile']\n",
    "        non_preference_profile = user_profile['non_preference_profile']\n",
    "        predicted_clicked, predicted_not_clicked = [], []\n",
    "\n",
    "        for news_id in displayed_news_ids:\n",
    "            news_embedding = news_embeddings_map.get(news_id)\n",
    "            if news_embedding is None:\n",
    "                news_ids_not_found += 1\n",
    "                continue\n",
    "            news_ids_found += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get model predictions\n",
    "                preference_score = model(\n",
    "                    torch.stack([preference_profile, news_embedding]).unsqueeze(0).to(device)\n",
    "                ).item()\n",
    "                non_preference_score = model(\n",
    "                    torch.stack([non_preference_profile, news_embedding]).unsqueeze(0).to(device)\n",
    "                ).item()\n",
    "                \n",
    "                # Print the scores for debugging\n",
    "                print(f\"News ID: {news_id}\")\n",
    "                print(f\"Preference Score: {preference_score:.4f}\")\n",
    "                print(f\"Non-Preference Score: {non_preference_score:.4f}\")\n",
    "\n",
    "            # Compare scores and classify\n",
    "            if preference_score > 0.6 and non_preference_score < 0.45:\n",
    "                predicted_clicked.append(news_id)  # High confidence clicked\n",
    "                high_conf_clicked += 1\n",
    "            elif non_preference_score > 0.6 and preference_score < 0.45:\n",
    "                predicted_not_clicked.append(news_id)  # High confidence not-clicked\n",
    "                high_conf_not_clicked += 1\n",
    "            else:\n",
    "                confidence_diff = abs(preference_score - non_preference_score)\n",
    "                if confidence_diff > 0.0001:\n",
    "                    if preference_score > non_preference_score:\n",
    "                        predicted_clicked.append(news_id)  # Confidence margin resolved as clicked\n",
    "                        confidence_margin_resolved += 1\n",
    "                    else:\n",
    "                        predicted_not_clicked.append(news_id)  # Confidence margin resolved as not-clicked\n",
    "                        confidence_margin_resolved += 1\n",
    "                else:\n",
    "                    predicted_not_clicked.append(news_id)  # Low confidence, default to not-clicked\n",
    "                    low_conf_not_clicked += 1\n",
    "\n",
    "        if clicked_news_ids:\n",
    "            clicked_eval = evaluate_predictions(predicted_clicked, clicked_news_ids)\n",
    "            if clicked_eval:\n",
    "                clicked_metrics.append(clicked_eval)\n",
    "        if not_clicked_news_ids:\n",
    "            not_clicked_eval = evaluate_predictions(predicted_not_clicked, not_clicked_news_ids)\n",
    "            if not_clicked_eval:\n",
    "                not_clicked_metrics.append(not_clicked_eval)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    avg_clicked_metrics = executor.submit(parallel_metric_calculation, clicked_metrics).result()\n",
    "    avg_not_clicked_metrics = executor.submit(parallel_metric_calculation, not_clicked_metrics).result()\n",
    "\n",
    "# Step 5: Printing results\n",
    "print(f\"News IDs Found: {news_ids_found}\")\n",
    "print(f\"News IDs Not Found: {news_ids_not_found}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Clicked Predictions:\")\n",
    "for metric, value in avg_clicked_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Not-Clicked Predictions:\")\n",
    "for metric, value in avg_not_clicked_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Print the counters for each logic part\n",
    "print(\"\\nLogic Part Counters:\")\n",
    "print(f\"High Confidence Clicked Predictions: {high_conf_clicked}\")\n",
    "print(f\"High Confidence Not-Clicked Predictions: {high_conf_not_clicked}\")\n",
    "print(f\"Confidence Margin Resolved Predictions: {confidence_margin_resolved}\")\n",
    "print(f\"Low Confidence Default to Not-Clicked Predictions: {low_conf_not_clicked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8122215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb7910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe67e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
